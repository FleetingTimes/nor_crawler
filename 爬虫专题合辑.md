# 通用爬虫专题合辑与深度指南（不依赖框架，含详注示例）

本合辑将所有专题集中到一个文档，系统阐述从零到一设计与实现稳定、可维护、可扩展的通用爬虫。包含：网络与协议、HTML 解析深入、反爬与对策、调度与并发、去重与增量、浏览器渲染、存储与索引、日志与监控、工程与安全、分布式爬虫、高级解析、媒体下载、国际化与编码、性能优化、站点实战案例。所有示例代码均使用详细中文注释，便于直接应用与扩展。

—

## 专题索引
- 专题 01：网络与协议基础（HTTP/缓存/会话/超时/代理）
- 专题 02：HTML 解析深入（CSS/XPath/选择器容错/正则增强）
- 专题 03：反爬与对策（礼貌抓取、UA/代理轮换、验证码、指纹）
- 专题 04：调度与并发（Frontier 队列、域级限流、优先级、异步 I/O）
- 专题 05：去重与增量（URL 规范化、内容指纹、条件请求、复跑）
- 专题 06：浏览器渲染（Playwright/Selenium、等待策略、稳定化）
- 专题 07：存储与索引（文件/数据库/对象存储、索引设计、检索）
- 专题 08：日志与监控（日志分级、指标暴露、告警、容量规划）
- 专题 09：工程与安全（测试/CI/CD/部署、账号风控、审计）
- 专题 10：分布式爬虫（消息队列、Frontier 分片、集中去重与一致性）
- 专题 11：高级解析（结构变更、模板匹配、SimHash 近重复检测）
- 专题 12：媒体下载（命名规则、完整性校验、断点续传）
- 专题 13：国际化与编码（字符集、RTL、日期与时区、本地化）
- 专题 14：性能优化（I/O/CPU/内存、并发模型、分析与调优）
- 专题 15：站点实战案例（新闻/论坛/电商三个模板案例）

—

## 专题 01：网络与协议基础（HTTP/缓存/会话/超时/代理）
本专题全面覆盖抓取过程中的网络与协议问题，从 HTTP 基础、缓存与条件请求、会话与认证、连接与超时管理、重试与熔断、代理池与健康检查、DNS/TLS/HTTP2/3、压缩与内容协商、流式下载与断点续传、重定向与跨域策略，到日志追踪与伦理合规边界。目标是让你在面对复杂网络环境和反爬策略时，仍能“慢而稳”地实现高可用抓取。

核心原则与设计目标：
- 可观测：每次请求的状态码、耗时、重试次数、代理/UA、目标域名，必须可记录与检索。
- 幂等与可重入：失败可重试、成功可跳过；条件请求与缓存减少无意义请求。
- 礼貌与合规：控制并发与频率，遵守 robots.txt 与站点策略（即便无强制约束）；避免造成实际负载影响。
- 安全与最小权限：认证信息、Cookie、Token 必须安全保存与最小暴露；对敏感页使用白名单限制。

一、HTTP 状态码与语义深解
- 2xx：表示成功，`200 OK`（常规成功）、`204 No Content`（通常在 API 返回空内容）。在成功情况下，仍需检查 `Content-Type` 与内容长度，避免被动返回干扰页。
- 3xx：重定向。`301/302` 多用于移动端跳转或登录页引导；`303` 常用于 POST 后跳转（表单提交）；`304 Not Modified` 是条件请求核心，收到 304 应直接跳过解析与写入。
- 4xx：客户端错误。`403 Forbidden` 通常是触发风控或权限不足；`404 Not Found` 表示资源不存在（注意是否有软 404，如返回 200 但内容提示未找到）；`429 Too Many Requests` 表示频率过高，应立刻降速并延长退避；`401 Unauthorized` 表示需要认证。
- 5xx：服务端错误。`500/502/503/504` 分别对应内部错误、网关错误、服务不可用、网关超时。对 5xx 实施指数退避重试，若持续失败需暂停该域名抓取。

二、重定向与跨域
- 跟随重定向默认开启，但对认证流（登录跳转）要启用“保留方法/保留头部”策略，避免 Cookie 或 Token 丢失。
- 跨域跳转尤其是在 SSO（单点登录）场景中，需要在会话中维护一致的 `Referer` 与必要头部，并记录从 A→B 的跳转链，以便后续分析风控行为。

三、头部与内容协商
- 常用头部：`User-Agent`（仿真常见浏览器，但避免过度伪装）、`Accept`（控制响应类型）、`Accept-Language`（获取本地化内容）、`Referer`（某些站点检查来源）。
- 内容压缩：`Accept-Encoding: gzip, br` 以获取更小响应体；记得正确解压；对超大响应体采用流式读取。
- 内容类型：按 `Content-Type` 分支处理（`text/html`→HTML解析，`application/json`→JSON解析，`image/*`→媒体下载）。

四、缓存模型与条件请求（ETag/Last-Modified）
- 强缓存与协商缓存：强缓存通过 `Cache-Control`/`Expires` 控制是否直接使用本地副本；协商缓存通过 `ETag/Last-Modified` 与服务器协商是否更新。
- 抓取场景目标：在不影响更新时效的前提下，最大化利用协商缓存，减少重复抓取与解析成本。
- 版本字段落库：将 `ETag` 与 `Last-Modified` 保存到持久化存储（如 SQLite/PostgreSQL），每次请求回填，以便增量抓取。

目标与收益（详细补充）
- 降低重复抓取的网络与计算成本，显著提升吞吐。
- 在内容未变更时通过 304 节省带宽与存储；在变更时快速定位需要重抓的资源。
- 为增量抓取、断点续抓与并发稳定性提供基础设施。

核心概念（详细补充）
- `ETag` / `If-None-Match`：服务器返回的实体标签；客户端下次抓取时携带 `If-None-Match: <etag>` 可获 304（未变更）。
- `Last-Modified` / `If-Modified-Since`：基于最后修改时间的条件请求；适合静态资源与稳定站点。
- 304 Not Modified：表示内容未变化；此时不需重新保存 body，只需更新时间戳。
- 弱 ETag（前缀 `W/`）：语义是“语义上相同但不保证字节级一致”，条件请求仍可用，但需要保守处理。

缓存键与索引设计（详细补充）
- URL 规范化：统一 scheme/host 大小写、去除 fragment、保留 query；避免同资源被重复缓存。
- Vary 维度：在需多语种/多登录态的站点，缓存键可包含 `Accept-Language`、登录态标识或自定义维度以避免串缓存。
- SQLite 元数据表（推荐字段）：
  - `url`（主键）、`status`、`etag`、`last_modified`、`content_hash`、`size_bytes`、`ts_last_fetch`、`ts_last_valid`、`cache_path`。
- 内容文件：将 body 存储到磁盘（例如 `output/pages/`），并在 SQLite 中记录路径与哈希方便去重与校验。

写入与失效策略（详细补充）
- Read-Through：
  - 首次抓取：写入 body 与元数据（记录 `etag`/`last_modified`）。
  - 下次抓取：先查缓存，若存在则做条件 GET；304 时仅更新时间戳，不写 body；200 且变更时更新全部。
- TTL 与主动刷新：对缺少 `etag/last_modified` 的资源配置回退 TTL（例如 12h/24h），过期后强制刷新；支持 `stale-while-revalidate`（先返回旧缓存，后台刷新）。
- 负缓存：对短期稳定的 404/410，记录一个短 TTL（例如 1h），避免热区反复探测；对 500/429 不做负缓存，只做退避重试。
- 并发防抖：对同一 URL 加互斥锁（per-URL lock），避免缓存雪崩与写入竞态；返回正在刷新中的缓存或阻塞到刷新完成。

条件请求流程（建议实现，详细补充）
- 判定：从缓存读取 `etag` 或 `last_modified`，若不存在则执行普通 GET；存在则构造条件 GET。
- 请求：优先使用 `If-None-Match`（对大多数现代站点更可靠），若无 ETag 再用 `If-Modified-Since`。
- 响应处理：
  - 304：更新 `ts_last_fetch` 与 `ts_last_valid`，不写 body；保留旧 `content_hash` 与 `size_bytes`。
  - 200：保存新 body 与哈希、更新 `etag/last_modified` 与状态码；如 `Cache-Control: no-store` 可选择只保存元信息用于下次条件请求（视合规策略）。
  - 412 Precondition Failed：说明条件头不被接受，回退为普通 GET 并刷新缓存。

代码示例（httpx，带中文注释）

```python
import httpx

def conditional_get(url: str, cache: dict) -> tuple[int, bytes | None, dict]:
    """
    执行带条件请求的抓取：
    - 从 cache 读取已有 etag/last_modified
    - 携带条件头发起 GET
    - 根据 304/200 处理返回并更新 cache 元数据
    返回：(status, body_bytes_or_None, new_meta)
    """
    headers = {}
    etag = cache.get("etag")
    last_modified = cache.get("last_modified")
    if etag:
        headers["If-None-Match"] = etag
    elif last_modified:
        headers["If-Modified-Since"] = last_modified

    with httpx.Client(http2=True, timeout=30.0) as client:
        r = client.get(url, headers=headers)
        status = r.status_code
        meta = {
            "etag": r.headers.get("ETag") or etag,
            "last_modified": r.headers.get("Last-Modified") or last_modified,
            "content_type": r.headers.get("Content-Type"),
            "size_bytes": int(r.headers.get("Content-Length") or 0) or len(r.content or b""),
        }
        if status == 304:
            # 未变更：无需写入新 body，仅更新时间戳
            return 304, None, meta
        elif status == 200:
            # 已变更或不支持条件：保存新 body
            return 200, r.content, meta
        else:
            # 其他状态：返回以供上层做重试/降级处理
            return status, None, meta
```

实践建议（详细补充）
- 何时优先 ETag：大多数动态页面与 CDN 前置站点会可靠提供 ETag；当两者都提供时以 ETag 为主，失败时再退回 Last-Modified。
- 何时忽略服务器缓存语义：部分站点错误设置 `Cache-Control`；对于“仅做条件验证”场景，可以在合规前提下忽略 `no-cache`，但不持久化敏感页面的 body。
- 图片与静态资源：通常更适合 `If-None-Match`；命中 304 后可跳过图片重新下载，直接使用本地缓存文件。
- robots.txt 与 sitemap：单独缓存并设置较短 TTL（例如 1h），同时支持条件请求以快速验证变更。
- 变更探测：为重要页面记录 `content_hash`（如 SHA-256），即便缺少 ETag 也能比较 body 是否变更；用于触发下游处理流程。
- 采集频率控制：结合站点更新规律（例如新闻站在工作日白天频繁更新）动态调整 TTL 与重访策略。

并发与一致性（详细补充）
- 避免惊群：为同一 URL 请求加分布式/进程内锁；使用“正在刷新”的标记让其他协程直接返回旧缓存（stale-while-revalidate）。
- 写入原子性：先写临时文件，完成后原子替换（例如 `os.replace`）；避免并发读到半写入文件。
- 索引同步：先更新文件，再更新 SQLite；异常时回滚或标记不一致并触发补偿任务。
- 监控指标：命中率（hit ratio）、304 比例、平均体积节省、失败率、锁等待时间；用于调优 TTL 与并发策略。

常见坑与规避（详细补充）
- 服务器时间格式不标准：`Last-Modified` 解析失败时退回普通 GET 并记录一次；不要反复重试解析。
- 时区与时差：条件时间头必须使用 GMT 格式；避免本地时区导致条件判断错误。
- 错误的弱 ETag：部分站点每次都返回新弱 ETag，导致条件请求命中率低；此时用 `content_hash` 辅助比较。
- 代理与压缩：缓存时建议存放解压后的 body（文本），但保留 `Content-Encoding` 元信息；图片/二进制可原样存放。
- 带登录态页面：缓存键必须包含登录态信息（例如用户ID或会话标签），避免跨账号串缓存。

落地步骤清单（详细补充）
- 1）为抓取层增加缓存索引表与文件存储结构。
- 2）在抓取前读取缓存并决定是否发送 `If-None-Match` 或 `If-Modified-Since`。
- 3）在响应处理流程中分支处理 304/200，分别更新索引与文件。
- 4）为错误与超时增加退避与负缓存策略；对 500/429 按退避重试，不入负缓存。
- 5）实现 per-URL 锁与 stale-while-revalidate，避免并发踩踏。
- 6）为 robots.txt 与 sitemap 独立配置 TTL 与条件请求，做周期性刷新。
- 7）接入监控指标，按命中率与节省比例迭代优化 TTL 与策略。

五、会话与认证（Cookie/Bearer/CSRF）
- Cookie：使用 `requests.Session` 管理 Cookie（自动携带与更新），持久化到安全存储（加密或受权限控制）。
- Bearer Token：API 常用；注意生命周期与刷新流程（Refresh Token）。
- CSRF：表单提交或特定 POST 请求需要 `csrf_token`；抓取时需先获取页面中的 token 并随请求提交，注意 Referer 与 Origin。

六、连接与超时管理
- 连接超时（connect timeout）：DNS 解析与 TCP/TLS 建连阶段超时，建议短一些（如 3–5s）。
- 读取超时（read timeout）：等待响应体阶段超时，可稍长（如 10–20s），大文件或渲染页更长。
- 连接复用与池化：减少握手开销；通过 `HTTPAdapter` 配置最大连接数、重试策略，提升稳定性。

七、重试、退避与熔断
- 重试仅适用于幂等操作（GET/HEAD）；POST 等非幂等请求谨慎重试，除非 idempotent-key 或事务保障。
- 退避策略：指数退避（如 0.5s, 1s, 2s, 4s, 随机抖动）减少 thundering herd；注意与域级节流协调。
- 熔断：对某域或代理连续失败超过阈值时暂停一段时间，防止雪崩与被封。

八、代理池与健康检查
- 评分模型：综合成功率、平均延时、失败类型（超时/403/429/5xx）等维度，计算健康分；低分代理熔断。
- 地域与协议兼容：某些站点对地区或 HTTP/HTTPS 有差异；代理调度应按站点策略选择。

九、DNS、TLS 与协议
- DNS：解析失败与污染要考虑本地/DoH（DNS over HTTPS）替代；在企业环境中考虑固定解析。
- TLS：证书校验默认启用；如站点使用过期证书，应在合规与风险评估后决定是否关闭校验（不建议）。
- HTTP/2 与 HTTP/3：多路复用与更快握手；库支持有限时保持回退策略。

十、压缩、传输与大文件
- 压缩格式：`gzip/br` 常见；确保自动或手动解压；注意响应头 `Content-Encoding`。
- 流式下载：对大文件使用 `stream=True`，避免一次性加载到内存；配合断点续传与完整性校验。

十一、日志追踪与可观测
- 结构化日志：记录 URL、域名、状态码、耗时、重试次数、代理、UA、响应大小、错误栈。
- Trace-ID：为每个请求生成 `trace_id`，在日志中关联抓取/解析/写入，便于问题排查与复盘。

十二、伦理与合规边界
- robots.txt：不是法律约束但为行业礼仪与站点意愿体现；建议遵守并在争议场景进行沟通与授权。
- 账号与权限：不要突破访问权限或绕过认证；对受限资源遵守使用条款。

示例 A：条件请求与本地版本缓存（含详注）
```python
# -*- coding: utf-8 -*-
# 目的：使用 ETag/Last-Modified 进行增量抓取，避免重复下载；将版本信息持久化到 SQLite
import sqlite3
import requests
from pathlib import Path

class VersionStore:
    """
    将 URL 对应的 ETag/Last-Modified 持久化，支持增量抓取。
    注意：生产中应考虑并发安全与异常回滚。
    """
    def __init__(self, db_path: str):
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(db_path)
        self._init()
    def _init(self):
        cur = self.conn.cursor()
        cur.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            url TEXT PRIMARY KEY,
            etag TEXT,
            last_modified TEXT,
            updated_at TEXT
        )
        """)
        self.conn.commit()
    def get(self, url: str) -> tuple[str|None, str|None]:
        cur = self.conn.cursor()
        cur.execute("SELECT etag, last_modified FROM versions WHERE url=?", (url,))
        row = cur.fetchone()
        return (row[0], row[1]) if row else (None, None)
    def set(self, url: str, etag: str|None, last_modified: str|None):
        cur = self.conn.cursor()
        cur.execute(
            "INSERT OR REPLACE INTO versions(url, etag, last_modified, updated_at) VALUES(?,?,?,datetime('now'))",
            (url, etag, last_modified)
        )
        self.conn.commit()

def fetch_incremental(url: str, store: VersionStore):
    etag, last_modified = store.get(url)  # 读取本地版本
    headers = {"User-Agent": "Crawler/1.0"}
    if etag:
        headers["If-None-Match"] = etag  # 携带 ETag，实现协商缓存
    if last_modified:
        headers["If-Modified-Since"] = last_modified  # 携带 Last-Modified
    resp = requests.get(url, headers=headers, timeout=(5, 20))  # 连接 5s，读取 20s
    if resp.status_code == 304:
        # 未修改，跳过解析与写入
        return None
    # 更新本地版本信息
    new_etag = resp.headers.get("ETag")
    new_last = resp.headers.get("Last-Modified")
    store.set(url, new_etag, new_last)
    return resp
```

示例 B：稳健请求封装（重试/退避/日志）（含详注）
```python
# -*- coding: utf-8 -*-
# 目的：封装 GET 请求，提供重试与指数退避、结构化日志、区分幂等场景
import time
import json
import requests

def robust_get(url: str, headers: dict | None = None, retries: int = 3, base_delay: float = 0.5, timeout: tuple=(5, 20)):
    headers = headers or {"User-Agent": "Crawler/1.0", "Accept": "text/html,application/json"}
    for attempt in range(1, retries+1):
        start = time.time()
        try:
            resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
            latency = time.time() - start
            # 结构化日志：在生产中写入到日志系统
            print(json.dumps({
                "url": url,
                "status": resp.status_code,
                "latency": round(latency, 3),
                "attempt": attempt,
                "size": int(resp.headers.get('Content-Length') or len(resp.content))
            }, ensure_ascii=False))
            if resp.status_code >= 500:
                # 服务端错误，继续重试
                raise requests.RequestException(f"server error {resp.status_code}")
            return resp  # 对于 4xx，视站点策略决定是否继续；这里返回由上层判断
        except requests.RequestException as e:
            # 指数退避 + 随机抖动（简化版：无抖动）
            delay = base_delay * (2 ** (attempt-1))
            time.sleep(delay)
            if attempt == retries:
                print(json.dumps({"url": url, "error": str(e), "attempt": attempt}, ensure_ascii=False))
                return None
    return None
```

示例 C：代理池健康检查与评分（含详注）
```python
# -*- coding: utf-8 -*-
# 目的：维护一个简单的代理池，按成功率与延时打分；对低分代理进行熔断
import time
import random
import requests

class Proxy:
    def __init__(self, addr: str):
        self.addr = addr
        self.success = 0
        self.fail = 0
        self.latency_avg = None
        self.circuit_open_until = 0  # 熔断截止时间戳
    def score(self) -> float:
        # 简化评分：成功率优先，其次延时；分数越高越好
        total = self.success + self.fail
        sr = (self.success / total) if total else 0.0
        la = self.latency_avg if self.latency_avg is not None else 1.0
        return sr - 0.1 * la
    def available(self) -> bool:
        return time.time() >= self.circuit_open_until

class ProxyPool:
    def __init__(self, proxies: list[str]):
        self.pool = [Proxy(p) for p in proxies]
    def pick(self) -> Proxy | None:
        # 选择可用且评分最高的代理
        candidates = [p for p in self.pool if p.available()]
        return max(candidates, key=lambda p: p.score(), default=None)
    def report(self, proxy: Proxy, success: bool, latency: float | None):
        if success:
            proxy.success += 1
            if latency is not None:
                if proxy.latency_avg is None:
                    proxy.latency_avg = latency
                else:
                    # 指数滑动平均，平滑延时
                    proxy.latency_avg = 0.7 * proxy.latency_avg + 0.3 * latency
        else:
            proxy.fail += 1
            # 连续失败达到阈值则熔断（简化：失败一次即短暂熔断）
            proxy.circuit_open_until = time.time() + 30  # 熔断 30 秒

def get_with_proxy(url: str, pool: ProxyPool):
    proxy = pool.pick()
    if not proxy:
        return robust_get(url)
    start = time.time()
    try:
        resp = requests.get(url, headers={"User-Agent": "Crawler/1.0"}, timeout=(5, 20), proxies={"http": proxy.addr, "https": proxy.addr})
        latency = time.time() - start
        pool.report(proxy, True, latency)
        return resp
    except requests.RequestException:
        pool.report(proxy, False, None)
        return None
```

示例 D：会话登录与 CSRF 处理（含详注）
```python
# -*- coding: utf-8 -*-
# 目的：演示如何使用 Session 管理 Cookie，获取 CSRF token 并提交登录表单
import re
import requests

def login(base_url: str, username: str, password: str) -> requests.Session | None:
    sess = requests.Session()
    sess.headers.update({"User-Agent": "Crawler/1.0", "Accept": "text/html"})
    # 第一步：访问登录页，拿到 CSRF token（假设在隐藏域或脚本中）
    resp = sess.get(base_url + "/login", timeout=(5, 20))
    if resp.status_code != 200:
        return None
    m = re.search(r'name="csrf_token" value="([^"]+)"', resp.text)
    csrf = m.group(1) if m else None
    if not csrf:
        # 有些站点在 meta/script 中存放 token；此处仅为示意
        return None
    # 第二步：提交表单
    data = {"username": username, "password": password, "csrf_token": csrf}
    resp2 = sess.post(base_url + "/login", data=data, headers={"Referer": base_url + "/login"}, timeout=(5, 20))
    if resp2.status_code in (302, 303) or "dashboard" in resp2.url:
        # 登录成功的常见信号：跳转到主页或带有特征路径
        return sess
    return None
```

示例 E：HTTPAdapter 配置连接池与重试（含详注）
```python
# -*- coding: utf-8 -*-
# 目的：通过 urllib3 Retry 与 requests 的 HTTPAdapter 配置连接池与自动重试
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def build_session(max_retries: int = 3, backoff_factor: float = 0.5, pool_maxsize: int = 50) -> requests.Session:
    sess = requests.Session()
    retry = Retry(
        total=max_retries,
        read=max_retries,
        connect=max_retries,
        status=max_retries,
        status_forcelist=[500, 502, 503, 504],
        backoff_factor=backoff_factor,
        raise_on_status=False,
        allowed_methods=["GET", "HEAD"]  # 幂等方法
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=pool_maxsize, pool_maxsize=pool_maxsize)
    sess.mount('http://', adapter)
    sess.mount('https://', adapter)
    sess.headers.update({"User-Agent": "Crawler/1.0"})
    return sess
```

综上：通过以上策略与示例代码，你可以在复杂网络环境下实现稳健抓取。建议优先实现“可观测与幂等增量”，再逐步接入代理池、熔断、适配 HTTP/2/3 与压缩，并结合站点策略实施礼貌抓取与合规访问。

—

## 专题 02：HTML 解析深入（CSS/XPath/选择器容错/正则增强）
本专题系统阐述 HTML 解析在通用爬虫中的工程落地，包括解析库选型权衡、选择器稳定性设计、容错与回退体系、文本清洗与标准化、链接归一化与过滤、半结构化字段的正则增强、复杂结构（表格/嵌套列表）解析、分页与异步加载识别、日期与时区本地化、结构化数据抽取（JSON-LD/microdata）、模板解析与版本管理、性能与内存优化、可测试性与回归、常见陷阱与对策。并提供若干带详尽中文注释的示例代码，帮助你直接集成到项目中。

一、解析库选型与权衡
- BeautifulSoup（bs4）：
  - 优点：易用、容错强、选择器（CSS）直观；适合初学与复杂容错场景。
  - 缺点：性能一般、XPath 支持不直接（需 lxml 解析器配合）。
- lxml + XPath：
  - 优点：速度快、内存占用相对可控；XPath 强大且表达能力高；适合高性能与复杂结构匹配。
  - 缺点：学习曲线略陡；XPath 书写需要规范与复用。
- parsel（Scrapy 的解析组件）：
  - 优点：统一提供 CSS/XPath 接口、链式选择；API 友好且稳定；可与 requests/aiohttp 单独使用。
  - 缺点：引入额外依赖；在极端性能场景仍建议直接 lxml。
选型建议：
- 小型爬虫或内容变化频繁的站点，优先 bs4 + CSS 选择器，辅以容错备选；
- 大量页面与复杂结构，优先 lxml + XPath；
- 希望统一接口与简洁链式选择，parsel 是很好的折衷。

二、选择器设计原则（稳定性优先）
- 最小可稳定子树：优先在局部容器内选择（例如 `.article` 内的 `h1.title`），减少全局依赖。
- 语义化标签优先：`h1`/`time`/`article`/`nav` 等语义标签更加稳定；
- 属性选择优先：使用稳定属性（如 `data-*`、`itemprop`、`aria-*`）比 class 名在部分站点更可靠；
- 避免脆弱选择：尽量避免 `:nth-child()` 与过度依赖层级，因为 DOM 结构微调会导致失效；
- 多路径备选：为关键字段（标题/正文/时间/作者）提供 2–4 条备选选择器，按优先级依次尝试，并记录命中路径用于回归。

三、容错与回退体系
- 逐条尝试：将多条选择器按优先级依次尝试，命中立即返回；未命中则回退到正则或文本启发式。
- 站点/模板分流：根据 URL 模式或 DOM 特征选择解析器（模板 A/B/C），在解析失败时自动尝试其他模板。
- 版本管理：记录“解析版本号”和“命中选择器”到数据存储，DOM 变化时便于快速定位与修正。

四、文本清洗与标准化
- 空白与换行：统一空白字符为普通空格，剔除重复换行；保留段落边界（例如将 `p` 换行转为 `\n`）。
- 去冗余元素：移除广告位、脚注、社交分享组件、脚本与样式标签；避免乱码与垃圾文本污染。
- 编码统一：优先使用响应头 `charset`，否则启用编码探测（参考专题 13），统一为 `utf-8`。

五、链接归一化与过滤
- 使用 `urljoin` 将相对链接转为绝对；剔除片段标识（`#...`）与统计参数（如 `utm_*`）。
- 域名过滤：仅保留目标域或白名单域；防止跨站抓取导致越界。
- 规范化规则与唯一键：对 URL 进行规则化（协议、子域、路径归一），作为去重键。

六、半结构化字段的正则增强
- 编号/ID：如“文章编号：ABC-123”，使用正则 `r"[A-Z]{2,}-\d+"` 抽取。
- 日期时间：针对多格式（`YYYY-MM-DD`/`DD.MM.YYYY`/中文日期），使用正则识别再交给日期解析库统一。
- 价格与数量：匹配带货币符号与小数的模式，注意千分位与本地化。

七、复杂结构解析（表格/嵌套列表/富文本）
- 表格：按 `tr`/`td` 抽取并映射到字段名；处理合并单元格（`rowspan`/`colspan`）。
- 嵌套列表：递归解析层级，提取层级结构（如评论楼层）。
- 富文本：处理图片、链接与引用；根据需求保留简单内联标签或纯文本。

八、分页与异步加载识别
- 分页：识别“下一页”链接（`rel="next"`/文本“下一页”/数字按钮）；构建迭代器抓取所有页。
- 异步加载：识别 `data-api` 或内嵌 JSON（Ajax 返回）；优先直接抓取接口而不是渲染 HTML。

九、日期、时区与本地化
- 使用 `dateparser` 统一解析多语言日期；统一输出 ISO 8601（UTC 或指定时区）。
- 提前定义站点时区与格式偏好，避免跨站数据混乱。

十、结构化数据（JSON-LD/microdata）
- JSON-LD：从 `<script type="application/ld+json">` 中解析结构化数据（如 `Article`/`Product`）。
- microdata：基于 `itemprop` 属性提取结构化字段。

十一、模板化解析与版本管理
- 基于 URL 模式（例如 `/news/` vs `/blog/`）或 DOM 特征（有无 `article` 容器）选择解析模板；
- 每次解析记录“命中模板名/选择器路径”，DOM 变化时快速定位。

十二、性能与内存优化
- 复用解析器：在批量处理时复用 lxml 的解析器对象；预编译 XPath。
- 减少不必要的查找：先定位容器，再在容器内做选择。
- 流式处理：大页面避免一次性构建全量对象，按需解析。

十三、可测试性与回归
- 对关键解析函数编写单元测试；保存 HTML 快照用于回归（站点改版时直接复跑测试）。
- 在日志中记录命中选择器与解析结果长度/摘要，帮助发现“半成功”问题（标题解析成功但正文为空）。

十四、常见陷阱与对策
- “软 404”：返回 200 但提示内容不存在；通过正文关键字识别并标记。
- “反爬占位”：内容区域为空或替换为占位文本；结合网络与反爬专题策略，必要时使用浏览器渲染。
- “文本污染”：脚注/广告位混入正文；清洗黑名单容器（如 `.share`, `.advert`, `.footer`）。

示例 A：选择器容错与文本清洗（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 BeautifulSoup 进行容错选择与清洗正文
from bs4 import BeautifulSoup
import re

TITLE_SELECTORS = [
    "h1.article-title",
    "header .title",
    ".post h1",
    "article h1",
]
DATE_SELECTORS = [
    "time[datetime]",
    ".pub-date",
    "span.date",
]
CONTENT_CONTAINER_SELECTORS = [
    "div.article-content",
    "article .content",
    "section.content",
]

BLACKLIST_SELECTORS = [
    ".share", ".advert", "script", "style", "noscript", ".footer", ".sidebar"
]

def normalize_whitespace(text: str) -> str:
    # 将各种空白字符归一化为单个空格，保留段落换行
    text = re.sub(r"\r\n|\r", "\n", text)
    text = re.sub(r"\u00A0", " ", text)  # 不间断空格转普通空格
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def pick_first_text(soup: BeautifulSoup, selectors: list[str]) -> str:
    # 逐条尝试选择器，命中后获取文本；time 标签优先 datetime 属性
    for sel in selectors:
        node = soup.select_one(sel)
        if node:
            if node.name == "time" and node.has_attr("datetime"):
                return node.get("datetime").strip()
            return node.get_text(strip=True)
    return ""

def clean_container(node) -> str:
    # 移除黑名单元素，保留正文容器内文本（按段落换行）
    for bad in BLACKLIST_SELECTORS:
        for n in node.select(bad):
            n.decompose()
    text_parts = []
    for p in node.select("p, h2, h3, li"):
        t = p.get_text(" ", strip=True)
        if t:
            text_parts.append(t)
    return normalize_whitespace("\n".join(text_parts))

def parse_article(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    title = pick_first_text(soup, TITLE_SELECTORS)
    date_raw = pick_first_text(soup, DATE_SELECTORS)
    content = ""
    for sel in CONTENT_CONTAINER_SELECTORS:
        node = soup.select_one(sel)
        if node:
            content = clean_container(node)
            break
    return {"title": title, "date": date_raw, "content": content}
```

示例 B：lxml + XPath 高性能解析（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 lxml 提供高性能 XPath 解析；适合批量抓取与复杂结构
from lxml import html

X_TITLE = [
    "//h1[@class='article-title']/text()",
    "//header//h1[contains(@class,'title')]/text()",
    "//article//h1/text()",
]
X_DATE = [
    "//time[@datetime]/@datetime",
    "//*[contains(@class,'pub-date')]/text()",
]
X_CONTENT_CONTAINER = [
    "//div[contains(@class,'article-content')]",
    "//article//*[contains(@class,'content')]",
]
X_BLACKLIST = [
    "//*[contains(@class,'share')]",
    "//*[contains(@class,'advert')]",
    "//script", "//style", "//noscript",
    "//*[contains(@class,'footer')]",
    "//*[contains(@class,'sidebar')]",
]

def xp_first(tree, paths):
    for p in paths:
        r = tree.xpath(p)
        if r:
            # r 可能是列表（文本或节点），统一取第一个并清洗
            v = r[0]
            if isinstance(v, str):
                return v.strip()
            return v.text_content().strip()
    return ""

def xp_clean_container(node) -> str:
    for p in X_BLACKLIST:
        for b in node.xpath(p):
            b.getparent().remove(b)
    # 收集段落文本
    paras = []
    for p in node.xpath('.//p|.//h2|.//h3|.//li'):
        t = p.text_content().strip()
        if t:
            paras.append(t)
    return "\n".join(paras)

def parse_article_xpath(html_text: str) -> dict:
    tree = html.fromstring(html_text)
    title = xp_first(tree, X_TITLE)
    date_raw = xp_first(tree, X_DATE)
    content = ""
    for p in X_CONTENT_CONTAINER:
        nodes = tree.xpath(p)
        if nodes:
            content = xp_clean_container(nodes[0])
            break
    return {"title": title, "date": date_raw, "content": content}
```

示例 C：parsel 统一接口（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 parsel 提供 CSS/XPath 统一选择接口，链式调用更直观
from parsel import Selector

def parse_with_parsel(html_text: str) -> dict:
    sel = Selector(text=html_text)
    title = (
        sel.css('h1.article-title::text').get()
        or sel.css('header .title::text').get()
        or sel.xpath('//article//h1/text()').get()
        or ''
    ).strip()
    date_raw = (
        sel.css('time[datetime]::attr(datetime)').get()
        or sel.css('.pub-date::text').get()
        or ''
    ).strip()
    content_node = (
        sel.css('div.article-content').get() or sel.css('article .content').get()
    )
    content = ''
    if content_node:
        sub = Selector(text=content_node)
        # 清理黑名单元素后收集文本段落
        for bad in ['.share', '.advert', 'script', 'style', 'noscript', '.footer', '.sidebar']:
            for n in sub.css(bad):
                # parsel 不直接支持删除节点，这里改用“忽略黑名单选择器收集”的策略
                pass
        paras = [p.get().strip() for p in sub.css('p::text, h2::text, h3::text, li::text') if p.get().strip()]
        content = '\n'.join(paras)
    return {"title": title, "date": date_raw, "content": content}
```

示例 D：JSON-LD 结构化数据提取（含详注）
```python
# -*- coding: utf-8 -*-
# 从 <script type="application/ld+json"> 中提取结构化数据，解析出 Article/NewsArticle 字段
import json
from bs4 import BeautifulSoup

def extract_jsonld(html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    for script in soup.find_all('script', type='application/ld+json'):
        try:
            data = json.loads(script.string)
        except Exception:
            continue
        # data 可能是对象或数组；选出 Article/NewsArticle
        candidates = data if isinstance(data, list) else [data]
        for obj in candidates:
            t = obj.get('@type') or obj.get('type')
            if isinstance(t, list):
                t = t[0]
            if t and t.lower() in ('article', 'newsarticle'):
                return {
                    'title': obj.get('headline') or obj.get('name') or '',
                    'date': obj.get('datePublished') or obj.get('dateCreated') or '',
                    'author': (obj.get('author') or {}).get('name', '') if isinstance(obj.get('author'), dict) else (obj.get('author') or ''),
                }
    return {}
```

示例 E：链接归一化与过滤（含详注）
```python
# -*- coding: utf-8 -*-
# 绝对化、去片段与统计参数、域名过滤；作为去重键的基础
from urllib.parse import urljoin, urlparse, urlunparse, parse_qsl, urlencode

STRIP_PARAMS = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content'}

def normalize_link(base_url: str, href: str, allow_domains: set[str]) -> str | None:
    if not href:
        return None
    abs_url = urljoin(base_url, href)
    parsed = urlparse(abs_url)
    if parsed.netloc not in allow_domains:
        return None
    # 去掉片段 (#...)
    parsed = parsed._replace(fragment='')
    # 移除统计参数
    q = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True) if k not in STRIP_PARAMS]
    parsed = parsed._replace(query=urlencode(q))
    return urlunparse(parsed)
```

示例 F：日期解析与统一（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 dateparser 解析多格式日期，统一输出 ISO 8601
from dateparser import parse
from datetime import timezone

def normalize_date(date_raw: str, tz='Asia/Shanghai') -> str:
    dt = parse(date_raw, settings={'TIMEZONE': tz, 'RETURN_AS_TIMEZONE_AWARE': True})
    if not dt:
        return ''
    # 统一为 ISO8601 字符串
    return dt.astimezone(timezone.utc).isoformat()
```

示例 G：简单正文抽取器（启发式评分）（含详注）
```python
# -*- coding: utf-8 -*-
# 在没有结构化容器的页面中，使用启发式对段落进行评分，抽取正文
from bs4 import BeautifulSoup

def extract_main_text(html: str) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    candidates = []
    for div in soup.select('div, article, section'):  # 候选容器集合
        text = div.get_text('\n', strip=True)
        length = len(text)
        # 简单评分：长度与段落数量加权
        paras = text.count('\n') + 1
        score = length * 0.7 + paras * 10
        candidates.append((score, text))
    candidates.sort(reverse=True)
    return candidates[0][1] if candidates else ''
```

示例 H：表格抽取到键值（含详注）
```python
# -*- coding: utf-8 -*-
# 将 <table> 转换为结构化列表或字典；处理合并单元格示意（简化）
from bs4 import BeautifulSoup

def extract_table(html: str) -> list[dict]:
    soup = BeautifulSoup(html, 'html.parser')
    table = soup.select_one('table')
    if not table:
        return []
    rows = []
    headers = [th.get_text(strip=True) for th in table.select('thead th')] or [td.get_text(strip=True) for td in table.select('tr:first-child td')]
    for tr in table.select('tbody tr') or table.select('tr')[1:]:
        cells = [td.get_text(strip=True) for td in tr.select('td')]
        row = {}
        for i, h in enumerate(headers):
            row[h or f'col_{i}'] = cells[i] if i < len(cells) else ''
        rows.append(row)
    return rows
```

示例 I：分页识别与迭代（含详注）
```python
# -*- coding: utf-8 -*-
# 识别“下一页”链接，迭代抓取所有页（简化）
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def find_next_page(html: str, base_url: str) -> str | None:
    soup = BeautifulSoup(html, 'html.parser')
    # rel="next" 优先，其次文本匹配
    a = soup.select_one('a[rel="next"]')
    if not a:
        a = soup.find('a', string=lambda s: s and ('下一页' in s or 'Next' in s))
    return urljoin(base_url, a.get('href')) if a and a.get('href') else None

def iterate_pages(start_url: str, fetch_html_func):
    url = start_url
    while url:
        html = fetch_html_func(url)
        if not html:
            break
        yield url, html
        url = find_next_page(html, url)
```

示例 J：正则增强与性能注意（含详注）
```python
# -*- coding: utf-8 -*-
# 对半结构化字段（编号/价格）进行正则抽取，注意预编译与多格式容错
import re

PAT_ID = re.compile(r"[A-Z]{2,}-\d+", re.I)
PAT_PRICE = re.compile(r"(?:(?:USD|\$|￥)\s*)?([0-9]{1,3}(?:,[0-9]{3})*(?:\.[0-9]{2})?)")

def match_id(text: str) -> str:
    m = PAT_ID.search(text or '')
    return m.group(0) if m else ''

def match_price(text: str) -> float | None:
    m = PAT_PRICE.search(text or '')
    if not m:
        return None
    v = m.group(1).replace(',', '')
    try:
        return float(v)
    except Exception:
        return None
```

总结：HTML 解析的核心在于“稳定、容错、可维护”。在站点变化与反爬策略并存的现实条件下，选择器设计要以稳定性为首、容错与回退体系要完善、文本与链接标准化要细致，同时通过模板化解析与版本管理降低维护成本。解析逻辑必须可测试、可回归，并在性能与内存方面进行必要优化，从而在长期抓取任务中保持质量与效率。

—

## 专题 03：反爬与对策（礼貌抓取、UA/代理轮换、验证码、指纹）
本专题从工程实战出发，系统整理反爬策略与应对手段，强调“礼貌抓取、动态控制、最小暴露、可观测与合规”。核心理念是：能不渲染就不渲染，能不伪装就不伪装；尽量走接口与静态路径；当必须面对挑战与指纹时，用温和、稳定、可解释的方法达成长期抓取。

一、礼貌抓取与动态限速
- 域级最小间隔与并发上限：为每个域设置最小请求间隔（如 0.5–2s）与并发上限（如 1–3），防止瞬时压测。
- 动态限速：根据近期失败率、`429/403/5xx` 信号自动调节间隔；恢复期采用“慢启动”。
- 指数退避：网络错误与 5xx 使用指数退避；`429` 适当加长退避窗口（如 60–120s）。
- 时段策略：避免站点高峰时段抓取；必要时夜间批量抓取。

示例 A：域级动态限速器（含详注）
```python
# -*- coding: utf-8 -*-
# 目标：按域名动态控制请求节奏。记录状态码与失败率，动态调整延迟，维持“慢而稳”。
import time
from collections import deque, defaultdict
from urllib.parse import urlparse

class DomainThrottler:
    def __init__(self, base_delay: float = 0.5, min_delay: float = 0.2, max_delay: float = 5.0, window: int = 50):
        self.base_delay = base_delay
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.window = window
        self.last_time = defaultdict(float)
        self.stats = defaultdict(lambda: deque(maxlen=window))
        self.delay = defaultdict(lambda: base_delay)
    def record(self, url: str, status: int | None, success: bool):
        dom = urlparse(url).netloc
        self.stats[dom].append((status, success, time.time()))
        codes = [s for s, ok, _ in self.stats[dom] if s is not None]
        fail_rate = 1.0 - (sum(1 for s, ok, _ in self.stats[dom] if ok) / max(1, len(self.stats[dom])))
        penalty = 0.0
        penalty += 0.5 * sum(1 for c in codes if c == 429)
        penalty += 0.2 * sum(1 for c in codes if c == 403)
        penalty += 0.1 * sum(1 for c in codes if 500 <= c < 600)
        target = self.base_delay * (1 + penalty) * (1 + fail_rate)
        self.delay[dom] = max(self.min_delay, min(self.max_delay, target))
    def wait(self, url: str):
        dom = urlparse(url).netloc
        now = time.time()
        since = now - self.last_time[dom]
        need = self.delay[dom]
        if since < need:
            time.sleep(need - since)
        self.last_time[dom] = time.time()
```

二、UA/Headers 与会话策略
- UA 轮换：准备常见桌面浏览器 UA 集合；减少“急变指纹”引发的异常。
- 头部组合：合理设置 `Accept`/`Accept-Language`/`Referer`/`Origin`；避免矛盾或过度头部。
- 会话管理：使用 `requests.Session` 或浏览器上下文，正确维护 Cookie 与 CSRF；防止 `SameSite` 影响跨域提交。
- 内容协商：按返回类型配置 `Accept`，减少“占位页”与错误降级。

示例 B：UA/Headers 轮换策略（含详注）
```python
# -*- coding: utf-8 -*-
import random

UA_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:118.0) Gecko/20100101 Firefox/118.0",
]

def build_headers(domain: str, prefer_json: bool = False) -> dict:
    ua = random.choice(UA_POOL)
    accept = "application/json, text/plain, */*" if prefer_json else "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    return {
        "User-Agent": ua,
        "Accept": accept,
        "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
    }
```

三、代理池与健康评分（进阶）
- 成功率与延时评分：按域名统计；对 `403/429` 分类加权惩罚。
- 熔断与降级：低分代理熔断一段时间；错误类型影响熔断时长（如 `429` 更长）。
- 地域/协议兼容：按站点偏好选择地区；确保 HTTP/HTTPS 与 TLS/SNI 兼容。
- 连接预热：新代理先健康探测再纳入生产使用。

示例 C：失败分类的代理评分（含详注）
```python
# -*- coding: utf-8 -*-
import time
import requests
from urllib.parse import urlparse

class ProxyStat:
    def __init__(self):
        self.success = 0
        self.fail_timeout = 0
        self.fail_403 = 0
        self.fail_429 = 0
        self.fail_5xx = 0
        self.latency_avg = None
        self.circuit_open_until = 0
    def score(self):
        total_fail = self.fail_timeout + self.fail_403 + self.fail_429 + self.fail_5xx
        total = self.success + total_fail
        sr = (self.success / total) if total else 0.0
        penalty = 0.3*self.fail_429 + 0.2*self.fail_403 + 0.1*self.fail_5xx + 0.2*self.fail_timeout
        la = self.latency_avg if self.latency_avg is not None else 1.0
        return sr - 0.05*penalty - 0.1*la

class DomainProxyPool:
    def __init__(self, proxies: list[str]):
        self.stats = {}
        self.proxies = proxies[:]
        self.domain_pref = {}
    def _get_stat(self, addr: str) -> ProxyStat:
        if addr not in self.stats:
            self.stats[addr] = ProxyStat()
        return self.stats[addr]
    def pick(self, domain: str) -> str | None:
        candidates = self.domain_pref.get(domain, self.proxies)
        best = None; best_score = -1e9
        now = time.time()
        for p in candidates:
            st = self._get_stat(p)
            if now < st.circuit_open_until:
                continue
            sc = st.score()
            if sc > best_score:
                best, best_score = p, sc
        return best
    def report(self, addr: str, success: bool, status: int | None, latency: float | None):
        st = self._get_stat(addr)
        if success:
            st.success += 1
            if latency is not None:
                st.latency_avg = latency if st.latency_avg is None else (0.7*st.latency_avg + 0.3*latency)
        else:
            if status is None:
                st.fail_timeout += 1
            elif status == 429:
                st.fail_429 += 1; st.circuit_open_until = time.time() + 120
            elif status == 403:
                st.fail_403 += 1; st.circuit_open_until = time.time() + 60
            elif 500 <= status < 600:
                st.fail_5xx += 1
            else:
                st.fail_timeout += 1

def get_via_proxy(url: str, pool: DomainProxyPool):
    dom = urlparse(url).netloc
    addr = pool.pick(dom)
    if not addr:
        return None
    start = time.time()
    try:
        r = requests.get(url, headers={"User-Agent": "Crawler/1.0"}, timeout=(5, 20), proxies={"http": addr, "https": addr})
        pool.report(addr, True, r.status_code, time.time()-start)
        return r
    except requests.RequestException:
        pool.report(addr, False, None, None)
        return None
```

四、挑战与验证码（Cloudflare/人机验证）
- JS/行为挑战：Cloudflare/Incapsula 等可能进行“行为与环境”检查；浏览器渲染更容易通过。
- reCAPTCHA/hCaptcha：在合规前提下使用第三方识别；否则尽量绕过挑战入口或降低频率避开。
- 白名单与授权：企业场景优先沟通开放接口或白名单；这是最稳的解决方案。

示例 D：Playwright “挑战等待”与基础伪装（含详注）
```python
# -*- coding: utf-8 -*-
from playwright.sync_api import sync_playwright

def render_with_stealth(url: str, selector_to_wait: str | None = None) -> str | None:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            locale='zh-CN', timezone_id='Asia/Shanghai',
            viewport={'width': 1366, 'height': 768},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36'
        )
        context.add_init_script(
            """
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
            Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh']});
            """
        )
        page = context.new_page()
        try:
            page.route('**/*', lambda route: route.continue_())
            page.goto(url, wait_until='domcontentloaded')
            try:
                page.wait_for_selector('div#cf-challenge-running, div.challenge-form', timeout=5000)
                page.wait_for_load_state('networkidle', timeout=15000)
            except Exception:
                pass
            page.evaluate("window.scrollTo(0, document.body.scrollHeight/3)"); page.wait_for_timeout(500)
            if selector_to_wait:
                page.wait_for_selector(selector_to_wait, timeout=15000)
            return page.content()
        finally:
            context.close(); browser.close()
```

示例 E：验证码模块化与合规（含详注）
```python
# -*- coding: utf-8 -*-
import typing as t

class CaptchaSolver:
    def solve(self, image_bytes: bytes, meta: dict) -> str | None:
        """在合规前提下返回识别结果；默认返回 None。"""
        return None

def handle_captcha(page, solver: CaptchaSolver) -> bool:
    try:
        img = page.query_selector('img.captcha')
        if not img:
            return True
        b = img.screenshot()
        ans = solver.solve(b, {"site": page.url})
        if ans:
            input_box = page.query_selector('input[name=captcha]')
            if input_box:
                input_box.fill(ans)
                return True
        return False
    except Exception:
        return False
```

五、指纹对抗与渲染策略
- 指纹维度：`navigator.webdriver`、`plugins`、`languages`、Canvas/Audio/WebGL、RTC、字体、时区、屏幕、UA、TLS/JA3、IP/ASN。
- 最小暴露：不要频繁改变全部指纹；在“可信参数集”内温和轮换，避免异常曲线。
- 渲染策略：仅在必要时启用渲染；优先接口抓取，减少指纹暴露与成本。
- 资源隔离：阻断第三方分析脚本（GA/广告），降低指纹采集与风控概率。

示例 F：行为模拟与第三方资源阻断（含详注）
```python
# -*- coding: utf-8 -*-
from playwright.sync_api import sync_playwright

def render_with_behavior(url: str) -> str | None:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(locale='zh-CN', timezone_id='Asia/Shanghai')
        page = context.new_page()
        BLOCK_PATTERNS = ['google-analytics.com', 'googletagmanager.com', 'doubleclick.net']
        def route_filter(route):
            url = route.request.url
            if any(b in url for b in BLOCK_PATTERNS):
                return route.abort()
            return route.continue_()
        page.route('**/*', route_filter)
        try:
            page.goto(url, wait_until='domcontentloaded')
            page.evaluate("window.scrollTo(0, document.body.scrollHeight*0.2)"); page.wait_for_timeout(500)
            page.evaluate("window.scrollTo(0, document.body.scrollHeight*0.6)"); page.wait_for_timeout(500)
            page.wait_for_load_state('networkidle')
            return page.content()
        finally:
            context.close(); browser.close()
```

六、请求签名与接口策略
- 接口优先：许多页面数据来自 Ajax/GraphQL/REST；直接抓接口更稳定与高效。
- 签名参数：可能存在 HMAC/MD5/时间戳混淆；需合法授权或基于文档复用签名逻辑。
- 密钥安全：密钥使用最小权限与安全存储（ENV/KMS），避免泄露。

示例 G：HMAC 签名示意（含详注）
```python
# -*- coding: utf-8 -*-
import hmac, hashlib

def build_signature(secret: bytes, payload: str) -> str:
    return hmac.new(secret, payload.encode('utf-8'), hashlib.sha256).hexdigest()
```

七、失败与风险管理
- 失败分类：网络超时、5xx、403、429、软 404、占位页、渲染失败、验证码未过、会话失效。
- 风险等级：维护站点风险分（低/中/高）；高风险站点自动降速或暂停。
- 暂停/解封：失败率超阈值暂停一段时间；必要时人工介入或沟通授权。
- 日志审计：结构化记录失败类型与响应体摘要，辅助策略迭代。

示例 H：失败分类与站点暂停（含详注）
```python
# -*- coding: utf-8 -*-
import time
from collections import defaultdict
from urllib.parse import urlparse

class RiskManager:
    def __init__(self, pause_secs: int = 600):
        self.fail_counts = defaultdict(lambda: defaultdict(int))
        self.paused_until = defaultdict(int)
        self.pause_secs = pause_secs
    def record_failure(self, url: str, fail_type: str):
        d = urlparse(url).netloc
        self.fail_counts[d][fail_type] += 1
        total_recent = sum(self.fail_counts[d].values())
        if total_recent > 20:
            self.paused_until[d] = time.time() + self.pause_secs
    def is_paused(self, domain: str) -> bool:
        return time.time() < self.paused_until[domain]
```

八、陷阱与误导链接识别
- Honeypot：隐藏且只有爬虫点击的链接或陷阱页；需要可见性判断与黑名单过滤。
- 诱导参数：如 `trap=1`；过滤避免抓取。
- 可见性与 CSS：判断 `display:none`/`visibility:hidden` 等特征，减少机器人行为特征。

示例 I：简单 Honeypot 过滤（含详注）
```python
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup

def is_visible_link(a) -> bool:
    cls = a.get('class') or []
    style = (a.get('style') or '').lower()
    if 'display:none' in style or 'visibility:hidden' in style:
        return False
    if isinstance(cls, list) and any(c in ('hidden', 'invisible') for c in cls):
        return False
    return True

def filter_links(html: str, base_url: str) -> list[str]:
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for a in soup.select('a[href]'):
        href = a.get('href')
        if not href or 'trap=' in href:
            continue
        if not is_visible_link(a):
            continue
        links.append(href)
    return links
```

九、合规与成本评估
- 合规优先：尊重站点策略与法律边界；账号与受限数据需授权。
- 成本衡量：渲染与代理成本高；评估接口抓取与缓存复用的替代性。
- 放弃与替代：当反爬强度过高或风险不可控时考虑放弃或合作。

总结：
- 反爬核心工程策略是“礼貌抓取 + 动态控制 + 最小暴露 + 可观测 + 风险管理”。
- 优先静态与接口抓取，必要时才引入浏览器渲染与指纹对抗；在合规边界内行动。
- 使用域级限速、失败分类暂停与代理健康评分，维持长期稳定性与数据质量。

—

## 专题 04：调度与并发（Frontier 队列、域级限流、优先级、异步 I/O）
本专题从“队列/优先级/限流/并发模型/回压/优雅停止/可观测性”六个维度系统构建稳定的抓取调度与并发体系。理念：以领域公平与资源约束为前提，使用轻量、可观测、可回退的调度策略，实现“慢而稳”的持续抓取。

一、总体架构与职责划分
- 核心角色：
  - Frontier（URL 优先队列）：承接发现入口、去重、优先级评分、取出下一个待抓条目。
  - Scheduler（调度器）：域级限速与公平分配；把 URL 派发给抓取 Worker。
  - Fetcher（抓取器）：HTTP 抓取（优先接口/静态）；记录延时与状态码。
  - Parser（解析器）：HTML/JSON 解析；结构化输出；支持并发执行。
  - Storage（存储）：幂等写入与批量提交；索引更新与去重标记。
  - Monitoring（监控）：日志与指标；告警与容量规划（见专题 08）。
- 流程约束：并发受域级限速、全局并发上限与队列容量三重约束；失败与重试由调度器管理，不交给 Worker 自发。

二、Frontier 优先队列与评分策略
- 评分因素：
  - 新鲜度：越久未更新，优先级越高。
  - 重要性：入口页面 > 列表页 > 详情页（可按业务权重定义）。
  - 失败率：近期失败高则降级；短暂网络波动不宜过度惩罚。
  - 站点策略：对“高风险/高负载”站点自动下调优先级。
- 数据结构：使用 `heapq` 实现最小堆；键为“浮点评分”，值为 `URLItem`。

示例 A：Frontier（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 heapq 实现优先队列；评分融合新鲜度、重要性、失败权重
import heapq, time
from dataclasses import dataclass, field
from typing import Optional

@dataclass(order=True)
class QueueItem:
    score: float
    url: str = field(compare=False)
    site: str = field(compare=False)
    kind: str = field(compare=False)       # 'seed'/'list'/'detail' 等
    last_seen: Optional[float] = field(default=None, compare=False)
    fail_recent: int = field(default=0, compare=False)

class Frontier:
    def __init__(self):
        self._heap = []
        self._seen = set()  # URL 去重（可替换为集中去重服务）

    def _calc_score(self, kind: str, last_seen: Optional[float], fail_recent: int) -> float:
        now = time.time()
        freshness = (now - (last_seen or 0)) / 3600.0  # 小时级新鲜度
        importance = {'seed': 0.5, 'list': 1.0, 'detail': 1.5}.get(kind, 1.0)
        penalty = 0.3 * fail_recent
        return importance - min(freshness, 24.0) * 0.05 + penalty  # 分数越小越优先

    def add(self, url: str, site: str, kind: str, last_seen: Optional[float], fail_recent: int):
        if url in self._seen:
            return
        self._seen.add(url)
        score = self._calc_score(kind, last_seen, fail_recent)
        heapq.heappush(self._heap, QueueItem(score, url, site, kind, last_seen, fail_recent))

    def pop(self) -> Optional[QueueItem]:
        return heapq.heappop(self._heap) if self._heap else None

    def __len__(self):
        return len(self._heap)
```

三、域级限流与公平并发
- 域级限速：每个域维护最小请求间隔与并发许可（信号量）。结合失败信号动态调节间隔（参考专题 03 的 Throttler）。
- 公平性：在多域场景下，避免某一热门域独占并发；采用“轮询或加权轮询”策略分配抓取机会。
- 全局并发上限：设置 `N` 作为总并发上限，保证内存与连接池稳定。

示例 B：域级信号量与延迟（含详注，async/await）
```python
# -*- coding: utf-8 -*-
# 为每个域设置并发许可与最小间隔，确保公平与礼貌抓取
import asyncio, time
from urllib.parse import urlparse

class DomainGate:
    def __init__(self, base_delay: float = 0.5, max_concurrency_per_domain: int = 2):
        self.delay = {}
        self.last = {}
        self.sema = {}
        self.base_delay = base_delay
        self.max_conc = max_concurrency_per_domain

    def _dom(self, url: str) -> str:
        return urlparse(url).netloc

    async def acquire(self, url: str):
        d = self._dom(url)
        if d not in self.sema:
            self.sema[d] = asyncio.Semaphore(self.max_conc)
            self.delay[d] = self.base_delay
            self.last[d] = 0.0
        await self.sema[d].acquire()
        # 等待最小间隔
        now = time.time()
        need = self.delay[d]
        since = now - self.last[d]
        if since < need:
            await asyncio.sleep(need - since)
        self.last[d] = time.time()

    def release(self, url: str):
        d = self._dom(url)
        self.sema[d].release()

    def feedback(self, url: str, status: int | None, success: bool):
        # 简化：根据状态码动态调整间隔（更详细逻辑见专题 03）
        d = self._dom(url)
        if status == 429:
            self.delay[d] = min(5.0, self.delay[d] * 2)
        elif success:
            self.delay[d] = max(0.2, self.delay[d] * 0.9)
```

四、异步抓取与连接池配置（aiohttp）
- 连接池：控制每主机并发、总连接数、DNS 缓存与超时时间；减少频繁握手带来的开销。
- 超时与重试：区分连接超时与读取超时；针对幂等 GET 进行有限重试；非幂等请求避免自动重试。
- 结构化日志：记录 URL、状态码、延时、重试次数、代理与 UA（见专题 01/08）。

示例 C：aiohttp 抓取封装（含详注）
```python
# -*- coding: utf-8 -*-
import aiohttp, asyncio, time

class AsyncFetcher:
    def __init__(self, gate: DomainGate, total_concurrency: int = 20):
        self.gate = gate
        self.sem = asyncio.Semaphore(total_concurrency)
        self.session = None

    async def _ensure_session(self):
        if self.session:
            return
        timeout = aiohttp.ClientTimeout(total=40, connect=5)
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=8, ssl=False)
        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector, headers={
            'User-Agent': 'Crawler/1.0'
        })

    async def get(self, url: str) -> tuple[int | None, bytes | None, float]:
        await self._ensure_session()
        await self.sem.acquire()
        await self.gate.acquire(url)
        start = time.time(); status = None; body = None
        try:
            async with self.session.get(url) as resp:
                status = resp.status
                body = await resp.read()
                return status, body, time.time() - start
        except Exception:
            return None, None, time.time() - start
        finally:
            self.gate.release(url)
            self.sem.release()

    async def close(self):
        if self.session:
            await self.session.close()
```

五、回压（Back-Pressure）与队列容量
- 设计原则：在 `asyncio.Queue` 上设置有限 `maxsize`，当下游（解析/存储）滞后时，阻塞上游（抓取）以防内存膨胀。
- 队列分层：将“待抓取 URL 队列”和“已抓页面待解析队列”分离，防止相互阻塞；使用独立并发控制。

示例 D：分层队列管线（含详注）
```python
# -*- coding: utf-8 -*-
import asyncio

async def producer(frontier: Frontier, url_q: asyncio.Queue):
    while True:
        item = frontier.pop()
        if not item:
            await asyncio.sleep(0.2)
            continue
        await url_q.put(item)  # 当队列满时阻塞，形成回压

async def fetcher(url_q: asyncio.Queue, page_q: asyncio.Queue, af: AsyncFetcher):
    while True:
        item = await url_q.get()
        status, body, lat = await af.get(item.url)
        # 结构化记录（省略具体日志实现）
        await page_q.put((item, status, body))
        url_q.task_done()

async def parser(page_q: asyncio.Queue, store_func):
    while True:
        item, status, body = await page_q.get()
        if status == 200 and body:
            # 调用解析函数，得到结构化结果
            data = {'url': item.url, 'len': len(body)}  # 示例，替换为实际解析
            await store_func(data)
        page_q.task_done()

async def main_pipeline(frontier: Frontier, af: AsyncFetcher, store_func):
    url_q = asyncio.Queue(maxsize=200)
    page_q = asyncio.Queue(maxsize=200)
    tasks = [
        asyncio.create_task(producer(frontier, url_q)),
        *[asyncio.create_task(fetcher(url_q, page_q, af)) for _ in range(5)],
        *[asyncio.create_task(parser(page_q, store_func)) for _ in range(4)],
    ]
    await asyncio.gather(*tasks)
```

六、重试策略与延后重排
- 重试队列：失败项不立即压回同优先级队列；放入“延后队列”，未来一段时间后再重排进入 Frontier。
- 指数退避：第 N 次失败后的下一次尝试时间 `now + base * 2^N`，上限可设为数分钟到数小时。
- 分类策略：对 `429/403` 与网络超时区别对待；对 5xx 可短期重试，对 404/410 不再重试。

示例 E：延后队列（含详注）
```python
# -*- coding: utf-8 -*-
import heapq, time

class DelayQueue:
    def __init__(self):
        self._heap = []  # (wake_ts, item)
    def put(self, wake_ts: float, item: QueueItem):
        heapq.heappush(self._heap, (wake_ts, item))
    def pop_ready(self):
        now = time.time(); ready = []
        while self._heap and self._heap[0][0] <= now:
            _, it = heapq.heappop(self._heap)
            ready.append(it)
        return ready
```

七、优雅停止与早停条件
- 优雅停止：捕获信号后停止接收新任务，等待队列 drain（消费完成），再关闭会话与连接。
- 早停：达到“抓取上限/耗时上限/失败阈值/无新内容”时提前退出，避免无谓运行。

示例 F：优雅停止（含详注）
```python
# -*- coding: utf-8 -*-
import asyncio, signal

class Graceful:
    def __init__(self):
        self.stop = asyncio.Event()
    def install(self):
        loop = asyncio.get_running_loop()
        loop.add_signal_handler(signal.SIGINT, self.stop.set)
        loop.add_signal_handler(signal.SIGTERM, self.stop.set)

async def run_until_stop(tasks: list[asyncio.Task], af: AsyncFetcher):
    g = Graceful(); g.install()
    await g.stop.wait()
    for t in tasks:
        t.cancel()
    try:
        await asyncio.gather(*tasks, return_exceptions=True)
    finally:
        await af.close()
```

八、解析并发与执行器选择
- I/O 与 CPU 分离：抓取使用协程（I/O 密集），解析可能为 CPU 或 I/O 混合；可使用 `ThreadPoolExecutor` 或 `ProcessPoolExecutor`。
- 基准评估：对解析函数做简单基准测试，选择线程或进程执行器；避免全量进程化带来上下文开销。

示例 G：线程执行器进行解析（含详注）
```python
# -*- coding: utf-8 -*-
import asyncio
from concurrent.futures import ThreadPoolExecutor

def parse_html_sync(body: bytes) -> dict:
    # 这里调用 lxml/bs4 等同步解析逻辑
    return {'length': len(body)}

async def parse_async(body: bytes, executor: ThreadPoolExecutor) -> dict:
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, parse_html_sync, body)
```

九、批量写入与幂等
- 批量写入：聚合一批解析结果后统一提交数据库/文件，降低事务与 I/O 开销。
- 幂等：使用唯一键（URL/指纹）进行 `INSERT OR REPLACE` 或 `UPSERT`；避免重复写入。

示例 H：批量提交示意（含详注）
```python
# -*- coding: utf-8 -*-
import asyncio, time

class BatchStore:
    def __init__(self, flush_every: int = 100, flush_seconds: float = 3.0):
        self.buf = []
        self.flush_every = flush_every
        self.flush_seconds = flush_seconds
        self._last_flush = time.time()
    async def add(self, item: dict):
        self.buf.append(item)
        if len(self.buf) >= self.flush_every or (time.time() - self._last_flush) >= self.flush_seconds:
            await self.flush()
    async def flush(self):
        if not self.buf:
            return
        # 将 self.buf 写入数据库（省略具体实现）；注意幂等与事务
        self.buf.clear(); self._last_flush = time.time()
```

十、领域公平与权重轮询
- 领域公平：将待抓取项按域分桶，采用轮询或加权轮询保证不同域都能获得抓取机会。
- 权重来源：站点重要性、历史成功率、失败信号；权重越大分配的机会越多。

示例 I：加权轮询（含详注）
```python
# -*- coding: utf-8 -*-
from collections import defaultdict, deque

class WeightedRoundRobin:
    def __init__(self):
        self.buckets = defaultdict(deque)  # domain -> deque[QueueItem]
        self.weight = defaultdict(lambda: 1)
    def add(self, item: QueueItem):
        self.buckets[item.site].append(item)
    def set_weight(self, domain: str, w: int):
        self.weight[domain] = max(1, w)
    def next(self) -> QueueItem | None:
        # 简化：按权重轮询选择非空桶（真实实现需维护当前权重指针）
        for dom, q in self.buckets.items():
            if q:
                return q.popleft()
        return None
```

十一、可观测性与调参
- 指标维度：队列长度、吞吐（每秒抓取/解析数）、平均延时、错误率、每域并发与延迟、重试次数。
- 调参方法：先观察，再小步调整（并发、间隔、队列容量）；记录变化带来的指标影响。
- 告警：队列积压、失败率攀升、响应体为空或占位页比例异常；自动降速或暂停（见专题 03/08）。

总结：
- 调度并发的工程核心是“Frontier 优先级 + 域级限速 + 分层队列 + 回压 + 优雅停止 + 可观测”。
- 协程用于 I/O 抓取，解析用线程/进程执行器；批量写入保证吞吐与开销均衡。
- 公平与礼貌优先，在稳定性基础上逐步提高并发与吞吐。

示例：优先级调度器（含详注）
```python
# -*- coding: utf-8 -*-
import heapq
from collections import defaultdict
from urllib.parse import urlparse

class ScheduledItem:
    def __init__(self, priority: float, url: str):
        self.priority = priority; self.url = url
    def __lt__(self, other):
        return self.priority < other.priority  # 最小堆：priority 越小越先出队

class Scheduler:
    def __init__(self, per_domain_concurrency: int = 2):
        self.frontier = []; self.inflight = defaultdict(int)
        self.per_domain_concurrency = per_domain_concurrency
        self.seen = set()  # URL 去重集合
    def push(self, url: str, priority: float = 1.0):
        if url in self.seen: return
        self.seen.add(url); heapq.heappush(self.frontier, ScheduledItem(priority, url))
    def pop(self) -> str | None:
        for _ in range(len(self.frontier)):
            item = heapq.heappop(self.frontier)
            dom = urlparse(item.url).netloc
            if self.inflight[dom] < self.per_domain_concurrency:
                self.inflight[dom] += 1; return item.url
            item.priority += 0.1; heapq.heappush(self.frontier, item)
        return None
    def mark_done(self, url: str):
        dom = urlparse(url).netloc
        self.inflight[dom] = max(0, self.inflight[dom] - 1)
```

—

## 专题 05：去重与增量（URL 规范化、内容指纹、条件请求、复跑）

本专题系统化搭建“重复控制与增量抓取”能力，以稳定、幂等、可回收为目标，覆盖：唯一键与 URL 规范化、内容指纹与近重复检测、集中去重与 TTL、条件请求（ETag/Last-Modified）、列表页增量边界、写入幂等（UPSERT/版本比较）、断点续抓与复跑策略、媒体资源增量、指标与测试，以及常见陷阱应对。所有示例均提供高密度中文注解，便于即插即用。

核心设计原则：
- 唯一键明确：URL 归一化 + 站点域/路径规则，确保跨入口一致性。
- 幂等优先：任何重复调用不产生副作用；写入采用 UPSERT 或条件更新。
- 渐进增量：以“时间戳/ID/版本”驱动；列表页遇到已知边界立即停止，避免全量重扫。
- 条件请求：优先 If-None-Match/If-Modified-Since，304 不变即可跳过下载与解析。
- 多层去重：发现去重（Frontier）→ 抓取去重（URL/ETag）→ 内容去重（指纹/近重复）→ 存储去重（唯一索引）。
- 断点可续：持久化队列与检查点；崩溃后恢复不丢量、不重复、可继续。

一、URL 规范化与唯一键设计
- 目标：不同来源的相同页面（协议不同、带 `utm_*`、哈希片段、大小写差异等）映射到同一唯一键，用于全流程去重。
- 建议：
  - 绝对化：`urljoin(base, href)`；
  - 去片段：移除 `#...`；
  - 清洗统计参数：移除 `utm_*`、`ref`、`spm` 等不影响内容的参数；
  - 查询参数排序与归一：按 key 排序，必要时白名单（例如仅保留 `id`/`page`）；
  - 协议与子域统一：`http→https`、去 `www.`（按站点策略）；
  - 大小写与编码：路径区分大小写的站点需谨慎；常见站点可将路径统一为小写、统一 `%xx` 编码。

示例 1：URL 规范化工具（含详注）
```python
# -*- coding: utf-8 -*-
# 将可能重复的 URL 归一化为稳定唯一键，用于发现与存储去重
from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode

DROP_QUERY_KEYS = {  # 站点无关的统计/追踪参数，统一删除
    'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
    'gclid', 'fbclid', 'spm', 'ref', 'referrer', 'from', 'track', 'ts',
}

def normalize_url(url: str) -> str:
    """将 URL 统一到可比较的规范形式。
    - 移除片段 `#...`
    - 参数白名单/黑名单与排序
    - 协议统一到 https（可按站点策略调整）
    - 子域归一（去 www.）
    """
    if not url:
        return ''
    parts = urlsplit(url)
    scheme = 'https' if parts.scheme in ('http', 'https') else parts.scheme
    netloc = parts.netloc.lower()
    netloc = netloc[4:] if netloc.startswith('www.') else netloc  # 去除 www.
    # 解析查询参数，并删除不影响内容的追踪参数
    q = [(k, v) for k, v in parse_qsl(parts.query, keep_blank_values=True) if k not in DROP_QUERY_KEYS]
    q.sort(key=lambda x: (x[0], x[1]))  # 稳定排序，避免顺序差异
    query = urlencode(q, doseq=True)
    # 片段置空；路径可按站点规则转小写
    path = parts.path  # 如需统一为小写：path = parts.path.lower()
    frag = ''
    return urlunsplit((scheme, netloc, path, query, frag))

def url_key(url: str) -> str:
    """唯一键：可直接使用规范化 URL；对少数站点可加站点前缀避免冲突。"""
    return normalize_url(url)
```

二、内容指纹与近重复检测（强重复 + 近重复）
- 强重复：同一正文（或一致的关键字段）使用 `SHA-256/MD5` 指纹；完全相同即可判定重复。
- 近重复：正文轻微差异（格式调整、广告改动）使用 `SimHash` 等近似指纹，汉明距离在阈值内判定为近重复。
- Canonical 化：在指纹前，对正文执行清洗与规范化（去脚注/冗余空白/统一引号），提升稳定性。

示例 2：文本 Canonical 化与指纹（含详注）
```python
# -*- coding: utf-8 -*-
# 对正文进行清洗再指纹化，支持强重复（SHA-256）与近重复（SimHash）
import re, hashlib

def canonical_text(text: str) -> str:
    """将正文规范化以提升指纹稳定性。
    - 移除脚注/广告等常见噪声（根据业务黑名单扩展）
    - 统一空白：合并连续空格/换行
    - 统一引号与破折号等符号表现
    """
    if not text:
        return ''
    t = re.sub(r"\s+", " ", text)  # 合并空白
    t = t.replace('\u00A0', ' ')      # 不断行空格替换为普通空格
    t = t.replace('“', '"').replace('”', '"')
    t = t.replace('’', "'").replace('‘', "'")
    t = re.sub(r"\s*-\s*", "-", t)  # 统一破折号周边空白
    return t.strip()

def sha256_of(text: str) -> str:
    t = canonical_text(text)
    return hashlib.sha256(t.encode('utf-8')).hexdigest()

def simhash64(tokens: list[str]) -> int:
    """简单 SimHash：按 token 的哈希位进行加权求和，正负决定位值。
    注：生产建议使用成熟库；此实现用于解释思路。"""
    bits = [0] * 64
    for tok in tokens:
        h = int(hashlib.md5(tok.encode('utf-8')).hexdigest(), 16)
        for i in range(64):
            bits[i] += 1 if (h >> i) & 1 else -1
    sig = 0
    for i, b in enumerate(bits):
        if b >= 0:
            sig |= (1 << i)
    return sig

def hamming_distance(a: int, b: int) -> int:
    x = a ^ b
    # 统计 1 的位数（即不同位数量）
    return bin(x).count('1')

def is_near_duplicate(sig_a: int, sig_b: int, threshold: int = 8) -> bool:
    """汉明距离 <= 阈值 即认为近重复；阈值按文本长度与业务容忍度调参。"""
    return hamming_distance(sig_a, sig_b) <= threshold
```

三、集中去重存储（发现/抓取层面的原子去重）
- 目标：避免多个 Worker 重复对同一 URL 抓取；在分布式环境中使用集中去重（Redis/DB 唯一索引）。
- 策略：SETNX（仅当不存在时设置）实现原子“占坑”；结合 TTL 释放异常占用；失败释放占用以便重试。

示例 3：Redis SETNX 原子占坑（含详注）
```python
# -*- coding: utf-8 -*-
# 集中去重：尝试占坑，成功则执行抓取；失败说明已被其他 Worker 占用
import redis, time

r = redis.Redis(host='localhost', port=6379, db=0)

def try_claim(url_key: str, ttl_seconds: int = 600) -> bool:
    """占坑成功返回 True；并设置 TTL 防止 Worker 崩溃导致长期占用。"""
    ok = r.setnx(f"seen:{url_key}", int(time.time()))
    if ok:
        r.expire(f"seen:{url_key}", ttl_seconds)
    return bool(ok)

def release_claim(url_key: str):
    """抓取完成或失败后释放占坑；增量复跑时允许再次占用。"""
    r.delete(f"seen:{url_key}")
```

四、条件请求（ETag/Last-Modified）与 TTL 增量
- 背景：对稳定页面使用条件请求，大幅降低带宽与解析成本；配合站点级/路径级 TTL 设定复抓频率。
- 行为：
  - 发送 `If-None-Match`（ETag）或 `If-Modified-Since`；
  - 服务器返回 `304 Not Modified` 则跳过下载与解析；
  - 服务器返回 `200 OK` 则更新缓存的 ETag/Last-Modified 信息。

示例 4：条件请求抓取器（含详注，SQLite 存储元数据）
```python
# -*- coding: utf-8 -*-
import sqlite3, requests, time
from dataclasses import dataclass

@dataclass
class Meta:
    etag: str | None = None
    last_modified: str | None = None
    last_fetch_ts: float | None = None

class ConditionalFetcher:
    """为每个规范化 URL 存储与使用 ETag/Last-Modified，实现条件抓取与增量。"""
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self._init()

    def _init(self):
        cur = self.conn.cursor()
        cur.execute("""
        CREATE TABLE IF NOT EXISTS fetch_meta (
            url_key TEXT PRIMARY KEY,
            etag TEXT,
            last_modified TEXT,
            last_fetch_ts REAL
        )
        """)
        self.conn.commit()

    def _get_meta(self, url_key: str) -> Meta:
        cur = self.conn.cursor()
        cur.execute("SELECT etag, last_modified, last_fetch_ts FROM fetch_meta WHERE url_key=?", (url_key,))
        row = cur.fetchone()
        return Meta(*row) if row else Meta()

    def _save_meta(self, url_key: str, meta: Meta):
        cur = self.conn.cursor()
        cur.execute(
            "INSERT INTO fetch_meta(url_key, etag, last_modified, last_fetch_ts) VALUES(?,?,?,?)\n"
            "ON CONFLICT(url_key) DO UPDATE SET etag=excluded.etag, last_modified=excluded.last_modified, last_fetch_ts=excluded.last_fetch_ts",
            (url_key, meta.etag, meta.last_modified, meta.last_fetch_ts or time.time())
        )
        self.conn.commit()

    def fetch(self, url: str, ttl_seconds: int = 86400) -> tuple[int, bytes | None, dict]:
        """返回 (status_code, content, headers)。
        - 若距离上次抓取 < TTL，直接返回 304（跳过）以减少压力。
        - 否则携带条件头抓取；304 返回 None；200 更新元数据并返回内容。"""
        key = url_key(url)
        meta = self._get_meta(key)
        now = time.time()
        if meta.last_fetch_ts and (now - meta.last_fetch_ts) < ttl_seconds:
            return 304, None, {}
        headers = {}
        if meta.etag:
            headers['If-None-Match'] = meta.etag
        if meta.last_modified:
            headers['If-Modified-Since'] = meta.last_modified
        resp = requests.get(url, headers=headers, timeout=20)
        if resp.status_code == 304:
            meta.last_fetch_ts = now
            self._save_meta(key, meta)
            return 304, None, dict(resp.headers)
        elif resp.status_code == 200:
            # 更新 ETag/Last-Modified；部分站点仅返回其中之一
            meta.etag = resp.headers.get('ETag')
            meta.last_modified = resp.headers.get('Last-Modified')
            meta.last_fetch_ts = now
            self._save_meta(key, meta)
            return 200, resp.content, dict(resp.headers)
        else:
            return resp.status_code, resp.content, dict(resp.headers)
```

五、列表页增量边界与早停
- 目标：列表页抓新条目，遇到“已知边界”（如已见 ID/发布时间早于检查点）立即停止向后翻页，避免全量重扫。

示例 5：列表页按发布时间增量（含详注）
```python
# -*- coding: utf-8 -*-
# 站点级检查点：记录最新抓取到的发布时间，下一次抓取遇到更早的时间则停止翻页
import sqlite3
from datetime import datetime

def parse_list(html: str) -> list[dict]:
    """解析列表页，返回条目（包含 url 与 published_at）。此处省略具体解析。"""
    return []

class Checkpoint:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path); self._init()
    def _init(self):
        cur = self.conn.cursor()
        cur.execute("CREATE TABLE IF NOT EXISTS cp(site TEXT PRIMARY KEY, last_ts REAL)"); self.conn.commit()
    def get(self, site: str) -> float:
        cur = self.conn.cursor(); cur.execute("SELECT last_ts FROM cp WHERE site=?", (site,))
        row = cur.fetchone(); return float(row[0]) if row else 0.0
    def set(self, site: str, ts: float):
        cur = self.conn.cursor(); cur.execute(
            "INSERT INTO cp(site,last_ts) VALUES(?,?) ON CONFLICT(site) DO UPDATE SET last_ts=excluded.last_ts",
            (site, ts)
        ); self.conn.commit()

def should_stop(published_at: datetime, last_ts: float) -> bool:
    return published_at.timestamp() <= last_ts

def handle_list_page(site: str, html: str, cp: Checkpoint) -> list[str]:
    last_ts = cp.get(site)
    new_urls = []
    newest_ts = last_ts
    for item in parse_list(html):
        ts = item.get('published_at')
        if ts and should_stop(ts, last_ts):
            # 到达增量边界：遇到旧条目，列表可提前结束；若是翻页流程可停止向后翻页
            break
        new_urls.append(item['url'])
        if ts:
            newest_ts = max(newest_ts, ts.timestamp())
    if newest_ts > last_ts:
        cp.set(site, newest_ts)  # 更新检查点
    return new_urls
```

六、写入幂等（UPSERT/版本比较）
- 目标：相同 URL 多次写入不导致重复；新版本覆盖旧版本（如发布时间更新、正文修订）。

示例 6：SQLite UPSERT（含详注，按发布时间更“新”才更新）
```python
# -*- coding: utf-8 -*-
import sqlite3

def init_articles(conn: sqlite3.Connection):
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS articles (
        url TEXT PRIMARY KEY,
        site TEXT,
        title TEXT,
        content TEXT,
        author TEXT,
        published_at REAL,  -- 用时间戳便于比较
        updated_at REAL
    )
    """)
    conn.commit()

def upsert_article(conn: sqlite3.Connection, row: dict):
    """按 url 幂等写入；仅当 incoming.published_at 更新更“新”时执行覆盖。"""
    cur = conn.cursor()
    cur.execute(
        """
        INSERT INTO articles(url, site, title, content, author, published_at, updated_at)
        VALUES(:url, :site, :title, :content, :author, :published_at, strftime('%s','now'))
        ON CONFLICT(url) DO UPDATE SET
            site=excluded.site,
            title=excluded.title,
            content=excluded.content,
            author=excluded.author,
            published_at=CASE WHEN excluded.published_at > articles.published_at THEN excluded.published_at ELSE articles.published_at END,
            updated_at=strftime('%s','now')
        WHERE excluded.published_at > articles.published_at
        """,
        row
    )
    conn.commit()
```

七、断点续抓与复跑策略
- 目标：程序中断或失败后可恢复现场；复跑仅抓落下的或更新的内容，不重复处理已完成条目。
- 机制：
  - 持久化 Frontier（待抓队列）与 inflight（进行中）；重启后按优先级续跑；
  - 失败列表记录错误类型与重试窗口；达到次数后暂停站点（见专题 03 风险控制）。

示例 7：持久化队列快照（含详注，简化版）
```python
# -*- coding: utf-8 -*-
import json, time
from pathlib import Path

class QueueSnapshot:
    def __init__(self, path: str):
        self.path = Path(path)
    def save(self, frontier: list[str], inflight: list[str]):
        data = {
            'ts': int(time.time()),
            'frontier': frontier,
            'inflight': inflight,
        }
        self.path.write_text(json.dumps(data, ensure_ascii=False))
    def load(self) -> tuple[list[str], list[str]]:
        if not self.path.exists():
            return [], []
        data = json.loads(self.path.read_text())
        return data.get('frontier', []), data.get('inflight', [])
```

八、媒体资源增量（ETag/Content-Length/哈希校验）
- 背景：图片/附件体积大、频繁更新少；应优先使用条件请求与校验，必要时断点续传（见专题 12）。

示例 8：媒体下载增量（含详注，ETag 驱动）
```python
# -*- coding: utf-8 -*-
import requests, hashlib
from pathlib import Path

def download_if_changed(url: str, dest_path: str, known_etag: str | None = None) -> tuple[bool, str | None]:
    """若服务器返回 304 则跳过；200 则写入文件并返回新的 ETag。
    返回 (downloaded, new_etag)。downloaded 为 True 表示发生下载与写入。"""
    headers = {}
    if known_etag:
        headers['If-None-Match'] = known_etag
    resp = requests.get(url, headers=headers, stream=True, timeout=30)
    if resp.status_code == 304:
        return False, known_etag
    if resp.status_code != 200:
        raise RuntimeError(f"download failed: {resp.status_code}")
    tmp = Path(dest_path + ".part")
    with tmp.open('wb') as f:
        for chunk in resp.iter_content(chunk_size=64 * 1024):
            if chunk:
                f.write(chunk)
    tmp.replace(dest_path)  # 原子替换
    return True, resp.headers.get('ETag')

def file_sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        for buf in iter(lambda: f.read(8192), b''):
            h.update(buf)
    return h.hexdigest()
```

九、去重漏斗与执行顺序建议
- 顺序：
  1) 发现时以 `url_key` 去重；
  2) 抓取时 `try_claim(url_key)` 原子占坑；
  3) 命中 TTL 或返回 304 跳过；
  4) 解析后以 `sha256_of(canonical_text)` 强重复去重；
  5) 写入层以 `UPSERT` + 条件更新保证幂等；
  6) 对正文相似的条目启用近重复过滤（SimHash 汉明距离阈值）。

十、指标与测试（保障增量正确性）

示例 9：去重与增量基础指标（含详注）
```python
# -*- coding: utf-8 -*-
# 统计“发现去重命中/抓取 304/内容强重复/近重复”命中次数
from prometheus_client import Counter

metrics_discovery_dedup = Counter('dedup_discovery_hits', '发现阶段去重命中')
metrics_fetch_304 = Counter('dedup_fetch_304', '抓取阶段返回304次数')
metrics_content_dup = Counter('dedup_content_exact', '内容强重复命中')
metrics_content_near = Counter('dedup_content_near', '内容近重复命中')

def mark_discovery_dedup():
    metrics_discovery_dedup.inc()
def mark_fetch_304():
    metrics_fetch_304.inc()
def mark_content_exact():
    metrics_content_dup.inc()
def mark_content_near():
    metrics_content_near.inc()
```

示例 10：URL 规范化与指纹的单元测试（含详注）
```python
# -*- coding: utf-8 -*-
def test_normalize_url_basic():
    u1 = 'http://www.example.com/a?utm_source=xx&id=1#top'
    u2 = 'https://example.com/a?id=1'
    assert normalize_url(u1) == normalize_url(u2)

def test_sha256_canonical_stability():
    t1 = '“示例” 文本\n\n 带有  多余空白'
    t2 = '"示例" 文本 带有 多余空白'
    assert sha256_of(t1) == sha256_of(t2)
```

十一、常见陷阱与对策
- URL 变体陷阱：同一内容的分享/短链/不同子域；务必规范化与白名单参数保留策略。
- 软更新：页面结构变化但内容无变更；依赖 ETag/Last-Modified 与正文指纹确认是否真正变化。
- 列表排序变更：按发布时间增量时注意站点改为“热门排序”；需明确排序字段或改用 ID 边界。
- 多语言与时区：发布时间解析需统一为 UTC 或站点时区（见专题 13）。
- 指纹碰撞与误判：强重复指纹极低概率碰撞；近重复阈值需结合业务回归测试调整。

十二、整合建议（工程落地）
- 在 `crawler/scheduler.py` 的 Frontier 插入 `url_key` 去重；
- 在 `crawler/fetcher.py` 统一接入 `ConditionalFetcher` 与 TTL；
- 在 `crawler/parser.py` 输出 `canonical_text` 与 `sha256`；
- 在 `crawler/storage.py` 采用 `UPSERT` 与按版本/时间条件更新；
- 建立 `Checkpoint`（站点维度）管理列表增量边界；
- 为媒体下载通道添加 `download_if_changed` 与哈希校验；
- 在监控中加入“重复/近重复/304/TTL 命中”指标，作为调参依据。

—

## 专题 06：浏览器渲染（Playwright/Selenium、等待策略、稳定化）

本专题系统阐述浏览器渲染在爬虫中的应用与边界，强调“能不渲染就不渲染，必须渲染要稳、要省、要合规”。围绕 Playwright/Selenium 两条线路，给出等待策略、指纹稳定化、资源阻断、交互脚本、滚动与分页、接口捕获、登录与验证码、会话与 Cookie 持久化、上下文复用、观测与错误分级等工程化实践。所有示例均为可直接粘贴使用的最简封装，并包含详细中文注释。

一、何时启用渲染与总体原则
- 场景判断：
  - 必须渲染：强依赖 JS 构建数据（SPA/React/Vue、接口参数在 JS 中生成）；
  - 不必渲染：纯静态页面、接口可直接抓取（Ajax/GraphQL/REST）；优先走 HTTP（参见专题 03 策略）。
- 成本与风险：渲染开销大、指纹暴露多、易触发风控；建议仅用于必要页面与短窗口抓取。
- 稳定化原则：
  - 指纹最小暴露：谨慎设置时区/语言/UA/窗口大小，避免极端组合；
  - 等待明确：优先事件/选择器等待，少用固定 `sleep`；
  - 资源阻断：阻断第三方分析与重资源，提高性能降低暴露；
  - 并发与速率：降低并发、增大等待与抖动；失败有限重试，必要时暂停。

二、Playwright 稳定化渲染封装（含详注）
```python
# -*- coding: utf-8 -*-
# Playwright 稳定化渲染：上下文指纹设置、资源阻断、选择器/事件等待、HTML 快照
from playwright.sync_api import sync_playwright

STEALTH_INIT_JS = """
// 在页面加载前注入，缓和部分常见自动化指纹
Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
window.chrome = { runtime: {} };
Object.defineProperty(navigator, 'plugins', {get: () => [1,2,3]});
Object.defineProperty(navigator, 'languages', {get: () => ['zh-CN', 'zh']});
"""

BLOCK_PATTERNS = [
    'google-analytics.com', 'googletagmanager.com', 'doubleclick.net',
    'hm.baidu.com', 'googlesyndication.com', 'adservice.google.com',
]

def render_html(url: str, selector_to_wait: str | None = None, wait_network_idle: bool = True) -> str | None:
    """稳定化渲染并返回 HTML（若失败返回 None）。
    - 指纹：locale/timezone/viewport/UA 初始化；
    - 资源阻断：拦截第三方分析/广告域，减少指纹采集与耗时；
    - 等待策略：networkidle（可选）+ 关键选择器；
    - 异常处理：确保 context/browser 关闭；
    """
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            locale='zh-CN', timezone_id='Asia/Shanghai',
            viewport={'width': 1366, 'height': 768},
            user_agent=('Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                        'AppleWebKit/537.36 (KHTML, like Gecko) '
                        'Chrome/118.0 Safari/537.36')
        )
        context.add_init_script(STEALTH_INIT_JS)
        page = context.new_page()
        # 路由拦截：阻断第三方重资源与分析脚本
        def route_filter(route):
            req_url = route.request.url
            if any(b in req_url for b in BLOCK_PATTERNS):
                return route.abort()
            return route.continue_()
        page.route('**/*', route_filter)
        try:
            page.goto(url, wait_until='domcontentloaded')
            if wait_network_idle:
                # networkidle：网络静默；部分站点永远不静默，可按需关闭
                page.wait_for_load_state('networkidle', timeout=20000)
            if selector_to_wait:
                page.wait_for_selector(selector_to_wait, timeout=20000)  # 等待关键块出现
            # 轻度行为：滚动一段以触发懒加载
            page.evaluate("window.scrollTo(0, document.body.scrollHeight*0.3)"); page.wait_for_timeout(400)
            page.evaluate("window.scrollTo(0, document.body.scrollHeight*0.7)"); page.wait_for_timeout(400)
            return page.content()
        finally:
            context.close(); browser.close()
```

三、捕获接口数据与 XHR/Fetch 响应（含详注）
```python
# -*- coding: utf-8 -*-
# 在渲染过程中捕获 XHR/Fetch 响应，优先直接使用 JSON 数据，避免再从 DOM 提取
from playwright.sync_api import sync_playwright
import json

def render_and_capture_api(url: str, api_prefixes: list[str]) -> tuple[str | None, list[dict]]:
    """返回 (html, api_payloads)。
    - api_prefixes：接口 URL 前缀列表（如 '/api/'、'graphql'），用于过滤响应；
    - 将 JSON 响应解析为 dict 存储，便于结构化输出；
    """
    payloads: list[dict] = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(locale='zh-CN', timezone_id='Asia/Shanghai')
        page = context.new_page()
        # 监听响应事件，捕获匹配的接口响应
        def on_response(resp):
            try:
                u = resp.url
                if not any(prefix in u for prefix in api_prefixes):
                    return
                ct = resp.headers.get('content-type', '')
                if 'application/json' in ct:
                    data = resp.json()
                else:
                    # 某些接口返回文本 JSON 或 HTML 片段
                    txt = resp.text()
                    try:
                        data = json.loads(txt)
                    except Exception:
                        data = {'raw': txt}
                payloads.append({'url': u, 'status': resp.status, 'data': data})
            except Exception:
                pass
        page.on('response', on_response)
        try:
            page.goto(url, wait_until='domcontentloaded')
            page.wait_for_load_state('networkidle')
            return page.content(), payloads
        finally:
            context.close(); browser.close()
```

四、无限滚动与早停策略（含详注）
```python
# -*- coding: utf-8 -*-
# 通用无限滚动：按滚动次数/高度阈值/无新内容时间早停，避免无穷等待
from playwright.sync_api import sync_playwright

def infinite_scroll(url: str, max_scrolls: int = 10, idle_ms: int = 1200) -> str | None:
    """滚动并等待懒加载。早停条件：
    - 达到 max_scrolls 次滚动；
    - 连续 idle_ms 毫秒未新增内容高度；
    """
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        try:
            page.goto(url, wait_until='domcontentloaded')
            last_height = page.evaluate("document.body.scrollHeight")
            for i in range(max_scrolls):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(500)
                page.wait_for_load_state('networkidle')
                new_height = page.evaluate("document.body.scrollHeight")
                if new_height <= last_height:
                    page.wait_for_timeout(idle_ms)  # 再给懒加载一个窗口
                    new_height = page.evaluate("document.body.scrollHeight")
                    if new_height <= last_height:
                        break  # 早停：没有新内容
                last_height = new_height
            return page.content()
        finally:
            context.close(); browser.close()
```

五、截图与快照保存（含详注）
```python
# -*- coding: utf-8 -*-
# 保存页面截图与 HTML 快照，用于解析回归与人工审阅
from pathlib import Path
from playwright.sync_api import sync_playwright
import re

def safe_name(url: str) -> str:
    name = re.sub(r"[^a-zA-Z0-9._-]", "_", url)
    return name[:128]

def snapshot_page(url: str, out_dir: str) -> tuple[str, str]:
    """返回 (html_path, png_path)。若失败抛出异常。"""
    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(viewport={'width': 1366, 'height': 768})
        page = context.new_page()
        page.goto(url, wait_until='networkidle')
        base = safe_name(url)
        html_path = out / f"{base}.html"
        png_path = out / f"{base}.png"
        html_path.write_text(page.content(), encoding='utf-8')
        page.screenshot(path=str(png_path), full_page=True)
        context.close(); browser.close()
        return str(html_path), str(png_path)
```

六、验证码检测与解题接口（含详注）
```python
# -*- coding: utf-8 -*-
# 验证码处理的通用接口：
# - 在合规前提下，捕获验证码图片与元信息；
# - 调用外部 solver（占位实现），或人工辅助；
from typing import Optional

class CaptchaSolver:
    def solve(self, image_bytes: bytes, meta: dict) -> Optional[str]:
        # 合规前提下返回识别结果；默认 None 表示不识别或失败
        return None

def handle_captcha(page, solver: CaptchaSolver) -> bool:
    try:
        img = page.query_selector('img.captcha, canvas.captcha')
        if not img:
            return True  # 无验证码阻塞
        b = img.screenshot()  # 截图为字节
        ans = solver.solve(b, {"site": page.url})
        if not ans:
            return False
        # 示例：填充到常见输入框
        box = page.query_selector('input[name=captcha], input#captcha')
        if box:
            box.fill(ans)
            return True
        return False
    except Exception:
        return False
```

七、Selenium 显式等待与兼容（含详注）
```python
# -*- coding: utf-8 -*-
# 在不使用 Playwright 的环境下，Selenium 亦可满足基本渲染与等待
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def selenium_render(url: str, css_selector: str, timeout: int = 20) -> str:
    opts = Options(); opts.add_argument('--headless'); opts.add_argument('--disable-gpu')
    with webdriver.Chrome(options=opts) as driver:
        driver.get(url)
        WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, css_selector)))
        return driver.page_source
```

八、上下文复用与页面池（含详注）
```python
# -*- coding: utf-8 -*-
# 频繁新建浏览器上下文开销大；复用 Context 并按域控制并发，提升性能与稳定性
from playwright.sync_api import sync_playwright
from queue import Queue

class PagePool:
    def __init__(self, size: int = 2):
        self.size = size
        self._pages = Queue(maxsize=size)
        self._p = None; self._browser = None; self._context = None

    def __enter__(self):
        self._p = sync_playwright().start()
        self._browser = self._p.chromium.launch(headless=True)
        self._context = self._browser.new_context(locale='zh-CN', timezone_id='Asia/Shanghai')
        for _ in range(self.size):
            self._pages.put(self._context.new_page())
        return self

    def __exit__(self, exc_type, exc, tb):
        while not self._pages.empty():
            pg = self._pages.get(); pg.close()
        self._context.close(); self._browser.close(); self._p.stop()

    def acquire(self):
        return self._pages.get()
    def release(self, page):
        self._pages.put(page)

def use_pool(urls: list[str]) -> list[str]:
    htmls = []
    with PagePool(size=2) as pool:
        for u in urls:
            pg = pool.acquire()
            try:
                pg.goto(u, wait_until='domcontentloaded')
                pg.wait_for_load_state('networkidle')
                htmls.append(pg.content())
            finally:
                pool.release(pg)
    return htmls
```

九、Cookie/会话持久化（含详注）
```python
# -*- coding: utf-8 -*-
# 将登录后的 Cookies 保存到磁盘，后续上下文复用减少登录流程与指纹暴露
from pathlib import Path
from playwright.sync_api import sync_playwright
import json

def save_cookies(url: str, cookie_path: str):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)  # 登录通常需要有头模式
        context = browser.new_context()
        page = context.new_page(); page.goto(url)
        input("请在页面完成登录后按回车继续...")
        cookies = context.cookies()
        Path(cookie_path).write_text(json.dumps(cookies, ensure_ascii=False))
        context.close(); browser.close()

def load_cookies_and_render(url: str, cookie_path: str) -> str:
    cookies = json.loads(Path(cookie_path).read_text())
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        context.add_cookies(cookies)
        page = context.new_page(); page.goto(url, wait_until='networkidle')
        html = page.content(); context.close(); browser.close(); return html
```

十、错误分类与暂停策略（含详注）
```python
# -*- coding: utf-8 -*-
# 根据错误类别调整行为：429/挑战页 -> 暂停域名；超时/网络 -> 延长等待与重试
from typing import Tuple
from playwright.sync_api import TimeoutError as PwTimeout

def render_safe(url: str, selector: str | None = None) -> Tuple[int, str | None]:
    try:
        html = render_html(url, selector_to_wait=selector)
        return 200 if html else 520, html
    except PwTimeout:
        return 504, None  # 网页超时
    except Exception:
        return 520, None  # 其它错误
```

十一、观测与调优建议
- 指标：页面加载耗时、等待命中率（选择器找到的比例）、资源阻断命中数、挑战页出现率；
- 日志：记录 `wait_until`、等待的选择器、滚动次数、阻断的域名；
- 调优：
  - 若 `networkidle` 长期无法达成，改为“选择器出现 + 固定短等待”；
  - 对重型页面强制阻断图片与视频（如 `*.jpg|*.png|*.mp4`）；
  - 上下文复用减少冷启动开销；小池化控制并发；
  - 登录态复用降低挑战与风控概率（见专题 03 与 `crawler/login.py`）。

十二、与专题 03/04/05 的配合
- 03 反爬：在渲染期间保持礼貌抓取、失败分类与暂停；指纹对策谨慎使用。
- 04 调度：对渲染任务单独设置并发上限与速率控制；结合 DomainGate 反馈。
- 05 去重与增量：渲染结果同样走 TTL/条件请求逻辑；接口捕获可直接进入增量存储。

结论：
- 渲染仅用于必要场景；封装为可复用模块，结合“资源阻断 + 明确等待 + 上下文复用 + 错误分级”，实现“慢而稳”的 JS 页面抓取。

—

## 专题 07：存储与索引（文件/数据库/对象存储、索引设计、检索）
本专题围绕“写入稳（幂等）、查询快（索引）、容量可控（分区/归档）、媒体可靠（校验）、演进可回退（迁移）”五个目标，给出文件/数据库/对象存储的工程化落地方案与详细示例。强调“按业务访问路径设计索引”，用事务、UPSERT 与批量写入保证正确性与效率，并提供全文检索（SQLite FTS5 / PostgreSQL `tsvector`）与 JSONB 索引的实践。

一、总体设计与选择权衡
- 文件存储：适合快照与媒体；读写简单、低依赖；与数据库配合存元数据与索引。
- SQLite：轻量嵌入、单机易用；适合原型、小型项目或边缘节点；支持 FTS5 全文索引。
- PostgreSQL：通用关系型，支持事务、UPSERT、分区、JSONB、`tsvector` 全文；适合中大型数据集与多维检索。
- ElasticSearch：强搜索与聚合；运维成本更高；适合复杂检索与排序，但入门可先用 PG/SQLite。
- 对象存储（S3/OSS）：大文件与媒体集中存放；配合元数据（DB）做引用与哈希校验，支持生命周期与分层。

关键原则：
- 幂等写入：以唯一键（如 `url` 或 `external_id`）为约束，采用 `UPSERT` 保证重复写不抛错。
- 业务驱动索引：按“站点/时间/关键字/类别/标签”构建复合索引，避免无用或过度索引。
- 批量提交：解析层缓冲到批，减少事务开销；错误分类处理，确保部分失败不影响整体。
- 分区与归档：时间维度（如月）分区，旧分区归档与只读，降低主库压力。
- 媒体校验：文件保存采取原子替换与哈希校验；对象存储记录 `etag/md5/sha256`。

二、统一数据模型（建议）
- `articles`：文章主表（站点、URL 唯一、标题、正文、作者、发布时间、类别、标签、版本字段）。
- `media`：媒体资源（`url` 唯一、`article_id` 引用、类型/扩展名/大小、哈希/etag、存储位置）。
- `site_meta`：站点级配置与抓取状态（增量边界、上次成功时间、失败计数等）。
- `event_log`：结构化日志（抓取/解析/写入事件，TraceID，错误分类，用于审计与回放）。

三、索引设计与查询路径
- 常见查询：
  - 按站点与时间范围检索文章（分页，按发布时间排序）。
  - 按关键字检索标题/正文（全文搜索，按相关度排序）。
  - 按标签/类别筛选（交集/并集）。
  - 根据 URL 或外部 ID 查重与定位。
- 设计原则：
  - 覆盖索引优先：查询字段在索引中，减少回表（PG 下视情况）。
  - 选择性高的列排前（如 site/published_at）；避免多余索引（维护成本高）。
  - 对 JSONB 字段按实际查询路径建 GIN（`jsonb_path_ops`），避免无谓全字段索引。

四、SQLite 表结构与 FTS5 全文（含详注）
```sql
-- -*- sql -*-
-- SQLite 基础表 + FTS5 全文索引；适合单机原型与小型项目
PRAGMA journal_mode=WAL;  -- 启用 WAL，提高并发读性能
PRAGMA synchronous=NORMAL; -- 写入同步级别折中，提升性能

-- 文章主表：唯一 URL，最少字段；可扩展 extra JSON
CREATE TABLE IF NOT EXISTS articles (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  url TEXT UNIQUE,               -- 唯一键，用于幂等写入
  site TEXT,
  title TEXT,
  content TEXT,                  -- 正文文本（用于 FTS）
  author TEXT,
  published_at TEXT,             -- ISO8601 文本，或 DATETIME
  category TEXT,
  tags TEXT,                     -- 以逗号分隔，或单独建关联表
  created_at TEXT DEFAULT (datetime('now')),
  extra JSON                      -- 半结构化扩展，SQLite 3.9+ 支持 JSON
);

-- FTS5 虚拟表：为 title/content 建全文索引
CREATE VIRTUAL TABLE IF NOT EXISTS articles_fts USING fts5(
  title, content, url UNINDEXED, site UNINDEXED
);

-- 触发器：写入/更新 articles 时同步 FTS 索引
CREATE TRIGGER IF NOT EXISTS articles_ai AFTER INSERT ON articles BEGIN
  INSERT INTO articles_fts(rowid, title, content, url, site)
  VALUES (new.id, new.title, new.content, new.url, new.site);
END;
CREATE TRIGGER IF NOT EXISTS articles_au AFTER UPDATE ON articles BEGIN
  UPDATE articles_fts
    SET title = new.title, content = new.content, url = new.url, site = new.site
    WHERE rowid = new.id;
END;
CREATE TRIGGER IF NOT EXISTS articles_ad AFTER DELETE ON articles BEGIN
  DELETE FROM articles_fts WHERE rowid = old.id;
END;
```

示例 A：SQLite 幂等 UPSERT 与全文检索（含详注）
```python
# -*- coding: utf-8 -*-
# SQLite 文章写入与 FTS5 检索封装：支持 UPSERT 幂等与 BM25 排序
import sqlite3
from typing import Iterable, Tuple

class SqliteStore:
    def __init__(self, path: str):
        # 开启 row_factory 以字典式访问；设置 timeout 防止锁等待过长
        self.conn = sqlite3.connect(path, timeout=10)
        self.conn.row_factory = sqlite3.Row

    def upsert_article(self, url: str, site: str, title: str, content: str,
                        author: str | None, published_at: str | None,
                        category: str | None, tags: str | None,
                        extra: str | None) -> int:
        """幂等写入文章：以 url 为唯一键。
        - 若存在则更新（覆盖标题/正文等），同时触发 FTS5 同步；
        - 返回记录 id（便于后续插入媒体等）；
        """
        cur = self.conn.cursor()
        cur.execute(
            """
            INSERT INTO articles(url, site, title, content, author, published_at, category, tags, extra)
            VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(url) DO UPDATE SET
              site=excluded.site,
              title=excluded.title,
              content=excluded.content,
              author=excluded.author,
              published_at=excluded.published_at,
              category=excluded.category,
              tags=excluded.tags,
              extra=excluded.extra
            ;
            """,
            (url, site, title, content, author, published_at, category, tags, extra)
        )
        # SQLite 的 lastrowid 在 UPSERT 更新时为最后插入行，若更新则需查询 id
        # 这里统一查询 id，以避免语义差异
        cur.execute("SELECT id FROM articles WHERE url=?", (url,))
        row = cur.fetchone()
        self.conn.commit()
        return int(row["id"]) if row else -1

    def search(self, q: str, site: str | None = None, limit: int = 20) -> list[dict]:
        """全文检索：使用 FTS5 的 BM25 排序；可选站点过滤。
        - q 示例：'新闻 AND 科技' 或 '"OpenAI"'
        - 返回基本字段，按相关度降序。
        """
        cur = self.conn.cursor()
        if site:
            cur.execute(
                """
                SELECT a.id, a.url, a.site, a.title, a.published_at,
                       bm25(articles_fts) AS score
                FROM articles_fts JOIN articles AS a ON a.id = articles_fts.rowid
                WHERE articles_fts MATCH ? AND a.site = ?
                ORDER BY score LIMIT ?
                """,
                (q, site, limit)
            )
        else:
            cur.execute(
                """
                SELECT a.id, a.url, a.site, a.title, a.published_at,
                       bm25(articles_fts) AS score
                FROM articles_fts JOIN articles AS a ON a.id = articles_fts.rowid
                WHERE articles_fts MATCH ?
                ORDER BY score LIMIT ?
                """,
                (q, limit)
            )
        rows = cur.fetchall()
        return [dict(r) for r in rows]

    def bulk_upsert(self, batch: Iterable[Tuple[str, str, str, str, str | None, str | None, str | None, str | None, str | None]]) -> int:
        """批量幂等写入：事务包裹；失败逐条回退（简单演示）。
        - 参数为 (url, site, title, content, author, published_at, category, tags, extra)
        - 返回成功写入/更新的记录数；
        """
        cur = self.conn.cursor()
        count = 0
        try:
            self.conn.execute('BEGIN')
            for item in batch:
                cur.execute(
                    """
                    INSERT INTO articles(url, site, title, content, author, published_at, category, tags, extra)
                    VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ON CONFLICT(url) DO UPDATE SET
                      site=excluded.site,
                      title=excluded.title,
                      content=excluded.content,
                      author=excluded.author,
                      published_at=excluded.published_at,
                      category=excluded.category,
                      tags=excluded.tags,
                      extra=excluded.extra
                    ;
                    """,
                    item
                )
                count += 1
            self.conn.commit()
        except Exception:
            # 出错时回滚；可按需记录错误并继续
            self.conn.rollback()
        return count
```

五、PostgreSQL 表结构、索引与 UPSERT（含详注）
```sql
-- -*- sql -*-
-- PostgreSQL：更强的事务与索引能力，适合中大型数据集
CREATE TABLE IF NOT EXISTS articles (
  id BIGSERIAL PRIMARY KEY,
  url TEXT UNIQUE,               -- 唯一键（幂等）
  site TEXT NOT NULL,
  title TEXT,
  content TEXT,                  -- 正文原文；也可放对象存储，DB 存摘要
  author TEXT,
  published_at TIMESTAMP WITH TIME ZONE,
  category TEXT,
  tags TEXT[],
  extra JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),
  tsv tsvector                     -- 全文索引字段（由触发器维护）
);

-- 复合索引：按站点与时间范围查询；常用筛选的覆盖索引
CREATE INDEX IF NOT EXISTS idx_articles_site_time ON articles(site, published_at DESC);
CREATE INDEX IF NOT EXISTS idx_articles_category ON articles(category);
CREATE INDEX IF NOT EXISTS idx_articles_tags_gin ON articles USING GIN(tags);
CREATE INDEX IF NOT EXISTS idx_articles_extra_gin ON articles USING GIN(extra jsonb_path_ops);
CREATE INDEX IF NOT EXISTS idx_articles_tsv_gin ON articles USING GIN(tsv);

-- 触发器：写入/更新时更新 tsv（标题权重高于正文）
CREATE OR REPLACE FUNCTION articles_tsv_update() RETURNS trigger AS $$
BEGIN
  NEW.tsv := setweight(to_tsvector('simple', coalesce(NEW.title,'')), 'A')
           || setweight(to_tsvector('simple', coalesce(NEW.content,'')), 'B');
  RETURN NEW;
END;$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS trg_articles_tsv_update ON articles;
CREATE TRIGGER trg_articles_tsv_update BEFORE INSERT OR UPDATE ON articles
FOR EACH ROW EXECUTE PROCEDURE articles_tsv_update();
```

示例 B：PostgreSQL 幂等 UPSERT 与全文检索（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 psycopg（PG 官方驱动）进行 UPSERT 与 tsquery 检索
import psycopg
from typing import Optional

class PgStore:
    def __init__(self, dsn: str):
        # 建议开启 autocommit=False，手动事务；连接池可用 psycopg_pool
        self.conn = psycopg.connect(dsn)

    def upsert_article(self, url: str, site: str, title: str, content: str,
                        author: Optional[str], published_at: Optional[str],
                        category: Optional[str], tags: list[str] | None,
                        extra: dict | None) -> int:
        """以 url 为唯一键；INSERT ON CONFLICT DO UPDATE。
        - 触发器自动维护 tsv；
        - 返回 id；
        """
        with self.conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO articles(url, site, title, content, author, published_at, category, tags, extra)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (url) DO UPDATE SET
                  site=EXCLUDED.site,
                  title=EXCLUDED.title,
                  content=EXCLUDED.content,
                  author=EXCLUDED.author,
                  published_at=EXCLUDED.published_at,
                  category=EXCLUDED.category,
                  tags=EXCLUDED.tags,
                  extra=EXCLUDED.extra
                RETURNING id
                """,
                (url, site, title, content, author, published_at, category, tags, extra)
            )
            row = cur.fetchone()
            self.conn.commit()
            return int(row[0])

    def search(self, q: str, site: Optional[str] = None, limit: int = 20) -> list[dict]:
        """全文检索：to_tsquery 与 rank 排序；
        - q 例：'新闻 & 科技'（AND）或 'OpenAI' → 'OpenAI:*'
        - 权重：标题权重 A > 正文权重 B；rank 越高越相关；
        """
        with self.conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
            if site:
                cur.execute(
                    """
                    SELECT id, url, site, title, published_at,
                           ts_rank(tsv, to_tsquery('simple', %s)) AS score
                    FROM articles
                    WHERE site=%s AND tsv @@ to_tsquery('simple', %s)
                    ORDER BY score DESC
                    LIMIT %s
                    """,
                    (q, site, q, limit)
                )
            else:
                cur.execute(
                    """
                    SELECT id, url, site, title, published_at,
                           ts_rank(tsv, to_tsquery('simple', %s)) AS score
                    FROM articles
                    WHERE tsv @@ to_tsquery('simple', %s)
                    ORDER BY score DESC
                    LIMIT %s
                    """,
                    (q, q, limit)
                )
            rows = cur.fetchall()
            return [dict(r) for r in rows]
```

六、文件存储布局与原子写入（含详注）
```python
# -*- coding: utf-8 -*-
# 将页面快照与媒体按哈希分桶存储，控制文件名安全与长度，写入原子替换
from pathlib import Path
import hashlib, os, re, json
from typing import Optional

SAFE_RE = re.compile(r"[^a-zA-Z0-9._-]")

def safe_name(name: str, max_len: int = 128) -> str:
    """规范化文件名：替换非法字符，限制长度。"""
    n = SAFE_RE.sub('_', name)
    return n[:max_len]

def hashed_subdir(key: str, fanout: int = 2) -> str:
    """按 sha1 前缀做分桶，避免单目录过多文件。"""
    h = hashlib.sha1(key.encode('utf-8')).hexdigest()
    return '/'.join([h[i] for i in range(fanout)])  # 如 'a/b'

def atomic_write(path: Path, data: bytes) -> None:
    """原子写入：先写临时文件，再替换为目标路径。"""
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = path.with_suffix(path.suffix + '.part')
    with open(tmp, 'wb') as f:
        f.write(data)
    os.replace(tmp, path)

def save_snapshot(base_dir: str, url: str, html: str, meta: dict | None = None) -> Path:
    """保存 HTML 快照与元数据 JSON。
    - 目录：base/哈希分桶/安全文件名.html
    - 元数据：同名 .json，包含 url/site/time/hash 等；
    """
    sub = hashed_subdir(url)
    name = safe_name(url) + '.html'
    html_path = Path(base_dir) / sub / name
    atomic_write(html_path, html.encode('utf-8'))
    if meta:
        meta_path = html_path.with_suffix('.json')
        atomic_write(meta_path, json.dumps(meta, ensure_ascii=False, indent=2).encode('utf-8'))
    return html_path
```

七、对象存储（S3/OSS）多段上传与元数据（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 boto3 将媒体上传到 S3，记录 etag/sha256 与业务元数据
import hashlib
import boto3
from botocore.config import Config
from typing import BinaryIO

s3 = boto3.client('s3', config=Config(signature_version='s3v4'))

def upload_stream(bucket: str, key: str, stream: BinaryIO, content_type: str, metadata: dict | None = None) -> dict:
    """流式上传对象：适合中小文件；返回 {etag, sha256, version_id}。
    - metadata 写入对象元数据（小量信息），更多信息应写入 DB。
    """
    sha256 = hashlib.sha256()
    data = stream.read()
    sha256.update(data)
    resp = s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type,
                         Metadata={k:str(v) for k,v in (metadata or {}).items()})
    return {'etag': resp['ETag'].strip('"'), 'sha256': sha256.hexdigest(), 'version_id': resp.get('VersionId')}

def ensure_object(bucket: str, key: str, expected_sha256: str) -> bool:
    """校验对象完整性：下载头部或小段比对；实际可用 ETag 或自定义哈希。"""
    head = s3.head_object(Bucket=bucket, Key=key)
    # 这里演示读取部分对象校验，真实场景可保留写入时的哈希以对比
    return bool(head)
```

八、批量写入与回压缓冲（含详注）
```python
# -*- coding: utf-8 -*-
# 写入缓冲器：聚合解析输出为批，触发条件（大小/时间）时批量提交，减少事务次数
import time
from typing import Callable, Any

class BatchBuffer:
    def __init__(self, max_items: int = 100, max_ms: int = 1000, submit: Callable[[list[Any]], None] | None = None):
        self.max_items = max_items
        self.max_ms = max_ms
        self.submit = submit or (lambda batch: None)
        self._buf: list[Any] = []
        self._ts = time.time() * 1000

    def add(self, item: Any) -> None:
        self._buf.append(item)
        now = time.time() * 1000
        if len(self._buf) >= self.max_items or (now - self._ts) >= self.max_ms:
            self.flush()

    def flush(self) -> None:
        if not self._buf:
            return
        try:
            self.submit(self._buf)
        finally:
            self._buf = []
            self._ts = time.time() * 1000
```

九、时间分区与归档（PG）（含详注）
```sql
-- -*- sql -*-
-- 按月分区：降低主表大小与索引成本；老分区可只读或归档
CREATE TABLE IF NOT EXISTS articles_parted (
  id BIGSERIAL, url TEXT, site TEXT, title TEXT, content TEXT, author TEXT,
  published_at TIMESTAMP WITH TIME ZONE, category TEXT, tags TEXT[], extra JSONB,
  created_at TIMESTAMP WITH TIME ZONE, tsv tsvector
) PARTITION BY RANGE (published_at);

-- 创建 2025 年 11 月分区示例
CREATE TABLE IF NOT EXISTS articles_2025_11 PARTITION OF articles_parted
FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');
```

示例 C：分区表写入封装（含详注）
```python
# -*- coding: utf-8 -*-
# 写入按 published_at 自动路由到对应分区（PG 分区由数据库规则处理，这里只需写主表）
import psycopg

class PgPartedStore:
    def __init__(self, dsn: str):
        self.conn = psycopg.connect(dsn)

    def upsert_article(self, url: str, site: str, title: str, content: str,
                        author: str | None, published_at: str, category: str | None,
                        tags: list[str] | None, extra: dict | None) -> int:
        with self.conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO articles_parted(url, site, title, content, author, published_at, category, tags, extra)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (url) DO UPDATE SET
                  site=EXCLUDED.site, title=EXCLUDED.title, content=EXCLUDED.content,
                  author=EXCLUDED.author, published_at=EXCLUDED.published_at,
                  category=EXCLUDED.category, tags=EXCLUDED.tags, extra=EXCLUDED.extra
                RETURNING id
                """,
                (url, site, title, content, author, published_at, category, tags, extra)
            )
            rid = cur.fetchone()[0]
            self.conn.commit()
            return int(rid)
```

十、媒体元数据与对象引用（含详注）
```sql
-- -*- sql -*-
-- 媒体表：记录对象存储位置与校验信息
CREATE TABLE IF NOT EXISTS media (
  id BIGSERIAL PRIMARY KEY,
  article_id BIGINT REFERENCES articles(id) ON DELETE CASCADE, -- 文章删除时同时删除媒体记录
  url TEXT UNIQUE,                     -- 原始媒体 URL（幂等依据）
  object_uri TEXT,                     -- 如 s3://bucket/key 或本地相对路径
  content_type TEXT,
  size BIGINT,
  md5 TEXT,
  sha256 TEXT,
  etag TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
CREATE INDEX IF NOT EXISTS idx_media_article ON media(article_id);
```

示例 D：媒体写入与校验（含详注）
```python
# -*- coding: utf-8 -*-
# 媒体下载后写入对象存储并记录元数据；重复 URL 幂等更新元信息
from typing import Optional

def upsert_media(pg: PgStore, article_id: int, url: str, object_uri: str,
                 content_type: str, size: int, md5: str | None, sha256: str | None,
                 etag: str | None) -> int:
    with pg.conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO media(article_id, url, object_uri, content_type, size, md5, sha256, etag)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (url) DO UPDATE SET
              article_id=EXCLUDED.article_id,
              object_uri=EXCLUDED.object_uri,
              content_type=EXCLUDED.content_type,
              size=EXCLUDED.size,
              md5=EXCLUDED.md5,
              sha256=EXCLUDED.sha256,
              etag=EXCLUDED.etag
            RETURNING id
            """,
            (article_id, url, object_uri, content_type, size, md5, sha256, etag)
        )
        rid = cur.fetchone()[0]
        pg.conn.commit()
        return int(rid)
```

十一、JSONB 索引与半结构化查询（含详注）
```sql
-- -*- sql -*-
-- 对 extra JSONB 中的字段做路径索引（仅在确实查询该字段时使用）
-- 例如 extra 中存 'source_id' 与 'lang'
CREATE INDEX IF NOT EXISTS idx_articles_extra_source_id ON articles USING GIN ((extra -> 'source_id'));
CREATE INDEX IF NOT EXISTS idx_articles_extra_lang ON articles USING GIN ((extra -> 'lang'));
```

示例 E：按 JSONB 字段查询（含详注）
```python
# -*- coding: utf-8 -*-
def find_by_source(pg: PgStore, source_id: str, limit: int = 50) -> list[dict]:
    with pg.conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
        cur.execute(
            """
            SELECT id, url, site, title, published_at FROM articles
            WHERE extra ->> 'source_id' = %s
            ORDER BY published_at DESC LIMIT %s
            """,
            (source_id, limit)
        )
        return [dict(r) for r in cur.fetchall()]
```

十二、读写分离与缓存（含详注）
```python
# -*- coding: utf-8 -*-
# 使用 Redis 对热点查询结果做短 TTL 缓存；注意失效策略与一致性需求
import json, redis

cache = redis.Redis(host='localhost', port=6379, db=0)

def cached_search(pg: PgStore, q: str, site: str | None, ttl: int = 30) -> list[dict]:
    key = f"search:{site or 'all'}:{q}"
    val = cache.get(key)
    if val:
        return json.loads(val)
    rows = pg.search(q, site=site, limit=50)
    cache.setex(key, ttl, json.dumps(rows, ensure_ascii=False))
    return rows
```

十三、迁移与演进（含详注）
```python
# -*- coding: utf-8 -*-
# Alembic 迁移基本骨架：记录 schema 变更，支持回滚

"""
alembic.ini 配置略；使用命令：
  alembic init migrations
  alembic revision -m "add articles table"
  alembic upgrade head
"""

from alembic import op
import sqlalchemy as sa

def upgrade():
    op.create_table(
        'articles',
        sa.Column('id', sa.BigInteger(), primary_key=True),
        sa.Column('url', sa.Text(), nullable=False, unique=True),
        sa.Column('site', sa.Text(), nullable=False),
        sa.Column('title', sa.Text()),
        sa.Column('content', sa.Text()),
        sa.Column('published_at', sa.DateTime(timezone=True)),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
    )

def downgrade():
    op.drop_table('articles')
```

十四、检索接口与分页排序（含详注）
```python
# -*- coding: utf-8 -*-
# 统一检索接口：站点 + 时间范围 + 关键字 + 标签；支持分页与排序
from dataclasses import dataclass
from typing import Optional

@dataclass
class Query:
    site: Optional[str] = None
    q: Optional[str] = None
    tag: Optional[str] = None
    start: Optional[str] = None  # ISO8601
    end: Optional[str] = None
    page: int = 1
    size: int = 20

def query_articles(pg: PgStore, query: Query) -> list[dict]:
    with pg.conn.cursor(row_factory=psycopg.rows.dict_row) as cur:
        sql = ["SELECT id, url, site, title, published_at FROM articles WHERE 1=1"]
        params = []
        if query.site:
            sql.append("AND site=%s"); params.append(query.site)
        if query.start:
            sql.append("AND published_at >= %s"); params.append(query.start)
        if query.end:
            sql.append("AND published_at < %s"); params.append(query.end)
        if query.tag:
            sql.append("AND %s = ANY(tags)"); params.append(query.tag)
        sql.append("ORDER BY published_at DESC LIMIT %s OFFSET %s")
        params.extend([query.size, (query.page - 1) * query.size])
        cur.execute(" ".join(sql), params)
        return [dict(r) for r in cur.fetchall()]
```

十五、常见陷阱与对策（存储与索引）
- 过度索引：每个新增索引都带来写入开销与维护复杂度；仅为高频查询建索引。
- 热键冲突：唯一键冲突频繁说明发现层去重不足或增量策略不当；检查 Frontier 与去重漏网。
- 长事务与锁争用：批量写入时控制批大小；避免长事务占用锁导致读阻塞。
- 大字段膨胀：正文/HTML 存储注意压缩与归档；考虑仅存摘要与外部对象引用。
- FTS 与语言：使用 simple/zhparser 等合适分词；评估中文检索效果与停用词。
- JSONB 滥用：仅在需要的少量字段使用；避免把全部字段塞入 JSONB 并索引。
- 分区迁移：跨分区更新需谨慎（通常按时间插入）；避免频繁跨表移动导致索引重建。

十六、工程整合建议
- 在 `crawler/storage.py` 提供统一 `Store` 接口（`upsert_article`/`upsert_media`/`search`）。
- 解析层输出统一字段（title/content/published_at/tags/extra），易于替换底层存储。
- 对大规模场景先用 PostgreSQL，必要时旁路接入 ES 做复杂检索；保持写路径稳定。
- 文件/对象存储与数据库元数据一致性：写入顺序采用“先对象后元数据”，失败时补偿与清理。
- 监控：写入成功率、冲突率、索引命中率、查询延时、分区大小、VACUUM/ANALYZE 周期。

结论：
- 存储与索引要以实际业务查询路径为核心，选用最简但可扩展的技术栈（SQLite/PG + S3）。
- 严格执行幂等写入与批量提交，避免重复数据与写入放大。
- 全文与结构化检索并行设计，JSONB 仅用于必要字段；时间分区与归档控制容量与成本。
- 媒体校验与原子写入确保文件可靠；迁移与演进要可回退、可审计。

—

## 专题 08：日志与监控（日志分级、指标暴露、告警、容量规划）

目标与范围（为什么做、做什么、怎么做）
- 目标：让爬虫系统可观察（可定位问题）、可告警（能及时响应）、可规划（容量可控）。
- 范围：统一日志（结构化 JSON）、暴露指标（Prometheus）、简单告警（Webhook/邮件）、容量估算与日志滚动。
- 方法：在抓取/解析/存储/调度关键路径埋点，提供可复用的日志与指标模块，配合采样与分级策略。

设计原则（落地可维护）
- 结构化日志优先：统一字段（`trace_id`、`url`、`status`、`elapsed_ms`、`error`、`component`）。
- 分级与采样：DEBUG 仅在开发或采样；INFO 记录主流程；WARN 可恢复异常；ERROR 严重失败。
- 指标要可聚合：计数器与直方图分标签（状态、域名、组件），便于在 Prometheus/Grafana 汇总与告警。
- 轻依赖与可插拔：日志写本地、输出到 stdout；指标暴露 HTTP；告警用简单 Webhook；无重型耦合。

一、结构化日志（含轮转与关联 ID）
```python
# -*- coding: utf-8 -*-
"""
结构化 JSON 日志与文件轮转
- 统一日志格式，便于后续收集（例如 Filebeat）或直接检索
- 支持按天轮转与压缩旧日志，避免长期占用磁盘
- 注入 trace_id 便于跨组件关联（抓取→解析→存储同一条链路）
"""
import json
import logging
import os
import gzip
from logging.handlers import TimedRotatingFileHandler
from typing import Optional, Dict

class JsonFormatter(logging.Formatter):
    """将日志记录转成 JSON 字符串，包含统一字段"""
    def format(self, record: logging.LogRecord) -> str:
        base = {
            "ts": self.formatTime(record, datefmt="%Y-%m-%dT%H:%M:%S"),
            "level": record.levelname,
            "logger": record.name,
            "msg": record.getMessage(),
        }
        # 额外字段：允许调用方通过 extra 注入（如 trace_id、url、component 等）
        if hasattr(record, "extra") and isinstance(record.extra, dict):
            base.update(record.extra)
        # 异常栈：仅当存在异常信息时附带（避免冗余）
        if record.exc_info:
            base["exc_info"] = self.formatException(record.exc_info)
        return json.dumps(base, ensure_ascii=False)

def get_logger(
    name: str = "crawler",
    logfile: Optional[str] = "logs/crawler.log",
    level: int = logging.INFO,
) -> logging.Logger:
    """
    获取结构化 JSON 日志记录器
    - 默认输出到文件并每日轮转；同时输出到 stdout 便于容器化环境收集
    - 通过 extra 注入统一上下文（不污染消息本身）
    """
    os.makedirs("logs", exist_ok=True)
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.propagate = False  # 避免重复输出

    fmt = JsonFormatter()

    # 文件日志：按天轮转，保留 7 个文件；在 rollover 时压缩旧文件
    if logfile:
        file_handler = TimedRotatingFileHandler(logfile, when="midnight", backupCount=7, encoding="utf-8")
        file_handler.setFormatter(fmt)
        logger.addHandler(file_handler)

        # 自定义轮转：将前一天日志压缩为 .gz，降低占用（可选）
        def compress_old_logs(handler: TimedRotatingFileHandler):
            # 仅在切换文件后尝试压缩前一日文件
            try:
                base = handler.baseFilename
                # TimedRotating 的命名规则：crawler.log.YYYY-MM-DD
                for fname in os.listdir(os.path.dirname(base)):
                    if fname.startswith(os.path.basename(base) + ".") and not fname.endswith(".gz"):
                        src = os.path.join(os.path.dirname(base), fname)
                        dst = src + ".gz"
                        if not os.path.exists(dst):
                            with open(src, "rb") as f_in, gzip.open(dst, "wb") as f_out:
                                f_out.write(f_in.read())
            except Exception:
                # 压缩失败不影响主流程
                pass

        # 将压缩动作挂到 handler 的 doRollover 之后（简单方式：包装原方法）
        original_rollover = file_handler.doRollover
        def wrapped_rollover():
            original_rollover()
            compress_old_logs(file_handler)
        file_handler.doRollover = wrapped_rollover

    # 控制台输出：适合容器 stdout 收集与开发调试
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(fmt)
    logger.addHandler(console_handler)

    return logger

def log_with_ctx(logger: logging.Logger, level: int, msg: str, ctx: Optional[Dict] = None, **extra):
    """统一入口：写日志时携带上下文（兼容 logging 原生 API）"""
    payload = {"extra": {**(ctx or {}), **extra}}
    logger.log(level, msg, extra=payload)

# 使用示例
if __name__ == "__main__":
    lg = get_logger(level=logging.DEBUG)
    trace_ctx = {"trace_id": "abc123", "component": "fetcher", "url": "https://example.com"}
    log_with_ctx(lg, logging.INFO, "计划抓取", trace_ctx)
    try:
        raise RuntimeError("网络超时")
    except Exception:
        lg.error("抓取失败", extra={"extra": trace_ctx}, exc_info=True)
```

二、Prometheus 指标（计数器 / 直方图 / 仪表）
```python
# -*- coding: utf-8 -*-
"""
Prometheus 指标设计与暴露
- 计数器：请求总数、重试次数、解析失败、存储失败等
- 直方图：请求延时、批量写入耗时、页面大小分布等
- 仪表：队列长度、并发工作者、域名并发等
"""
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from urllib.parse import urlparse

# 计数器：按状态与域名分标签，便于聚合与告警
REQUESTS_TOTAL = Counter("crawler_requests_total", "总请求次数", ["status", "domain"])
RETRIES_TOTAL = Counter("crawler_retries_total", "总重试次数", ["domain"])
PARSE_ERRORS_TOTAL = Counter("crawler_parse_errors_total", "解析失败次数", ["domain"])
STORAGE_ERRORS_TOTAL = Counter("crawler_storage_errors_total", "存储失败次数", ["table"])

# 直方图：请求延时与写入耗时（P50/P95 可视化）
REQUEST_LATENCY = Histogram("crawler_request_latency_seconds", "请求延时分布", buckets=(0.1, 0.3, 0.5, 1, 2, 3, 5, 8, 13))
BULK_FLUSH_LATENCY = Histogram("crawler_bulk_flush_latency_seconds", "批量写入耗时")
PAGE_SIZE_BYTES = Histogram("crawler_page_size_bytes", "页面大小分布", buckets=(5_000, 20_000, 50_000, 100_000, 500_000, 1_000_000, 5_000_000))

# 仪表：队列长度与并发（实时状态）
FRONTIER_SIZE = Gauge("crawler_frontier_size", "待抓取队列长度")
WORKERS_RUNNING = Gauge("crawler_workers_running", "活跃抓取工作者数量")
DOMAIN_CONCURRENCY = Gauge("crawler_domain_concurrency", "域名并发", ["domain"])

def init_metrics(port: int = 8000):
    """启动指标 HTTP 服务，默认端口 8000"""
    start_http_server(port)

def record_request(url: str, status: str, elapsed: float, size_bytes: int = 0):
    domain = urlparse(url).netloc or "unknown"
    REQUESTS_TOTAL.labels(status=status, domain=domain).inc()
    REQUEST_LATENCY.observe(elapsed)
    if size_bytes:
        PAGE_SIZE_BYTES.observe(size_bytes)

def record_retry(url: str):
    domain = urlparse(url).netloc or "unknown"
    RETRIES_TOTAL.labels(domain=domain).inc()

def record_parse_error(url: str):
    domain = urlparse(url).netloc or "unknown"
    PARSE_ERRORS_TOTAL.labels(domain=domain).inc()

def record_storage_error(table: str):
    STORAGE_ERRORS_TOTAL.labels(table=table).inc()

def set_queue_size(n: int):
    FRONTIER_SIZE.set(max(0, int(n)))

def set_workers_running(n: int):
    WORKERS_RUNNING.set(max(0, int(n)))

def set_domain_concurrency(domain: str, n: int):
    DOMAIN_CONCURRENCY.labels(domain=domain).set(max(0, int(n)))

# 使用示例
if __name__ == "__main__":
    init_metrics(8000)
    t0 = time.time()
    # 模拟一次成功请求
    time.sleep(0.2)
    record_request("https://news.example.com/a", status="200", elapsed=time.time()-t0, size_bytes=35678)
    set_queue_size(1234)
    set_workers_running(8)
    set_domain_concurrency("news.example.com", 2)
```

三、抓取层埋点（HTTP 请求包装，日志 + 指标）
```python
# -*- coding: utf-8 -*-
"""
抓取包装：在发起 HTTP 请求时统一记录结构化日志与指标
- 计时与状态归类：200/3xx/4xx/5xx/异常
- 记录页面大小：响应体字节数（便于容量与性能分析）
- 关联 trace_id：贯穿抓取→解析→存储链路
"""
import logging
import time
import requests
from typing import Optional, Dict
from .obs_logging import get_logger, log_with_ctx  # 假设将上一节日志模块保存为 obs_logging.py
from .obs_metrics import record_request, record_retry  # 假设将指标模块保存为 obs_metrics.py

class FetchError(Exception):
    pass

def fetch_with_metrics(
    url: str,
    method: str = "GET",
    headers: Optional[Dict[str, str]] = None,
    timeout: float = 15.0,
    trace_id: Optional[str] = None,
    logger: Optional[logging.Logger] = None,
) -> requests.Response:
    logger = logger or get_logger("fetcher")
    ctx = {"trace_id": trace_id, "component": "fetcher", "url": url}
    t0 = time.time()
    try:
        log_with_ctx(logger, logging.INFO, "开始抓取", ctx)
        resp = requests.request(method, url, headers=headers or {}, timeout=timeout)
        elapsed = time.time() - t0
        size_bytes = len(resp.content or b"")
        status_family = str(resp.status_code)
        record_request(url, status=status_family, elapsed=elapsed, size_bytes=size_bytes)
        log_with_ctx(logger, logging.INFO, "抓取完成", ctx, status=resp.status_code, elapsed_ms=int(elapsed*1000), size_bytes=size_bytes)
        # 简单失败分类：可扩展为重试或降级策略
        if resp.status_code >= 500:
            log_with_ctx(logger, logging.WARN, "服务器错误，考虑重试", ctx, status=resp.status_code)
        elif resp.status_code == 429:
            log_with_ctx(logger, logging.WARN, "限流，降低并发或切换代理", ctx)
        return resp
    except requests.Timeout as e:
        elapsed = time.time() - t0
        record_request(url, status="timeout", elapsed=elapsed)
        record_retry(url)
        logger.error("抓取超时", extra={"extra": ctx}, exc_info=True)
        raise FetchError("timeout") from e
    except requests.RequestException as e:
        elapsed = time.time() - t0
        record_request(url, status="request_error", elapsed=elapsed)
        logger.error("请求异常", extra={"extra": ctx}, exc_info=True)
        raise FetchError("request_error") from e
```

四、调度层埋点（队列/并发）
```python
# -*- coding: utf-8 -*-
"""
调度层：暴露队列长度与工作者并发，配合域名并发限制
"""
from .obs_metrics import set_queue_size, set_workers_running, set_domain_concurrency

class Scheduler:
    def __init__(self):
        self.frontier = []  # 简化：真实场景为去重队列/优先队列
        self.domain_running = {}

    def enqueue(self, url: str):
        self.frontier.append(url)
        set_queue_size(len(self.frontier))

    def start_worker(self, domain: str):
        self.domain_running[domain] = self.domain_running.get(domain, 0) + 1
        set_workers_running(sum(self.domain_running.values()))
        set_domain_concurrency(domain, self.domain_running[domain])

    def stop_worker(self, domain: str):
        self.domain_running[domain] = max(0, self.domain_running.get(domain, 0) - 1)
        set_workers_running(sum(self.domain_running.values()))
        set_domain_concurrency(domain, self.domain_running[domain])
```

五、存储层埋点（UPSERT 与批量写入）
```python
# -*- coding: utf-8 -*-
"""
存储层：记录写入失败与批量 flush 耗时
"""
import time
from typing import List, Dict
from .obs_logging import get_logger
from .obs_metrics import BULK_FLUSH_LATENCY, record_storage_error

logger = get_logger("storage")

def upsert_articles(conn, items: List[Dict]):
    t0 = time.time()
    try:
        # 省略具体 SQL，实现应使用事务与 UPSERT
        # ...
        elapsed = time.time() - t0
        BULK_FLUSH_LATENCY.observe(elapsed)
        logger.info("批量写入完成", extra={"extra": {"component": "storage", "elapsed_ms": int(elapsed*1000), "n": len(items)}})
    except Exception:
        record_storage_error("articles")
        logger.error("写入失败", extra={"extra": {"component": "storage"}}, exc_info=True)
        raise
```

六、告警（Webhook/邮件、窗口化错误率）
```python
# -*- coding: utf-8 -*-
"""
简单告警模块：滚动窗口计算错误率，超过阈值触发 Webhook
注意：真实告警应由 Prometheus Alertmanager 负责，这里提供轻量内嵌方案
"""
import time
import json
import threading
from collections import deque
from typing import Callable
import requests

class RollingErrorRate:
    def __init__(self, window_seconds: int = 300):
        self.window = window_seconds
        self.events = deque()  # 存放 (ts, is_error)

    def add(self, is_error: bool):
        now = time.time()
        self.events.append((now, is_error))
        # 清理过期事件
        cutoff = now - self.window
        while self.events and self.events[0][0] < cutoff:
            self.events.popleft()

    def error_rate(self) -> float:
        if not self.events:
            return 0.0
        errs = sum(1 for _, e in self.events if e)
        return errs / len(self.events)

def send_webhook(url: str, title: str, text: str):
    # 以飞书/Slack Webhook 为例
    payload = {"msg_type": "text", "content": {"text": f"[{title}] {text}"}}
    try:
        requests.post(url, data=json.dumps(payload), timeout=5)
    except Exception:
        pass

def start_error_alert_loop(rate: RollingErrorRate, threshold: float, webhook: str, interval: int = 30):
    def loop():
        while True:
            er = rate.error_rate()
            if er >= threshold:
                send_webhook(webhook, "CrawlerAlert", f"错误率 {er:.2%} 超过阈值 {threshold:.0%}")
            time.sleep(interval)
    th = threading.Thread(target=loop, daemon=True)
    th.start()

# 使用：在抓取/解析失败时调用 rate.add(True)，成功调用 rate.add(False)
```

七、容量规划（估算与日志滚动/压缩策略）
```python
# -*- coding: utf-8 -*-
"""
容量估算工具：根据日抓取量与平均页面/媒体大小估算存储增长
并给出日志滚动与压缩建议
"""
from dataclasses import dataclass

@dataclass
class CapacityPlan:
    pages_per_day: int
    avg_html_kb: float
    media_per_page: int
    avg_media_kb: float
    retention_days: int

    def estimate(self) -> dict:
        html_total_mb = self.pages_per_day * self.avg_html_kb / 1024
        media_total_mb = self.pages_per_day * self.media_per_page * self.avg_media_kb / 1024
        daily_mb = html_total_mb + media_total_mb
        monthly_gb = daily_mb * 30 / 1024
        retained_gb = daily_mb * self.retention_days / 1024
        return {
            "daily_mb": round(daily_mb, 2),
            "monthly_gb": round(monthly_gb, 2),
            "retained_gb": round(retained_gb, 2),
            "advice": "启用日志按天轮转，保留 7-14 天并压缩；媒体考虑分层存储（热/冷）。",
        }

# 使用示例
if __name__ == "__main__":
    plan = CapacityPlan(pages_per_day=200_000, avg_html_kb=200, media_per_page=1, avg_media_kb=500, retention_days=14)
    print(plan.estimate())
```

八、日志采样与分级开关（降低噪音）
```python
# -*- coding: utf-8 -*-
"""
在高 QPS 下控制日志量：仅采样部分 DEBUG，关键路径保留 INFO/WARN/ERROR
"""
import random
import logging

class DebugSampler:
    def __init__(self, rate: float = 0.05):  # 默认采样 5%
        self.rate = max(0.0, min(1.0, rate))

    def should_log(self) -> bool:
        return random.random() < self.rate

def debug_log(logger: logging.Logger, msg: str, **extra):
    sampler = extra.pop("sampler", DebugSampler())
    if sampler.should_log():
        logger.debug(msg, extra={"extra": extra})
```

九、错误分类与指标联动（统一告警口径）
```python
# -*- coding: utf-8 -*-
"""
错误分类枚举：统一指标与告警口径，避免碎片化错误描述
"""
import enum
from .obs_metrics import REQUESTS_TOTAL

class Err(enum.Enum):
    TIMEOUT = "timeout"
    DNS_FAIL = "dns_fail"
    TLS_FAIL = "tls_fail"
    HTTP_5XX = "http_5xx"
    HTTP_4XX = "http_4xx"
    PARSE_FAIL = "parse_fail"
    STORE_FAIL = "store_fail"

def record_error(url: str, err: Err):
    # 将错误类型投递到请求总数指标的 status 维度（也可单独建立 COUNTER）
    from urllib.parse import urlparse
    domain = urlparse(url).netloc or "unknown"
    REQUESTS_TOTAL.labels(status=err.value, domain=domain).inc()
```

十、端到端整合建议（放入项目）
- 在 `crawler/obs_logging.py` 中放置“结构化 JSON 日志”模块；在 `crawler/obs_metrics.py` 中放置“Prometheus 指标”模块。
- 在抓取器（如 `crawler/fetcher.py`）使用 `fetch_with_metrics` 替换原始请求函数，统一日志与指标。
- 在调度器（如 `crawler/scheduler.py`）周期性调用 `set_queue_size/set_workers_running/set_domain_concurrency` 更新实时仪表。
- 在存储层（如 `crawler/storage.py`）批量写入时调用直方图埋点并在异常时 `record_storage_error("articles")`。
- 在入口进程（如 `main.py`）启动时调用 `init_metrics(8000)` 暴露指标 HTTP；必要时按环境变量调整端口。
- 告警选型：优先使用 Prometheus + Alertmanager；若不具备，临时采用 RollingErrorRate + Webhook。
- 采样策略：生产环境将 DEBUG 采样率降到 1%-5%；开发环境关闭采样以完整输出。

Grafana/Prometheus 仪表建议（可落地的面板）
- 请求总数与错误率：`sum by(status)(rate(crawler_requests_total[5m]))`，重点关注 `timeout/http_5xx`。
- 延时 P95：`histogram_quantile(0.95, sum(rate(crawler_request_latency_seconds_bucket[5m])) by (le))`。
- 队列长度与并发：`crawler_frontier_size`、`crawler_workers_running`、`crawler_domain_concurrency{domain="news.example.com"}`。
- 批量写入耗时：`histogram_quantile(0.9, sum(rate(crawler_bulk_flush_latency_seconds_bucket[5m])) by (le))`。
- 页面大小分布：`sum(rate(crawler_page_size_bytes_bucket[5m])) by (le)`，用于容量与带宽分析。

迁移与注意事项
- 日志字段保持稳定（新增字段尽量向后兼容），避免解析器或面板频繁调整。
- 指标标签需受控（域名/状态枚举），避免高基数导致 Prometheus 负载过高。
- 端口暴露需限制访问（容器内网或加反向代理鉴权），避免指标泄露。
- 长期运行注意日志文件数与压缩策略，避免磁盘上限；同时监控 `retained_gb` 与对象存储费用。

落地小结
- 通过“结构化日志 + Prom 指标 + 轻量告警 + 容量规划”，让爬虫在工程上“可观测、可预警、可扩展”。
- 以上模块互相独立、可插拔，先从入口进程暴露指标与统一日志开始，逐步加到抓取/解析/存储关键路径。

—

## 专题 09：工程与安全（测试/CI/CD/部署、账号风控、审计）

目标与范围（让系统“可测、可发、可控、可审”）
- 目标：构建可回归的测试体系、可持续的 CI/CD、可灰度的安全部署、可合规的审计记录。
- 范围：单元/集成测试、HTTP 模拟与离线快照、CI 工作流、部署策略与回滚、密钥与账号风控、审计与合规。
- 方法：以“最小依赖、强注释、可插拔”为准则，在现有 `crawler` 模块基础上提供落地示例。

一、测试体系（单元 + 快照 + 模拟 HTTP）
```python
# -*- coding: utf-8 -*-
"""
示例 A：解析函数的单元测试（pytest）
- 目的：确保解析逻辑在结构轻微变化时仍容错，便于版本回归
- 方法：参数化用例 + 选择器回退 + 边界值（空、缺字段）
"""
import pytest
from bs4 import BeautifulSoup

def parse_title(html: str) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    # 容错选择器：优先 h1.article-title，回退 h1#title，再回退 data-title
    node = soup.select_one('h1.article-title') or soup.select_one('h1#title')
    if node:
        return node.get_text(strip=True)
    # 回退：某些站点把标题放在 data-title 属性
    dt = soup.select_one('[data-title]')
    return dt['data-title'].strip() if dt and dt.has_attr('data-title') else ''

@pytest.mark.parametrize("html,expected", [
    ('<h1 class="article-title">示例标题</h1>', '示例标题'),
    ('<h1 id="title">另一种标题</h1>', '另一种标题'),
    ('<div data-title="隐含标题"></div>', '隐含标题'),
    ('<div>无标题</div>', ''),
])
def test_parse_title_variants(html, expected):
    assert parse_title(html) == expected
```

```python
# -*- coding: utf-8 -*-
"""
示例 B：离线 HTML 快照回归测试
- 目的：解析器在真实页面快照上执行，避免线上结构变更影响测试稳定
- 方法：使用本地文件（来自抓取时保存的快照），限定字段断言
"""
from pathlib import Path
from bs4 import BeautifulSoup

def parse_fields(html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    title = (soup.select_one('h1.article-title') or soup.select_one('h1#title'))
    time_node = soup.select_one('time[datetime]')
    return {
        'title': title.get_text(strip=True) if title else '',
        'published_at': time_node['datetime'] if time_node and time_node.has_attr('datetime') else '',
    }

def test_snapshot_parse():
    snap = Path('tests/fixtures/news_example.html')  # 将真实快照置于 fixtures 目录
    html = snap.read_text(encoding='utf-8')
    data = parse_fields(html)
    # 断言仅关注稳定字段，避免样式与微文案变更导致脆弱测试
    assert data['title']
    assert data['published_at'].startswith('20')  # ISO 年份前缀
```

```python
# -*- coding: utf-8 -*-
"""
示例 C：模拟 HTTP（httpx + respx）验证抓取器重试与退避
- 目的：不依赖真实网络，验证 5xx/429/网络错误的重试分支与退避
- 方法：respx 拦截 httpx 请求，根据尝试次数返回不同响应
"""
import pytest
import respx
import httpx
import asyncio

from crawler.fetcher import Fetcher

@pytest.mark.asyncio
async def test_fetcher_retry_backoff(monkeypatch):
    # 构造 fetcher，禁用 robots 以简化测试路径
    f = Fetcher(
        ua_pool=["UA"], proxies=[], per_domain_delay_ms=0,
        max_retries=2, backoff_initial_ms=10, backoff_max_ms=50,
        respect_robots=False,
    )

    calls = {"n": 0}
    @respx.mock
    async def run():
        respx.get("https://example.com/").mock(side_effect=lambda request: _resp(calls))
        status, text = await f.fetch("https://example.com/")
        assert status == 200 and text == "ok"

    def _resp(calls):
        calls["n"] += 1
        if calls["n"] == 1:
            return httpx.Response(503, text="unavailable")  # 第一次返回可重试错误
        elif calls["n"] == 2:
            return httpx.Response(429, text="too many")     # 第二次限流，继续重试
        else:
            return httpx.Response(200, text="ok")           # 第三次成功

    try:
        await run()
    finally:
        await f.close()
```

二、集成测试（端到端小管线）
```python
# -*- coding: utf-8 -*-
"""
示例 D：小型端到端测试（队列→抓取→存储）
- 目的：确保核心管线可运行、无异常、状态号与持久化记录正确
- 方法：使用内存 URL、模拟 HTTP、写入临时 SQLite（或内存 DB）
"""
import asyncio
import sqlite3
import tempfile
from crawler.scheduler import Scheduler
from crawler.fetcher import Fetcher
from crawler.storage import Storage

async def test_end_to_end(tmp_path):
    # 临时 SQLite 与输出目录
    db = tmp_path / "test.db"
    out = tmp_path / "out"; out.mkdir()
    storage = Storage(str(db), str(out), html_subdir="pages")

    fetcher = Fetcher(
        ua_pool=["UA"], proxies=[], per_domain_delay_ms=0,
        max_retries=0, backoff_initial_ms=10, backoff_max_ms=50,
        respect_robots=False,
    )

    # 调度器：仅一个种子，allowed_domains 可留空以放行
    scheduler = Scheduler(["https://example.com/"], [])

    async def worker(url: str):
        # 直接模拟成功响应（绕过真实网络）
        status, html = 200, "<html><body>ok</body></html>"
        storage.record_page(url, status)
        storage.save_html(url, html)

    await scheduler.run(worker, concurrency=1)
    # 验证 SQLite 里有记录
    conn = sqlite3.connect(str(db))
    n = conn.execute("SELECT COUNT(1) FROM pages").fetchone()[0]
    conn.close()
    assert n == 1
    await fetcher.close()
```

三、CI/CD（GitHub Actions 示例 + 分环境配置）
```yaml
# .github/workflows/python-ci.yml（示例）
name: CI
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest respx httpx beautifulsoup4
      - name: Run tests
        run: pytest -q
      - name: Lint (可选)
        run: |
          pip install flake8
          flake8 crawler

  build-release:
    needs: [ test ]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - name: Package
        run: |
          mkdir -p dist
          zip -r dist/nor_crawler.zip crawler plugins requirements.txt
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: nor_crawler
          path: dist/nor_crawler.zip
```

CI 的工程建议
- 测试分级：为需要真实网络的测试标注 `@pytest.mark.network`，在 CI 默认跳过；仅在预生产或手动触发时运行。
- 分环境配置：`config.example.json` 仅保留通用字段；敏感信息通过环境变量或 Secrets 注入（见安全章节）。
- 产物管理：将 `dist/*` 作为可部署包；使用版本号或 Commit Hash 标识，便于回滚。

四、部署与回滚（计划任务、灰度、早停）
```powershell
# Windows 计划任务示例（PowerShell）
# 每小时执行一次，传入配置路径；输出日志到文件（配合专题 08 轮转/压缩）
$action = New-ScheduledTaskAction -Execute "python" -Argument "-m crawler.cli --config config\\alljavxx.json" -WorkingDirectory "C:\\crawler"
$trigger = New-ScheduledTaskTrigger -Once -At (Get-Date) -RepetitionInterval (New-TimeSpan -Minutes 60) -RepetitionDuration ([TimeSpan]::MaxValue)
Register-ScheduledTask -TaskName "nor_crawler_hourly" -Action $action -Trigger $trigger -Description "Hourly crawl"
```

```python
# -*- coding: utf-8 -*-
"""
示例 E：早停与灰度控制（入口参数开关）
- 目的：避免全量扩散，先在小规模灰度运行，满足条件后再扩大
- 方法：限制分页上限、任务上限、失败阈值触发停止
"""
import argparse

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--max-pages', type=int, default=100, help='分页上限（灰度）')
    p.add_argument('--max-tasks', type=int, default=2000, help='任务上限（灰度）')
    p.add_argument('--fail-stop', type=int, default=100, help='失败计数超过则早停')
    return p.parse_args()
```

五、安全（密钥管理、最小权限、账号风控）
```python
# -*- coding: utf-8 -*-
"""
示例 F：密钥加载器（ENV/JSON），避免明文硬编码
- 目的：统一读入敏感信息（Cookie/Token/Webhook），只在需要处使用
- 方法：优先环境变量，其次读取受限权限的 JSON 文件
"""
import os, json
from typing import Optional

def get_secret(name: str, fallback_file: Optional[str] = None) -> str:
    # 优先从环境变量读取（CI/CD、容器）
    val = os.environ.get(name)
    if val:
        return val
    # 其次从本地 JSON 文件读取（权限控制，避免共享盘）
    if fallback_file and os.path.exists(fallback_file):
        try:
            data = json.load(open(fallback_file, 'r', encoding='utf-8'))
            if name in data:
                return str(data[name])
        except Exception:
            pass
    return ""  # 若不存在，调用方自行处理（降级/禁用相关功能）
```

```python
# -*- coding: utf-8 -*-
"""
示例 G：账号风控（滑窗异常检测 + 限流暂停）
- 目的：登录/访问异常集中时，自动降速或暂停避免触发站点封禁
- 方法：滚动窗口计数错误率，达到阈值对账号或域进行暂停
"""
import time
from collections import deque, defaultdict
from urllib.parse import urlparse

class AccountGuard:
    def __init__(self, window_sec: int = 300, pause_sec: int = 600, threshold: float = 0.3):
        self.window = window_sec
        self.pause_sec = pause_sec
        self.threshold = threshold
        self.events = defaultdict(lambda: deque())      # domain -> deque[(ts, is_err)]
        self.paused_until = defaultdict(int)            # domain -> ts

    def record(self, url: str, is_error: bool):
        d = urlparse(url).netloc or 'unknown'
        now = time.time(); ev = self.events[d]
        ev.append((now, is_error))
        cutoff = now - self.window
        while ev and ev[0][0] < cutoff:
            ev.popleft()
        self._update_pause(d)

    def _update_pause(self, domain: str):
        ev = self.events[domain]
        if not ev:
            return
        rate = sum(1 for _, e in ev if e) / len(ev)
        if rate >= self.threshold:
            self.paused_until[domain] = int(time.time() + self.pause_sec)

    def is_paused(self, domain: str) -> bool:
        return time.time() < self.paused_until.get(domain, 0)
```

六、审计与合规（安全事件与敏感操作日志）
```python
# -*- coding: utf-8 -*-
"""
示例 H：审计日志（JSON）记录敏感操作（登录/下载媒体/写入数据库）
- 目的：满足合规要求，可追溯谁在何时做了什么、是否成功
- 方法：结构化字段 + 统一入口，避免遗漏
"""
import json, logging
from datetime import datetime

class AuditLogger:
    def __init__(self, path: str = 'logs/audit.log'):
        self.path = path
        self.logger = logging.getLogger('audit')
        self.logger.setLevel(logging.INFO)
        self.handler = logging.FileHandler(self.path, encoding='utf-8')
        self.logger.addHandler(self.handler)

    def write(self, actor: str, action: str, resource: str, ok: bool, trace_id: str = ''):
        rec = {
            'ts': datetime.utcnow().isoformat(),
            'actor': actor,            # 执行者（进程/模块/账号标识）
            'action': action,          # 操作类型：login/download/insert/update
            'resource': resource,      # 目标资源：URL/表名/文件路径
            'ok': bool(ok),
            'trace_id': trace_id,
        }
        # 避免敏感数据泄露：不记录 Cookie/Token 的明文，只保留哈希摘要（如需）
        self.logger.info(json.dumps(rec, ensure_ascii=False))
```

七、工程建议与落地
- 测试：在 `tests/` 下组织单元与集成测试；快照放 `tests/fixtures/`；网络类测试打上标记避免 CI 默认执行。
- CI/CD：先跑测试与静态检查，产出版本化 ZIP；部署流程可在私有环境解包运行。
- 部署：生产环境使用计划任务或守护进程；按域并发与速率配置灰度；启用早停阈值避免扩散性错误。
- 安全：敏感信息由环境变量或受限 JSON 文件注入，模块内仅在必要时读取；账号风控与综错暂停结合专题 03 的限速器。
- 审计：统一使用 `AuditLogger` 写入操作记录，配合专题 08 的结构化日志与指标，形成完整可观测链路。

扩展阅读与工具选择
- 测试：pytest、responses/respx（HTTP 模拟）、pytest-cov（覆盖率）、hypothesis（属性化测试）。
- CI/CD：GitHub Actions、GitLab CI、Jenkins；容器化建议配合 Docker 与 Compose。
- 安全：KMS/Secrets Manager（云环境）、sops（本地密文管理）。
- 审计：Filebeat/Fluentd 收集审计日志，集中到 Elasticsearch 或对象存储归档。

—

## 专题 10：分布式爬虫（消息队列、Frontier 分片、集中去重与一致性）

目标与范围（把单机变为“稳态分布式”）
- 目标：让抓取在多进程/多机器下稳定扩展，保证“尽可能不重复（集中去重）、尽可能不丢失（至少一次）、尽可能无副作用（幂等写入）”。
- 范围：消息队列（Redis/RabbitMQ/云MQ）、Frontier 分片与一致性哈希、集中去重服务（Redis/DB）、消费者组与重分配、死信与重试、观测与限流。
- 方法：以“轻耦合、强注释、可替换”为原则提供代码骨架；生产建议使用托管 MQ（提升可靠性）。

一、架构总览（角色与数据流）
- Producer（发现/生成任务）：插件或解析器将新 URL 推入队列；对 URL 做规范化与打分（优先级）。
- Deduper（集中去重）：以原子操作（Redis `SETNX`/DB 唯一索引）防止重复任务入队。
- Queue（消息队列）：承担跨进程/跨机器的任务分发；支持消费确认与重试。
- Consumer（抓取/解析/存储）：从队列拉取任务，应用域级限速与失败退避，幂等写入存储。
- Monitor（观测与控制）：队列深度、消费者滞后、失败率、域并发；告警与暂停机制（见专题 08）。

二、集中去重服务（Redis SETNX 与 TTL）
```python
# -*- coding: utf-8 -*-
"""
集中去重：使用 Redis SETNX 原子检查并标记，避免重复入队
- 设计：key 使用规范化 URL 的哈希；值可为简单标记或过期时间
- TTL：对某些场景（如列表页）可以设置短 TTL，允许一段时间后重抓增量
"""
import hashlib
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def url_key(url: str) -> str:
    # 真实场景请使用专题 05 的 URL 规范化；此处用 sha256 作为唯一键示例
    return hashlib.sha256(url.encode('utf-8')).hexdigest()

def try_claim(url: str, ttl_sec: int | None = None) -> bool:
    key = f"seen:{url_key(url)}"
    if ttl_sec:
        # Redis >= 2.6: SET key value NX EX seconds
        return bool(r.set(key, 1, ex=int(ttl_sec), nx=True))
    else:
        return bool(r.setnx(key, 1))
```

三、队列模型选型（列表/有序集/流）
- 列表（`LPUSH`/`BRPOP`）：简单、易用；缺点是不可直接实现优先级与消费确认。
- 有序集（`ZADD`/`ZPOPMIN`）：可实现优先级；需要额外保证“取出即不可见”（事务或 Lua 脚本）。
- 流（Redis Streams `XADD`/`XREADGROUP`）：原生消费组与确认（`XACK`）；可以重分配挂起消息；更接近 MQ 语义。

示例 A：使用 Redis Streams 搭建基本队列（含消费组与确认）
```python
# -*- coding: utf-8 -*-
"""
Redis Streams 基础用法（单流 + 消费组）
- XADD：生产者添加消息；字段包含 URL、优先级、trace_id 等
- XREADGROUP：消费者组读取消息（阻塞）；读到后处理并 XACK 确认
- XPENDING：监控挂起消息，超过一定时间未确认则重分配（消费者崩溃场景）
"""
import redis, time

STREAM = 'frontier'
GROUP = 'workers'
CONSUMER = 'worker-1'

r = redis.Redis(host='localhost', port=6379, db=0)

def ensure_group():
    try:
        r.xgroup_create(STREAM, GROUP, id='0', mkstream=True)
    except redis.ResponseError:
        # 组已存在，忽略
        pass

def enqueue(url: str, priority: int = 100, trace_id: str = ''):
    # priority 仅作为字段存储；真实优先级可用多流或分数（见 ZSet 方案）
    r.xadd(STREAM, {'url': url, 'pri': str(priority), 'trace': trace_id})

def consume_blocking(process_fn, block_ms: int = 5000):
    ensure_group()
    while True:
        # '>' 表示从未投递给本组的消息；若要补领挂起消息可用 '0'
        resp = r.xreadgroup(GROUP, CONSUMER, {STREAM: '>'}, count=1, block=block_ms)
        if not resp:
            continue
        # resp 格式：[(stream, [(id, fields), ...])]
        _, messages = resp[0]
        for mid, fields in messages:
            url = fields.get(b'url', b'').decode('utf-8')
            ok = False
            try:
                ok = process_fn(url)
            except Exception:
                ok = False
            finally:
                # 成功则确认；失败可选择不确认，让其进入挂起，由后台重试或转死信
                if ok:
                    r.xack(STREAM, GROUP, mid)
                else:
                    # 也可直接写入“失败流”，或记录重试次数（见后续重试策略）
                    pass
```

示例 B：挂起消息重分配（消费者故障处理）
```python
# -*- coding: utf-8 -*-
"""
XPENDING + XCLAIM 处理长时间未确认的挂起消息
- 思路：定期扫描 PEL（挂起列表），超过阈值的消息由健康消费者“领取”重新处理
"""
def reassign_stale(pending_min_ms: int = 30000):
    # XPENDING 返回：count, min, max, consumers
    info = r.xpending(STREAM, GROUP)
    total = info['pending'] if isinstance(info, dict) else info[0]
    if not total:
        return 0
    # 列出具体挂起消息（仅取前 N 条）
    items = r.xpending_range(STREAM, GROUP, min='-', max='+', count=50)
    claim = []
    now = int(time.time() * 1000)
    for entry in items:
        mid = entry['message_id']
        idle = entry['idle']  # 挂起时长（毫秒）
        if idle >= pending_min_ms:
            claim.append(mid)
    if claim:
        # 将长时间挂起的消息领取到当前消费者
        r.xclaim(STREAM, GROUP, CONSUMER, min_idle_time=pending_min_ms, message_ids=claim)
    return len(claim)
```

四、优先级队列（ZSet + Lua 原子取出）
```python
# -*- coding: utf-8 -*-
"""
基于有序集实现优先级 Frontier：分数越小越优先（如越新越小）
- 原子取出：使用 Lua 脚本 ZPOPMIN + 写入“进行中集合”避免多消费者抢同一条
"""
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

LUA_POP = r.register_script("""
local q = KEYS[1]
local inprog = KEYS[2]
local res = redis.call('ZPOPMIN', q, 1)
if #res == 0 then return nil end
local url = res[1]
redis.call('HSET', inprog, url, 1)
return url
""")

def zadd_task(url: str, score: float):
    r.zadd('frontier_z', {url: score})

def pop_task() -> str | None:
    url = LUA_POP(keys=['frontier_z', 'inprog'])
    return url.decode('utf-8') if url else None
```

五、分片与一致性哈希（按域或哈希范围）
```python
# -*- coding: utf-8 -*-
"""
一致性哈希将任务按域或 URL 哈希分布到多个分片（队列）
- 目的：减少跨域干扰，便于域级并发控制；支持横向扩展
"""
import hashlib

def shard_id(url: str, shards: int = 8) -> int:
    # 按域名或 URL 哈希；此处使用 sha1 简化
    h = hashlib.sha1(url.encode('utf-8')).hexdigest()
    return int(h[:8], 16) % max(shards, 1)

def enqueue_sharded(url: str, shards: int = 8):
    sid = shard_id(url, shards)
    r.xadd(f'frontier:{sid}', {'url': url})
```

六、消费者（抓取/解析/存储）骨架与幂等写入
```python
# -*- coding: utf-8 -*-
"""
消费者骨架：从分片队列拉取 → 域级限速 → 抓取 → 解析 → 幂等入库 → 确认
- 域级限速：防止对单域名压测（见专题 03/04 的 DomainGate）
- 幂等入库：使用唯一索引或 UPSERT 保证重复写入不产生副作用（见专题 07）
"""
import asyncio
import logging
import sqlite3
from urllib.parse import urlparse

from crawler.fetcher import Fetcher
from crawler.storage import Storage

log = logging.getLogger('consumer')

class DomainGate:
    def __init__(self, base_delay: float = 0.5, max_conc_per_domain: int = 2):
        self.last = {}
        self.sema = {}
        self.base = base_delay
        self.maxc = max_conc_per_domain
    def _dom(self, url: str) -> str:
        return urlparse(url).netloc
    async def acquire(self, url: str):
        d = self._dom(url)
        if d not in self.sema:
            self.sema[d] = asyncio.Semaphore(self.maxc)
            self.last[d] = 0.0
        await self.sema[d].acquire()
        now = asyncio.get_event_loop().time()
        need = self.base
        since = now - self.last[d]
        if since < need:
            await asyncio.sleep(need - since)
        self.last[d] = asyncio.get_event_loop().time()
    def release(self, url: str):
        self.sema[self._dom(url)].release()

async def worker_loop(shard: int, storage: Storage, fetcher: Fetcher, gate: DomainGate):
    ensure_group()  # 建立消费组（示例基于单流，可扩展为分片流）
    while True:
        resp = r.xreadgroup(GROUP, f'worker-{shard}', {f'frontier:{shard}': '>'}, count=1, block=5000)
        if not resp:  # 无消息，继续阻塞等待
            continue
        _, msgs = resp[0]
        for mid, fields in msgs:
            url = fields.get(b'url', b'').decode('utf-8')
            await gate.acquire(url)
            try:
                status, html = await fetcher.fetch(url)
                storage.record_page(url, status)  # 幂等：INSERT OR REPLACE（见 Storage）
                if status == 200:
                    storage.save_html(url, html)
                r.xack(f'frontier:{shard}', GROUP, mid)
            except Exception:
                log.exception('处理失败：%s', url)
                # 失败不确认，留在挂起列表，供后台重试或转死信
            finally:
                gate.release(url)
```

七、重试策略与死信队列（按失败类型分流）
```python
# -*- coding: utf-8 -*-
"""
重试与死信：按失败类型分流到不同队列，提高可观测与调优（见专题 08 的错误分类）
"""
def route_failure(url: str, fail_type: str):
    # 例：429/403 -> 限流队列；5xx/网络 -> 短时间重试队列；解析失败 -> 人工处理队列
    r.xadd(f'fail:{fail_type}', {'url': url})

def retry_from(queue: str, max_attempts: int = 3):
    # 简化：从某失败队列读取并有限次重试；超过次数转死信
    resp = r.xread({queue: '0'}, count=10, block=0)
    for _, msgs in resp:
        for mid, fields in msgs:
            url = fields.get(b'url', b'').decode('utf-8')
            # ...进行重试（略）
            # 成功：XACK + 删除；失败：写入 dead-letter
            r.xadd('dead', {'url': url, 'from': queue})
```

八、观测（队列深度/滞后/失败率）与限流联动
```python
# -*- coding: utf-8 -*-
"""
结合专题 08 的 Prometheus 指标：
- 队列深度：各分片 frontier:* 的长度；用于容量与速度调参
- 消费者滞后：XPENDING 挂起数；过高说明消费者不足或失败率高
- 失败率：各失败队列的入流速率；联动账户/域限流（专题 03 的风控）
"""
from prometheus_client import Gauge

Q_DEPTH = Gauge('frontier_depth', '队列长度', ['shard'])
Q_PENDING = Gauge('frontier_pending', '挂起消息数', ['shard'])

def refresh_metrics(shards: int = 8):
    for sid in range(shards):
        depth = r.xlen(f'frontier:{sid}')
        Q_DEPTH.labels(shard=str(sid)).set(depth)
        info = r.xpending(f'frontier:{sid}', GROUP)
        pend = info['pending'] if isinstance(info, dict) else info[0]
        Q_PENDING.labels(shard=str(sid)).set(pend or 0)
```

九、语义选择：至少一次 vs 近似恰好一次
- 至少一次（At-least-once）：最常见，保证“不丢任务”；需要幂等写入避免重复副作用。
- 近似恰好一次（Quasi exactly-once）：通过“集中去重 + 幂等存储 + 任务确认”组合，降低重复概率；严格 exactly-once 代价与复杂度很高，不推荐在爬虫场景强求。

十、工程整合建议（如何放入你的项目）
- 在 `plugins/*` 的解析结果中，将新发现 URL 经 `try_claim` 去重后才入队；提供 `enqueue_sharded(url)` 作为分布式入口。
- 将现有 `crawler/scheduler.py` 的 `asyncio.Queue` 替换为可配置：本地队列或 Redis Streams；保留相同 `worker` 处理函数签名，降低改造成本。
- `crawler/fetcher.py` 保持不变；在消费者侧按域应用 `DomainGate` 控制并发与最小间隔。
- `crawler/storage.py` 保持 UPSERT/幂等写入；配合失败分流与死信，避免重复导致副作用。
- 观测：接入专题 08 的指标与日志模块，新增队列深度与挂起数指标，设定告警阈值与自动扩缩容策略。

十一、注意事项与踩坑
- 高基数标签：Prometheus 标签请限制维度（如 shard/domain），避免高基数导致存储膨胀。
- 任务粒度：尽量细粒度（单 URL 为单位），避免“大任务”阻塞队列与并发。
- 失败退避：`429/403` 走较长退避队列；`5xx/网络` 走指数退避短队列；解析失败尽量自动降级或人工审查。
- 托管 MQ：生产使用云 MQ（如云 Redis/RabbitMQ/Kafka），以获得持久化与运维支持；自建需注意持久化与快照恢复。
- 数据一致性：集中去重与写入幂等配合使用；必要时在 DB 层建立唯一索引作为最终防线。

十二、RabbitMQ 对照（简版示意）
```python
# -*- coding: utf-8 -*-
"""
RabbitMQ（AMQP）简版：持久队列 + 手动确认 + 死信交换
- 适合需要更强交付保证的场景；示意为对照，不强依赖
"""
import pika

def rmq_producer(ch, queue='frontier'):
    ch.queue_declare(queue=queue, durable=True)
    ch.basic_publish(
        exchange='', routing_key=queue,
        body=b'https://example.com/',
        properties=pika.BasicProperties(delivery_mode=2),  # 持久消息
    )

def rmq_consumer(ch, queue='frontier'):
    ch.queue_declare(queue=queue, durable=True)
    def on_msg(ch, method, props, body):
        url = body.decode('utf-8')
        ok = True  # 处理逻辑...
        if ok: ch.basic_ack(delivery_tag=method.delivery_tag)
        else: ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)  # 入死信
    ch.basic_qos(prefetch_count=1)
    ch.basic_consume(queue=queue, on_message_callback=on_msg)
    ch.start_consuming()
```

落地小结
- 使用“集中去重 + 队列消费组 + 分片 + 幂等写入 + 观测告警”的组合，让分布式爬虫在工程上稳定、可控、可扩展。
- 优先选择 Redis Streams 或托管 MQ；按域或哈希分片控制公平与速率；以 DB 唯一索引兜底一致性。

—

## 专题 11：高级解析（结构变更、模板匹配、SimHash 近重复检测）

目标与范围（把“解析易碎”变为“解析稳态”）
- 目标：面对站点结构变更、选择器失效、噪声污染、近重复内容，使解析仍然稳定产出结构化数据，且维护成本可控。
- 范围：模板库与选择器分级回退、DOM 版本漂移识别与差异修复、正文抽取与噪声过滤、多语言与格式处理、近重复检测与去重、解析审计与测试回归。
- 方法：提供“设计要点 + 详注示例代码”，强调可观测与可维护，避免“写死选择器”导致频繁崩溃。

一、结构变更的鲁棒性策略（分层解耦）
- 入口识别：按 URL 模式（如 `/news/`）、DOM 大致特征（如 `article` 容器）先判类，再套具体模板。
- 选择器分级：主选择器失败时，尝试备用选择器（同语义不同路径）；最后回退到启发式抽取（如最大文本块）。
- 容器先行：先定位语义容器（正文、标题、作者、时间），在容器内再进行细粒度选择，降低结构变化影响面。
- 清洗优先：先行剔除噪声容器（广告/脚注/分享/推荐），再解析正文，避免污染。
- 可观测性：记录“命中选择器、命中次数、失败原因、替代选择器是否触发”，便于后续维护与模板修复。

二、模板库与选择器分级回退（策略设计）
- 模板定义：每个模板包含“触发条件 + 字段选择器 + 备用选择器 + 清洗规则 + 版本号”。
- 触发条件：URL 模式（正则）、DOM 特征（某节点存在/缺失）、站点标识（域名/路径）。
- 评分与匹配：多个模板可竞争；按“触发命中数 + 字段命中率 + 历史成功率”评分，选最高者。
- 版本管理：模板随站点变更升级版本号；解析日志记录版本号，便于定位历史差异。

示例 A：鲁棒选择器与回退（含详注）
```python
# -*- coding: utf-8 -*-
"""
鲁棒选择器：主/备选择器分级回退 + 清洗规则
- 使用 BeautifulSoup 演示；真实场景可用 lxml/bs4 并预编译 XPath
"""
from bs4 import BeautifulSoup
from typing import Optional, Callable

class CleanRule:
    def __init__(self, blacklist_selectors: list[str]):
        self.blacklist = blacklist_selectors
    def apply(self, soup: BeautifulSoup):
        for sel in self.blacklist:
            for n in soup.select(sel):
                n.decompose()  # 移除噪声节点，避免正文污染

def robust_text(soup: BeautifulSoup, selectors: list[str], post: Optional[Callable[[str], str]] = None) -> Optional[str]:
    # 逐级尝试选择器；命中第一个即返回
    for sel in selectors:
        n = soup.select_one(sel)
        if n:
            text = n.get_text(separator=' ', strip=True)
            return post(text) if post else text
    return None  # 所有选择器失败时返回 None，由上层决定回退策略

def extract_article(html: str) -> dict:
    soup = BeautifulSoup(html, 'html.parser')
    # 1) 清洗噪声容器（广告/脚注/分享区等）
    CleanRule(['.advert', '.share', '.footer', 'script', 'style']).apply(soup)
    # 2) 标题：主/备选择器回退
    title = robust_text(soup, [
        'h1.article-title',  # 主选择器
        'header h1',         # 备1：结构稍变
        '.title h1',         # 备2：站点改版后的路径
    ])
    # 3) 时间：正则/属性 + 多格式处理（见专题 13）
    published = robust_text(soup, [
        'time[datetime]',       # 现代语义标签
        'meta[property="article:published_time"]',
        'span.pub-date',
    ])
    # 4) 正文：优先容器，再拼接段落；失败时回退到最大文本块
    content_node = soup.select_one('article .content') or soup.select_one('#content') or soup.select_one('.post-body')
    if content_node:
        paras = [p.get_text(' ', strip=True) for p in content_node.select('p')]
        content = '\n'.join([p for p in paras if p])
    else:
        # 回退：选择文本量最大的容器作为正文（粗略启发式）
        candidates = sorted((n for n in soup.find_all(True)), key=lambda n: len(n.get_text(strip=True)), reverse=True)
        content = candidates[0].get_text('\n', strip=True) if candidates else ''
    return {'title': title, 'published': published, 'content': content}
```

三、字段抽取稳定化（容错与清洗）
- 规范化：统一空白与换行，移除多余空格与不可见字符；统一全角半角标点。
- 噪声剔除：删除版权声明、分享提示、相关推荐等非正文内容；可维护黑名单选择器。
- 容错格式：日期/价格/数量等字段按多格式匹配；失败时标注为“未知/待确认”。
- 半结构化增强：对正文中的结构化片段（表格/列表/JSON-LD）优先抽取并规范化。

示例 B：字段清洗与规范化（含详注）
```python
# -*- coding: utf-8 -*-
"""
字段清洗：空白统一、标点归一、版权/分享等噪声剔除
"""
import re

SPACE_RE = re.compile(r"\s+")
FULL2HALF = str.maketrans({'，': ',', '。': '.', '；': ';', '：': ':', '（': '(', '）': ')'})

def canonical_text(s: str) -> str:
    s = s.translate(FULL2HALF)
    s = SPACE_RE.sub(' ', s).strip()
    return s

def strip_noise(text: str) -> str:
    noise = [
        r"^版权.*$", r"^未经授权.*$", r"^分享到.*$", r"^相关推荐.*$"
    ]
    lines = [l for l in text.split('\n') if l and not any(re.search(p, l) for p in noise)]
    return '\n'.join(lines)
```

四、DOM 漂移与差异修复（Diff + 审计）
- 漂移识别：在解析失败时，将快照与成功案例进行 DOM Diff，定位结构变化位置（节点增删/属性变更）。
- 差异修复：为变更位置添加备用选择器或清洗规则；记录模板版本升级与变更摘要。
- 自动化辅助：为常见变更（新容器包裹、class 改名）提供半自动修复建议。

示例 C：极简 DOM Diff（含详注，思路演示）
```python
# -*- coding: utf-8 -*-
"""
极简 DOM Diff：比较两个 HTML 快照的结构差异（节点数/特定选择器存在性）
实际可使用更强工具（如 gumtree/lxml 比对），这里只演示思路
"""
from bs4 import BeautifulSoup

def dom_has(html: str, selector: str) -> bool:
    return bool(BeautifulSoup(html, 'html.parser').select_one(selector))

def diff_report(old_html: str, new_html: str, selectors: list[str]) -> dict:
    rep = {}
    for sel in selectors:
        rep[sel] = {
            'old': dom_has(old_html, sel),
            'new': dom_has(new_html, sel)
        }
    return rep  # 用于审计：哪些关键选择器从存在变为不存在，提示模板修复
```

五、近重复检测（SimHash/MinHash/LSH）与写入去重
- 目的：避免“内容基本相同但标题/时间略变”的重复写入；节省存储与提高检索质量。
- SimHash：对分词/特征加权后按位求和，得到签名；汉明距离小（如 ≤3/64）判定为近重复。
- MinHash：更适合集合相似度；可结合 LSH 做近似邻近查找，提升性能。
- 写入策略：在存储层以“唯一键 + 近重复签名”联合判定；近重复则走“更新/合并”而非新增。

示例 D：SimHash 近重复检测（含详注，阈值判定）
```python
# -*- coding: utf-8 -*-
import hashlib

def simple_simhash(tokens: list[str], bits: int = 64) -> int:
    vec = [0] * bits
    for t in tokens:
        h = int(hashlib.md5(t.encode('utf-8')).hexdigest(), 16)
        for i in range(bits):
            vec[i] += 1 if ((h >> i) & 1) else -1
    sig = 0
    for i, v in enumerate(vec):
        if v >= 0:
            sig |= (1 << i)
    return sig

def hamming(a: int, b: int) -> int:
    return bin(a ^ b).count('1')

def is_near_dup(sig_a: int, sig_b: int, threshold: int = 3) -> bool:
    return hamming(sig_a, sig_b) <= threshold
```

六、正文抽取与噪声过滤（启发式与语义信号）
- 启发式：最大文本块、段落平均长度、标点密度、链接密度（正文链接密度通常较低）。
- 语义信号：包含时间/作者/来源词汇、标题相似度；多信号综合评分提升稳定性。
- 结构先行：优先从已知正文容器抽取；失败时启发式全页回退。

示例 E：正文启发式抽取（含详注）
```python
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup

def text_density(node) -> float:
    txt = node.get_text(' ', strip=True)
    links = node.select('a')
    return (len(txt) + 1) / (len(links) + 1)  # 简化：文本长度/链接数，越大越可能是正文

def extract_main_content(html: str) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    candidates = [n for n in soup.find_all(True)]
    top = max(candidates, key=text_density)
    paras = [p.get_text(' ', strip=True) for p in top.select('p')]
    content = '\n'.join(p for p in paras if p)
    return content or top.get_text('\n', strip=True)
```

七、多语言与格式处理（与专题 13 联动）
- 日期与数值：多格式统一解析；输出 ISO 8601 与标准数值格式。
- 语言探测：针对跨站点内容按语言维度建立索引与抽取规则；必要时做轻量翻译或规范化。
- 编码与方向：RTL 语言布局特殊性；避免假空格与编码异常导致解析失败。

八、解析审计与测试回归（可观测性优先）
- 选择器审计：记录每个字段的命中选择器、命中失败次数、回退触发情况，用于报表与维护。
- 快照测试：保存页面快照；模板变更后跑回归，比较解析输出差异（允许小幅字段差异）。
- 误差容忍：对非关键字段允许“未知/空值”；关键字段失败时上报并触发模板修复流程。

示例 F：选择器审计（含详注）
```python
# -*- coding: utf-8 -*-
from dataclasses import dataclass, field

@dataclass
class SelectorAudit:
    hits: dict[str, int] = field(default_factory=dict)
    fallback_hits: dict[str, int] = field(default_factory=dict)
    fails: dict[str, int] = field(default_factory=dict)
    def hit(self, field: str, sel: str, fallback: bool = False):
        (self.fallback_hits if fallback else self.hits).setdefault(f"{field}:{sel}", 0)
        (self.fallback_hits if fallback else self.hits)[f"{field}:{sel}"] += 1
    def fail(self, field: str):
        self.fails.setdefault(field, 0); self.fails[field] += 1
```

九、工程整合与维护建议
- 模板模块化：按站点/栏目建立模板文件；统一注册到模板库并记录版本。
- 解析管道：解析前清洗 → 容器定位 → 字段抽取 → 清洗与规范化 → 近重复检测 → 入库。
- 指标与日志：结合专题 08 的结构化日志与 Prometheus 指标，监控解析成功率、平均字段缺失率、近重复判定率。
- 修复流程：每日汇总解析审计报表；对失败高的模板做修复与回归测试，版本升级与变更摘要入库。

十、常见陷阱与对策
- 选择器过拟合：仅适配单页特例，导致跨页失败；改为容器 + 备选。
- 噪声污染：未清洗分享/推荐模块导致正文掺杂；先清洗再抽取。
- 标题/时间漂移：站点改版导致标签变化；维护备选选择器与模板版本。
- 高代价近重复：阈值过严导致不同文章误判；结合站点/栏目与时间窗口进行判定。

小结
- 通过“模板库 + 选择器分级回退 + 正文启发式 + 近重复检测 + 审计与回归”，将解析从“易碎”升级为“稳态、可维护、可观测”。

—

## 专题 12：媒体下载（命名规则、完整性校验、断点续传）

目标与范围（把“易损下载”变为“稳态下载”）
- 目标：可靠下载图片/视频/文件，保障命名安全、完整性校验、断点续传与幂等写入；在失败与网络抖动下稳定恢复。
- 范围：安全命名与路径、扩展识别与归一、完整性校验（长度/哈希/ETag）、断点续传与并发策略、错误分类与退避、对象存储与原子写入、跨平台兼容（Windows/Unix）。
- 方法：提供“规范解释 + 详注代码”，强调原子性、可观测与可维护，避免半文件、重复写入、命名冲突。

一、安全命名与路径（Windows/Unix 兼容）
- 非法字符：Windows 路径禁止 `\ / : * ? " < > |`；统一替换为 `_`。
- 长路径风险：Windows 旧版本存在 260 长度限制；建议控制文件名长度（如 ≤128），目录层级适度。
- 归一规则：统一扩展名大小写（`.jpeg` → `.jpg`）、去除尾部空白与不可见字符；保持路径可读且稳定。

示例 A：安全命名与扩展归一（含详注）
```python
# -*- coding: utf-8 -*-
"""
安全命名工具：去非法字符、控制长度、扩展归一；适配 Windows/Unix
"""
import os
import re
from urllib.parse import urlparse

ILLEGAL = r"[\\/:*?\"<>|]"  # Windows 非法字符正则
MAX_NAME = 128

EXT_MAP = {
    '.jpeg': '.jpg',
    '.JPG': '.jpg',
    '.PNG': '.png',
}

def safe_stem(name: str) -> str:
    # 1) 去非法字符
    stem = re.sub(ILLEGAL, '_', name)
    # 2) 去不可见字符与多空格
    stem = re.sub(r"\s+", ' ', stem).strip()
    # 3) 控制长度（保留前后各一半，中间省略号）
    if len(stem) > MAX_NAME:
        half = MAX_NAME // 2 - 2
        stem = stem[:half] + '..' + stem[-half:]
    return stem

def infer_ext(url: str, content_type: str | None = None) -> str:
    # 1) 优先 URL 路径后缀
    path = urlparse(url).path
    _, ext = os.path.splitext(path)
    if not ext and content_type:
        # 2) 回退 Content-Type（常见图片类型）
        if 'image/jpeg' in content_type: ext = '.jpg'
        elif 'image/png' in content_type: ext = '.png'
        elif 'image/webp' in content_type: ext = '.webp'
        elif 'image/gif' in content_type: ext = '.gif'
    # 3) 归一映射
    ext = EXT_MAP.get(ext, ext or '')
    return ext or ''
```

二、完整性校验与原子写入（长度/哈希/ETag）
- 长度校验：期望大小与实际大小一致（`Content-Length`）；未知长度下以哈希校验为主。
- 哈希校验：SHA256/MD5（注意 MD5 碰撞风险，但文件完整性可用）；下载完成后计算比对。
- ETag 校验：若服务端提供 ETag（常见为内容哈希），可做 If-None-Match 或 If-Range 断点验证。
- 原子写入：先写临时文件（`.part`），校验成功后 `os.replace(tmp, final)`；避免出现半文件。

示例 B：带 SHA256 校验的下载（含详注）
```python
# -*- coding: utf-8 -*-
import os, hashlib, requests

def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b''):
            h.update(chunk)
    return h.hexdigest()

def download_checked(url: str, final_path: str, expect_sha256: str | None = None) -> bool:
    tmp = final_path + '.part'
    os.makedirs(os.path.dirname(final_path), exist_ok=True)
    with requests.get(url, stream=True, timeout=30) as r:
        if r.status_code != 200:
            return False
        with open(tmp, 'wb') as f:
            for chunk in r.iter_content(1024 * 64):
                if chunk:
                    f.write(chunk)
    # 完整性校验（可选）
    if expect_sha256:
        if sha256_file(tmp) != expect_sha256:
            os.remove(tmp)
            return False
    os.replace(tmp, final_path)  # 原子替换，避免半文件
    return True
```

三、断点续传与 If-Range/ETag（对大文件与不稳定网络）
- 前提：服务器支持 `Accept-Ranges: bytes`；否则无法续传（会返回 200 全量）。
- 续传头部：`Range: bytes=start-`；若有 ETag，优先用 `If-Range: <ETag>` 保证续传基于同一版本。
- 206 判定：续传返回应为 `206 Partial Content`；若返回 `200`，说明不支持续传或校验失效，需重新全量下载。
- 异常处理：`416 Range Not Satisfiable` 表示本地偏移超过远端长度或版本变化；应清理临时文件重下。

示例 C：健壮的断点续传（含详注）
```python
# -*- coding: utf-8 -*-
import os, requests

def resume_download(url: str, dest: str, etag: str | None = None) -> bool:
    tmp = dest + '.part'
    pos = os.path.getsize(tmp) if os.path.exists(tmp) else 0
    headers = {'User-Agent': 'Crawler/1.0'}
    if pos > 0:
        headers['Range'] = f'bytes={pos}-'
        if etag:
            headers['If-Range'] = etag  # 若远端版本变更，服务器将返回200要求全量
    with requests.get(url, headers=headers, stream=True, timeout=60) as r:
        if r.status_code == 416:  # 本地偏移不合法或远端长度变化
            if os.path.exists(tmp): os.remove(tmp)
            return resume_download(url, dest, etag)  # 清理后重试一次
        if r.status_code not in (200, 206):
            return False
        os.makedirs(os.path.dirname(dest), exist_ok=True)
        mode = 'ab' if r.status_code == 206 else 'wb'
        with open(tmp, mode) as f:
            for chunk in r.iter_content(1024 * 128):
                if chunk:
                    f.write(chunk)
    os.replace(tmp, dest)
    return True
```

四、并发策略与限流（单连接/分块并行）
- 单连接续传：对大多数站点最稳，与服务器压力最低；首选方式。
- 分块并行：将文件分块（如 8MB/块）多连接并行下载，最后合并；需确认站点允许并发与 Range 负载，谨慎使用。
- 限流与礼貌：结合专题 03 的域级限速与并发上限，避免对单域并行压测；失败分类联动暂停。

示例 D：分块并行下载（含详注，谨慎使用）
```python
# -*- coding: utf-8 -*-
import os, math, requests
from concurrent.futures import ThreadPoolExecutor

def _download_block(url: str, start: int, end: int, part_path: str) -> bool:
    headers = {'Range': f'bytes={start}-{end}', 'User-Agent': 'Crawler/1.0'}
    with requests.get(url, headers=headers, stream=True, timeout=60) as r:
        if r.status_code != 206:
            return False  # 并块需返回206
        with open(part_path, 'wb') as f:
            for chunk in r.iter_content(1024 * 128):
                if chunk:
                    f.write(chunk)
    return True

def parallel_download(url: str, dest: str, block_size: int = 8 * 1024 * 1024, max_workers: int = 4) -> bool:
    # 1) 预取长度
    head = requests.head(url, timeout=15)
    if head.status_code >= 400:
        return False
    total = int(head.headers.get('Content-Length', '0'))
    if total <= 0 or 'bytes' not in head.headers.get('Accept-Ranges', '').lower():
        # 不支持并块或未知长度，回退到单连接下载
        return resume_download(url, dest, head.headers.get('ETag'))
    # 2) 计算块范围
    parts = []
    n = math.ceil(total / block_size)
    for i in range(n):
        start = i * block_size
        end = min(total - 1, (i + 1) * block_size - 1)
        parts.append((start, end, f"{dest}.part.{i}"))
    # 3) 并行下载
    os.makedirs(os.path.dirname(dest), exist_ok=True)
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        results = list(ex.map(lambda p: _download_block(url, *p), parts))
    if not all(results):
        # 失败清理
        for _, _, p in parts:
            if os.path.exists(p): os.remove(p)
        return False
    # 4) 合并为最终文件（原子生成）
    tmp = dest + '.part'
    with open(tmp, 'wb') as out:
        for _, _, p in parts:
            with open(p, 'rb') as f:
                out.write(f.read())
            os.remove(p)
    os.replace(tmp, dest)
    return True
```

五、错误分类与退避（稳定恢复）
- 分类：`403/429`（访问限制/速率限制）→ 暂停域或提升等待；`404`（资源不存在）→ 标记终止；`5xx/网络` → 指数退避重试。
- 重试幂等：下载操作幂等（同路径重复写最终一致）；失败时清理 `.part` 或保留偏移视情况而定。
- 指标与日志：记录下载成功率、平均速率、失败类型、重试次数、分块成功率，接入专题 08 的结构化日志与 Prometheus 指标。

六、对象存储与本地文件系统（写入与校验）
- 本地 FS：使用临时文件与原子替换；目录按站点/日期分层，避免单目录文件过多。
- 对象存储：S3/OSS/OBS 上传时先本地校验，再使用多段上传与 ETag 校验；按 `key` 规划层级（站点/日期/类型）。

示例 E：S3 多段上传（思路与注释，伪代码）
```python
# -*- coding: utf-8 -*-
"""
S3 思路：对大文件采用 multipart upload，分块上传并在完成后校验 ETag；注意限流与重试。
"""
def s3_multipart_upload(file_path: str, bucket: str, key: str) -> bool:
    # 1) 创建上传会话（返回 upload_id）
    # 2) 分块读取文件并并发上传块（记录每块的 ETag）
    # 3) 完成上传并提交所有块的 ETag 列表（服务端合并校验）
    # 4) 失败时中止会话并清理（避免残留未完成上传）
    pass
```

七、命名冲突与幂等（不重复写入）
- 唯一键：`站点/路径/文件名 + 扩展名` 或 `URL 哈希`；写入前检查是否已存在，存在则跳过或校验更新。
- 哈希去重：保存 `sha256` 到数据库，重复校验时直接比对，避免同名不同内容污染。
- 版本更新：若服务器 ETag/长度变化，视策略更新或保留旧版本；建议增加版本号或时间后缀。

示例 F：幂等下载接口（含详注）
```python
# -*- coding: utf-8 -*-
import sqlite3

def ensure_table(conn: sqlite3.Connection):
    conn.execute("""
    CREATE TABLE IF NOT EXISTS media(
      url TEXT PRIMARY KEY,
      path TEXT NOT NULL,
      sha256 TEXT,
      size INTEGER,
      etag TEXT,
      updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)

def download_idempotent(conn: sqlite3.Connection, url: str, path: str) -> bool:
    # 1) 已有记录则判断文件是否存在与哈希是否一致
    cur = conn.execute("SELECT sha256, size, etag FROM media WHERE url=?", (url,))
    row = cur.fetchone()
    if row:
        if os.path.exists(path):
            return True  # 简化：实际应比对哈希/大小，一致则跳过，不一致则重新下载
    # 2) 下载并校验
    ok = resume_download(url, path)
    if not ok: return False
    # 3) 回填元数据
    sha = sha256_file(path)
    size = os.path.getsize(path)
    conn.execute("REPLACE INTO media(url, path, sha256, size) VALUES(?,?,?,?)", (url, path, sha, size))
    return True
```

八、跨平台兼容与边界
- Windows 特性：路径分隔符 `\\` 与非法字符限制；注意网络驱动器与长路径设置。
- Unix 特性：区分大小写；权限与用户/组；谨慎使用特殊文件名（以 `.` 开头）。
- 路径规范：使用 `os.path` 与 `pathlib` 统一路径构建；避免手写字符串拼接。

九、工程整合建议
- 把下载封装为模块（如 `crawler/media.py`）：包含安全命名、扩展识别、断点续传、校验与幂等接口。
- 与专题 08 联动：暴露指标（下载成功率、平均速率、分块成功率、失败类型分布），设置告警阈值。
- 与专题 03/04 联动：下载任务使用域级限速与失败分类暂停；在 Scheduler 中为媒体任务设独立并发与队列。

十、常见陷阱与对策
- 半文件：程序中断或失败未清理 `.part`；通过原子替换与失败清理避免脏文件。
- 错误续传：ETag/If-Range 未使用导致跨版本续传；严格校验返回码与头部。
- 名称污染：非法字符未清洗或扩展名不一致；统一安全命名与扩展归一。
- 并块过载：对同一域并发分块造成压力与封禁；默认使用单连接续传，分块仅在允许时启用。

小结
- 使用“安全命名 + 完整性校验 + 原子写入 + 断点续传 + 并发限流 + 幂等记录”的组合，使媒体下载在工程上稳定、可维护、可观测。

—

## 专题 13：国际化与编码（字符集、RTL、日期与时区、本地化）
本专题聚焦“抓取、解析、存储”的国际化细节与工程实践，目标是：
- 统一而稳健的字符解码（避免“乱码/半乱码/错码”），在不丢信息的前提下最大化可读性；
- 正确处理右到左（RTL）文本与双向字符（BiDi），存储层保持语义、显示层考虑排版；
- 跨语言日期/时区解析与统一归档（ISO 8601/UTC），避免跨站混乱与时差误判；
- URL 与域名国际化（IDN/Punycode）与路径百分号编码，保证链接的可抓取与可重现；
- 语言检测、模板选择与本地化数字/货币解析；
- 全链路可观测：将编码错误、语言分布、日期解析失败等作为指标与日志。

一、统一编码解码策略（HTTP/HTML/BOM/探测）
- 原则：
  - 优先“服务器声明的编码”（响应头 `Content-Type: ...; charset=...`），其次“HTML `<meta charset>`”，最后“启发式探测（charset_normalizer/chardet）”。
  - 始终使用 `errors='replace'` 或 `'ignore'`，避免抛异常中断；但同时记录“替换/丢弃字符计数”以便审计。
  - 针对 UTF-8/UTF-16/UTF-32 的 BOM（字节序标记）进行识别与剥离，避免混入正文。
  - 对 JSON/CSV 等非 HTML 内容，尽量从头部与约定获取编码；探测仅在必要时启用。

示例 A：鲁棒 HTML 解码（详注，含 BOM/头部/探测/Meta）
```python
# -*- coding: utf-8 -*-
from __future__ import annotations
import re
import codecs

# 第三方库建议：charset-normalizer（比 chardet 更现代，Py3 友好）
from charset_normalizer import from_bytes

META_CHARSET_RE = re.compile(
    r"<meta[^>]+charset=\s*['\"]?([A-Za-z0-9_\-]+)['\"]?",
    re.IGNORECASE
)

def _strip_bom(data: bytes) -> bytes:
    """
    剥离常见 BOM：UTF-8/UTF-16/UTF-32。避免 BOM 残留导致首字符异常。
    """
    for bom in (codecs.BOM_UTF8, codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE,
                codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
        if data.startswith(bom):
            return data[len(bom):]
    return data

def robust_decode_html(body: bytes, content_type: str | None) -> tuple[str, str]:
    """
    将 HTML 字节串稳健解码为文本。
    返回 (text, used_encoding)，便于记录实际使用的编码。
    策略优先级：HTTP 头 charset > HTML meta charset > 探测器 > 兜底 utf-8。
    注意：errors 使用 'replace'，避免因单个字符无法解码而中断。
    """
    raw = _strip_bom(body)
    used = None

    # 1) HTTP 头部显式声明（如 Content-Type: text/html; charset=gb18030）
    if content_type:
        m = re.search(r"charset=([A-Za-z0-9_\-]+)", content_type, re.IGNORECASE)
        if m:
            enc = m.group(1).strip()
            try:
                return raw.decode(enc, errors='replace'), enc
            except LookupError:
                pass

    # 2) HTML <meta charset> 声明（优先匹配前 ~4KB）
    head = raw[:4096].decode('ascii', errors='ignore')
    m2 = META_CHARSET_RE.search(head)
    if m2:
        enc = m2.group(1).lower()
        try:
            return raw.decode(enc, errors='replace'), enc
        except LookupError:
            pass

    # 3) 探测器（charset_normalizer）：返回最优结果，可能为 'gbk' 'shift_jis' 等
    best = from_bytes(raw).best()
    if best is not None:
        txt = str(best)
        used = getattr(best, 'encoding', None) or 'detected'
        return txt, used

    # 4) 兜底 utf-8（极少数探测失败时）
    return raw.decode('utf-8', errors='replace'), 'utf-8'
```

二、URL 与域名国际化（IDN/Punycode 与路径编码）
- 问题：
  - 非 ASCII 域名（如中文域名）需要按 IDNA 规则转换为 Punycode（如 `例子.测试` → `xn--fsqu00a.xn--0zwm56d`），否则无法解析。
  - 路径/查询参数中的非 ASCII 字符需按 UTF-8 百分号编码；同时避免重复/错码（混用本地编码）。
- 方案：
  - 主机名：使用 `idna` 编码（Python 内置支持），保证 DNS 可用。
  - 路径与查询：逐段 `urllib.parse.quote`，指定 `safe` 字符集，统一 UTF-8。

示例 B：国际化 URL 规范化（详注）
```python
# -*- coding: utf-8 -*-
from urllib.parse import urlsplit, urlunsplit, quote, unquote

def normalize_i18n_url(url: str) -> str:
    """
    将包含非 ASCII 域名/路径的 URL 规范化为可抓取形式：
    - 主机名 → IDNA（Punycode）
    - 路径/查询 → UTF-8 百分号编码
    - 片段（#...）可按策略移除或保留
    """
    s = urlsplit(url)
    netloc = s.netloc.encode('idna').decode('ascii') if s.netloc else s.netloc
    path_txt = unquote(s.path, errors='ignore')
    path_enc = quote(path_txt, safe="/-._~")
    query_txt = unquote(s.query, errors='ignore')
    query_enc = quote(query_txt, safe="=&?/-._~")
    frag = ''
    return urlunsplit((s.scheme, netloc, path_enc, query_enc, frag))
```

三、Unicode 归一化与方向控制字符（NFC/NFKC/ZWJ/ZWNJ/LRM/RLM）
- 问题：
  - 不同源可能使用不同 Unicode 规范化形态（NFC/NFD），导致比较/去重出错（例如“é” vs “e + ́”）。
  - 文本中包含零宽连接符（ZWJ/ZWNJ）或方向控制字符（LRM/RLM 等），影响显示与子串匹配。
- 方案：
  - 存储前做规范化（首选 NFC；当需要兼容宽度/半角时考虑 NFKC）。
  - 在解析/清洗阶段，根据需要剥离方向控制字符；对 ZWJ/ZWNJ 谨慎处理（某些语言书写依赖）。

示例 C：文本规范化与方向字符清理（详注）
```python
# -*- coding: utf-8 -*-
import unicodedata

DIR_MARKS = {
    "LRM": "\u200E",
    "RLM": "\u200F",
    "LRE": "\u202A",
    "RLE": "\u202B",
    "PDF": "\u202C",
    "LRO": "\u202D",
    "RLO": "\u202E",
}

def canonical_text(s: str, form: str = 'NFC') -> str:
    try:
        return unicodedata.normalize(form, s)
    except Exception:
        return s

def strip_direction_marks(s: str) -> str:
    for ch in DIR_MARKS.values():
        s = s.replace(ch, '')
    return s
```

四、RTL 与双向文本（BiDi）处理要点
- 原则：
  - 存储层保持原始代码点顺序（不做视觉重排），保证信息不丢失；显示层根据需要做视觉重排。
  - 检测 RTL：根据 `unicodedata.bidirectional(ch)` 的返回值（如 'R'/'AL' 表示阿拉伯/希伯来），或根据页面 `<html dir="rtl">` 与 `<body dir="rtl">`。
  - 显示时（如导出 Markdown/PDF）可结合 `python-bidi` 与 `arabic_reshaper` 做视觉重排；爬虫解析通常不改写文本。

示例 D：RTL 检测与（可选）视觉重排（详注）
```python
# -*- coding: utf-8 -*-
import unicodedata

def is_rtl_text(s: str, threshold: float = 0.2) -> bool:
    if not s:
        return False
    total = len(s)
    rtl = sum(1 for ch in s if unicodedata.bidirectional(ch) in ('R', 'AL', 'RLE', 'RLO', 'RLI'))
    return (rtl / total) >= threshold

try:
    from bidi.algorithm import get_display
    import arabic_reshaper

    def visual_order_for_display(s: str) -> str:
        reshaped = arabic_reshaper.reshape(s)
        return get_display(reshaped)
except Exception:
    pass
```

五、语言检测与模板选择（page-level 与 field-level）
- 方案：
  - Page-level：优先使用 `<html lang="...">` 或服务器区域信息作为先验；无先验则用轻量检测库（如 `langdetect`/`langid`）。
- Field-level：短文本检测不稳定，结合站点/栏目上下文加权判断。
- 模板选择：按语言触发解析模板（不同语言的日期/数字规则），记录命中与回退。

示例 E：语言检测（详注，结合 `<html lang>` 先验）
```python
# -*- coding: utf-8 -*-
import re
from typing import Optional

def detect_lang(text: str, html_lang: Optional[str] = None) -> str:
    if html_lang:
        return html_lang.split('-')[0].lower()
    if re.search(r"[\u0600-\u06FF]", text):
        return 'ar'
    if re.search(r"[\u4e00-\u9fff]", text):
        return 'zh'
    return 'en'
```

六、日期与时区解析（多语言/歧义/UTC 统一）
- 原则：
  - 使用 `dateparser` 解析多语言日期；统一转换为 `UTC` 并按 ISO 8601 存储（如 `2025-11-11T08:00:00Z`）。
  - 明确站点时区先验；相对时间需指定参考时间与时区。
  - 使用 `zoneinfo` 处理 DST 与历史偏移。

示例 F：多语言日期解析与 UTC 归一（详注）
```python
# -*- coding: utf-8 -*-
from __future__ import annotations
from datetime import datetime
from zoneinfo import ZoneInfo

try:
    import dateparser
except Exception:
    dateparser = None

def parse_date_multi(text: str, ref_tz: str = 'UTC', languages: list[str] | None = None):
    if not dateparser:
        return None
    settings = {
        'TIMEZONE': ref_tz,
        'RETURN_AS_TIMEZONE_AWARE': False,
        'PREFER_DAY_OF_MONTH': 'first',
        'RELATIVE_BASE': datetime.now(ZoneInfo(ref_tz)),
    }
    return dateparser.parse(text, languages=languages, settings=settings)

def to_utc_iso(dt: datetime, src_tz: str) -> str:
    local = dt.replace(tzinfo=ZoneInfo(src_tz))
    utc_dt = local.astimezone(ZoneInfo('UTC'))
    return utc_dt.isoformat().replace('+00:00', 'Z')
```

七、数字与货币的本地化解析（千分位/小数点/货币符号）
- 问题：`1.234,56`（欧式）与 `1,234.56`（美式）符号相反；符号位置与种类多样。
- 方案：识别 locale 后使用 `babel` 或简化规则解析；统一为 Decimal 与 ISO 4217 货币代码。

示例 G：本地化数字/货币解析（简化示例）
```python
# -*- coding: utf-8 -*-
import re
from decimal import Decimal

def parse_local_number(s: str, locale: str = 'en') -> Decimal | None:
    s = s.strip()
    if locale == 'de':
        s = s.replace('.', '').replace(',', '.')
    else:
        s = s.replace(',', '')
    try:
        return Decimal(s)
    except Exception:
        return None

def normalize_currency(text: str):
    m = re.search(r"([\$€£¥])\s*([\d.,]+)", text)
    if not m:
        return None, None
    sym, num = m.groups()
    amt = parse_local_number(num, locale='en')
    code = {'$': 'USD', '€': 'EUR', '£': 'GBP', '¥': 'JPY'}.get(sym, None)
    return amt, code
```

八、存储与索引的国际化字段设计
- 存储：`lang`（ISO 639-1）、`script`（如 `Latn`/`Arab`）、`is_rtl`；`published_at_utc`、`site_tz`、`raw_date_text`。
- 规范化：文本字段统一 `NFC`；必要时保留 `raw_text` 用于审计。
- 索引：按语言维度建立联合索引（如 `site+lang+published_at_utc`）；检索器针对不同语言使用合适分词。

九、国际化相关的错误分类与可观测性
- 指标：`i18n_decode_fail_total`、`i18n_date_parse_fail_total`、`i18n_lang_und_total`。
- 结构化日志示例：
```python
# -*- coding: utf-8 -*-
import logging
log = logging.getLogger("i18n")

def log_decode_issue(url: str, site: str, used_enc: str, replaced_count: int) -> None:
    log.warning("i18n_decode_issue", extra={
        "url": url,
        "site": site,
        "used_encoding": used_enc,
        "replaced_count": replaced_count,
    })
```

十、测试与回归（多样化样本与边界条件）
- 样本集：编码（UTF-8/GBK/Shift_JIS/ISO-8859-1/Win-1252，含/不含 BOM、头部错标）、文本（LRM/RLM、ZWJ/ZWNJ、混排）、日期（绝对/相对、时区/DST）、URL（IDN 与百分号编码一致性）。
- 断言：解码不抛异常、替换字符不超阈值；归一化后比较稳定（去重键一致）；日期统一到 UTC 且范围合理；URL 规范化后可成功请求。

小结与工程化建议
- 将“解码 → 归一化 → 方向字符处理 → 语言检测 → 模板选择 → 日期/数字解析 → UTC 存储”固定为解析管线步骤。
- 在 `parser.py` 封装 `robust_decode_html`、`canonical_text`、`strip_direction_marks`、`detect_lang`、`parse_date_multi` 等函数；
- 在 `storage.py` 写入路径记录 `lang/is_rtl/site_tz/published_at_utc` 并体现在索引设计；
- 与专题 08 联动：暴露解码/语言/日期相关指标；与专题 11 联动：解析模板针对不同语言差异化配置。

—

## 专题 14：性能优化（I/O/CPU/内存、并发模型、分析与调优）
本章从工程视角系统化阐述如何让爬虫“跑得更稳、更快、更省”，覆盖 I/O、CPU、内存、并发模型与性能分析。所有示例均含详细中文注释，便于在真实项目落地。

一、I/O 优化（连接、超时、批量、写入）
- 连接复用：HTTP Keep-Alive + 连接池减少握手开销；`requests.Session`/`aiohttp` 的连接器参数需按域名并发与服务端限制而定。
- 合理超时：区分连接超时与读超时；在不可达或慢源时快速失败，避免线程/协程被长期占用。
- 批量操作：数据库/文件写入尽量批量；避免频繁的小文件写导致磁盘碎片与大量系统调用。
- 压缩与传输：开启 `Accept-Encoding: gzip, deflate, br`，减少网络传输体积；解析前进行解压。

示例 A：Requests 会话与连接池调优（详注）
```python
# -*- coding: utf-8 -*-
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def build_session(max_pool: int = 20, per_host: int = 10) -> requests.Session:
    """
    构建连接复用的会话：
    - 连接池大小：控制并发连接数量（全局/每主机）
    - 重试策略：对临时性错误（网络抖动、5xx）进行指数退避重试
    - 合理超时：连接超时与读超时分离，避免线程长期阻塞
    """
    sess = requests.Session()

    # 指数退避重试策略：避免瞬时失败导致整体任务中断
    retry = Retry(
        total=3,                # 总重试次数
        connect=3, read=3,      # 连接/读取分别限制
        backoff_factor=0.5,     # 退避系数：0.5, 1.0, 2.0...
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"]
    )
    adapter = HTTPAdapter(
        max_retries=retry,
        pool_connections=max_pool,  # 全局连接池大小
        pool_maxsize=per_host       # 每主机最大连接数
    )
    sess.headers.update({
        "User-Agent": "NorCrawler/1.0",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive"
    })
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    return sess

def safe_get(sess: requests.Session, url: str, timeout=(3.0, 10.0)) -> requests.Response | None:
    """
    安全 GET：
    - timeout=(连接超时, 读超时)
    - 对非 2xx 状态码不重试（除非在 Retry 配置中明确）
    - 返回 None 表示失败，避免抛出异常中断主流程
    """
    try:
        resp = sess.get(url, timeout=timeout)
        return resp if 200 <= resp.status_code < 300 else None
    except requests.RequestException:
        return None
```

示例 B：aiohttp 并发抓取与限速（详注）
```python
# -*- coding: utf-8 -*-
import asyncio
import aiohttp

class RateLimiter:
    """简单速率限制器：限制每个主机的并发与每秒请求数。"""
    def __init__(self, max_concurrency: int = 10, per_second: int = 5):
        self.sem = asyncio.Semaphore(max_concurrency)
        self.per_second = per_second
        self._tokens = per_second
        self._last = asyncio.get_event_loop().time()

    async def acquire(self):
        await self.sem.acquire()
        # 漏桶/令牌桶的简化实现：按秒补充令牌
        now = asyncio.get_event_loop().time()
        if now - self._last >= 1.0:
            self._tokens = self.per_second
            self._last = now
        while self._tokens <= 0:
            await asyncio.sleep(0.05)
            now = asyncio.get_event_loop().time()
            if now - self._last >= 1.0:
                self._tokens = self.per_second
                self._last = now
        self._tokens -= 1

    def release(self):
        self.sem.release()

async def fetch(session: aiohttp.ClientSession, url: str, rl: RateLimiter):
    await rl.acquire()
    try:
        # 连接器限制并发；超时区分连接与总超时
        async with session.get(url, timeout=aiohttp.ClientTimeout(connect=3, total=12)) as resp:
            if resp.status != 200:
                return None
            # 使用压缩传输，aiohttp 会自动解压
            return await resp.text()
    except (aiohttp.ClientError, asyncio.TimeoutError):
        return None
    finally:
        rl.release()

async def run(urls: list[str]):
    # 连接池：限制总并发连接，避免过度占用系统资源
    connector = aiohttp.TCPConnector(limit=30, force_close=False)
    async with aiohttp.ClientSession(connector=connector, headers={
        "User-Agent": "NorCrawler/1.0",
        "Accept-Encoding": "gzip, deflate, br",
    }) as session:
        rl = RateLimiter(max_concurrency=10, per_second=6)
        tasks = [asyncio.create_task(fetch(session, u, rl)) for u in urls]
        return await asyncio.gather(*tasks)

# 用法：asyncio.run(run(url_list))
```

二、CPU 优化（解析路径、正则、并行）
- 解析路径优化：减少重复 DOM 遍历与字符串复制；共享中间结果（如预解析节点树）。
- 正则优化：预编译、使用非回溯或锚定模式，避免灾难性回溯；在热路径优先简单解析替代复杂正则。
- 并行：CPU 密集任务移交进程池，绕开 GIL；轻度文本处理可用线程但注意 GIL 竞争。

示例 C：正则预编译与灾难性回溯规避（详注）
```python
# -*- coding: utf-8 -*-
import re

# 预编译常用模式，避免重复编译开销
PAT_TITLE = re.compile(r"<h1[^>]*>(.*?)</h1>", re.IGNORECASE | re.DOTALL)

def safe_extract_title(html: str) -> str:
    """
    在热路径中避免复杂/贪婪模式带来的灾难性回溯：
    - 限定匹配范围（例如先截取 <body> 内部）
    - 使用非贪婪与明确边界
    """
    m = PAT_TITLE.search(html)
    return m.group(1).strip() if m else ""
```

示例 D：进程池执行 CPU 密集任务（详注）
```python
# -*- coding: utf-8 -*-
from concurrent.futures import ProcessPoolExecutor, as_completed

def heavy_parse(text: str) -> dict:
    """模拟 CPU 密集的解析逻辑（如复杂清洗/实体抽取）。"""
    # ... 复杂计算 ...
    return {"ok": True, "len": len(text)}

def run_heavy_batch(items: list[str], max_workers: int = 4) -> list[dict]:
    results: list[dict] = []
    with ProcessPoolExecutor(max_workers=max_workers) as ex:
        futs = [ex.submit(heavy_parse, it) for it in items]
        for fut in as_completed(futs):
            try:
                results.append(fut.result())
            except Exception:
                results.append({"ok": False})
    return results
```

三、内存优化（队列、流式、批量）
- 队列长度：对生产/消费采用有界队列，防止堆积；设置背压策略（阻塞或丢弃低优先任务）。
- 流式解析：`iter_content`/流式 HTML 解析，避免一次性加载大文档；谨慎使用大字典/列表。
- 批量写入：数据库批量 `executemany` 或批量事务，降低提交开销。

示例 E：流式下载与写入（详注）
```python
# -*- coding: utf-8 -*-
import requests

def stream_download(url: str, path: str, chunk_size: int = 1 << 15) -> bool:
    """
    以流式方式下载并写入文件：
    - 避免将整个响应加载到内存
    - 控制块大小以平衡系统调用与缓冲占用
    """
    try:
        with requests.get(url, stream=True, timeout=(3, 20)) as r:
            if r.status_code != 200:
                return False
            with open(path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
        return True
    except requests.RequestException:
        return False
```

四、并发模型（协程/线程/进程 + 解耦）
- 抓取使用协程：大量 I/O 等待时协程高效；使用连接池与限速控制并发。
- 解析使用线程/进程：IO 轻、CPU 重的解析用进程池；轻度文本处理可线程池。
- 解耦：生产者-消费者 + 有界队列，避免互阻；明确取消与关闭流程。

示例 F：协程抓取 + 线程解析 + 队列背压（详注）
```python
# -*- coding: utf-8 -*-
import asyncio
import concurrent.futures
import aiohttp

async def producer(urls: list[str], out_q: asyncio.Queue, session: aiohttp.ClientSession):
    for u in urls:
        try:
            async with session.get(u, timeout=aiohttp.ClientTimeout(connect=3, total=12)) as resp:
                if resp.status == 200:
                    txt = await resp.text()
                    # 有界队列：满则等待，形成背压
                    await out_q.put((u, txt))
        except Exception:
            await out_q.put((u, None))

def parse_sync(item: tuple[str, str | None]) -> tuple[str, dict | None]:
    url, txt = item
    if not txt:
        return url, None
    # 模拟解析
    return url, {"title": "...", "len": len(txt)}

async def consumer(in_q: asyncio.Queue, results: list[dict], ex: concurrent.futures.Executor):
    while True:
        item = await in_q.get()
        if item is None:
            break
        # 在线程池中执行轻度 CPU 的同步解析函数
        url, parsed = await asyncio.get_event_loop().run_in_executor(ex, parse_sync, item)
        if parsed:
            results.append(parsed)
        in_q.task_done()

async def pipeline(urls: list[str]):
    out_q: asyncio.Queue = asyncio.Queue(maxsize=100)  # 有界，防止堆积
    connector = aiohttp.TCPConnector(limit=50)
    async with aiohttp.ClientSession(connector=connector) as session:
        results: list[dict] = []
        ex = concurrent.futures.ThreadPoolExecutor(max_workers=8)
        prod = asyncio.create_task(producer(urls, out_q, session))
        cons = asyncio.create_task(consumer(out_q, results, ex))
        await prod
        await out_q.put(None)  # 发送停止信号
        await out_q.join()
        await cons
        ex.shutdown(wait=True)
        return results

# 用法：asyncio.run(pipeline(urls))
```

五、性能分析与指标（定位热点，量化成效）
- 采样与追踪：`cProfile` 分析总体耗时分布；按累计时间排序找热点；必要时结合 `line_profiler` 做行级分析。
- 指标：为关键路径埋点（请求耗时、解析耗时、队列长度、失败率），以图表/告警驱动调优。

示例 G：cProfile 使用（详注）
```python
# -*- coding: utf-8 -*-
import cProfile, pstats

def main():
    # 抓取主流程：这里调用你的 pipeline/调度器
    pass

if __name__ == '__main__':
    with cProfile.Profile() as pr:
        main()
    stats = pstats.Stats(pr)
    # strip_dirs: 去掉路径噪音；cumtime: 按累计时间排序
    stats.strip_dirs().sort_stats('cumtime').print_stats(30)
```

示例 H：简单耗时指标装饰器（详注）
```python
# -*- coding: utf-8 -*-
import time, logging
log = logging.getLogger("perf")

def timeit(name: str):
    def deco(fn):
        def wrapper(*args, **kwargs):
            t0 = time.perf_counter()
            try:
                return fn(*args, **kwargs)
            finally:
                dt = (time.perf_counter() - t0) * 1000
                log.info("perf_metric", extra={"metric": name, "ms": dt})
        return wrapper
    return deco

@timeit("parse_detail_ms")
def parse_detail(html: str) -> dict:
    # 你的解析逻辑
    return {"ok": True}
```

六、调参与落地建议（可操作清单）
- I/O：配置连接池大小（全局/主机）、统一超时、开启压缩、批量写入、避免大量小文件。
- CPU：预编译正则、优化解析路径、将重任务移交进程池、减少不必要的复制与转换。
- 内存：有界队列 + 背压、流式下载/解析、批量提交，控制协程/线程数量。
- 并发：抓取协程 + 解析线程/进程；清晰的关闭与取消协议，避免资源泄漏。
- 分析：定期使用 `cProfile` 审视热点；建立耗时与失败率指标，调优以数据为依据。

—

## 专题 15：站点实战案例（新闻/论坛/电商）
本章以可落地的工程路径，分别给出新闻/论坛/电商三类站点的抓取与解析要点。强调“正确性、稳健性、可维护性”，并用注释详细解释每一步的设计取舍与常见坑位。

通用工程原则（适用于三类站点）
- 礼貌与限速：域级节流与并发限制，避免对方封禁；错误分级重试（网络抖动 vs 业务拒绝）。
- 幂等与增量：以 URL/主键为唯一约束，重复写入不报错；按 TTL 或游标做增量更新。
- 解析稳定性：选择器尽量鲁棒，优先结构稳定的标记（如 `data-*`）；在 Topic 13 的国际化解码后再解析。
- 媒体稳定下载：图片/视频下载遵循 Topic 12 命名与校验；失败分类与断点续传。
- 可观测与回归：日志结构化记录、指标监控（成功率/耗时/重试次数）；单元与契约测试覆盖关键路径。

一、新闻站点：列表 → 详情 → 追加写入与去重
- 发现：列表页分页与时间窗；按发布时间递减抓取，遇到旧文触发“短路”结束本轮增量。
- 详情解析：标题/时间/正文与作者；时间统一到 UTC（见 Topic 13）；正文清洗保留结构（段落/换行）。
- 去重与存储：使用 SQLite/表唯一约束；写入失败不抛错而是记录冲突。

示例 A：新闻站点小型爬虫（详注，含限速/重试/去重/增量）
```python
# -*- coding: utf-8 -*-
import os, time, sqlite3
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE_URL = "https://example-news.com/"
DB = "output_news.db"
HEADERS = {"User-Agent": "NorCrawler/1.0", "Accept-Encoding": "gzip, deflate, br"}

def init_db():
    """初始化 SQLite：唯一约束确保幂等；失败时不影响主流程。"""
    conn = sqlite3.connect(DB)
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS articles (
            url TEXT PRIMARY KEY,
            title TEXT,
            published_at_utc TEXT,
            content TEXT,
            created_at INTEGER
        )
        """
    )
    conn.commit(); conn.close()

def polite_get(url: str, retries: int = 3, delay: float = 0.5):
    """
    带礼貌等待与有限重试的 GET：
    - 固定最简节流（生产建议域级令牌桶）
    - 网络错误与 5xx 才重试；4xx 一般不重试（除 429 可退避）
    """
    for i in range(retries):
        try:
            time.sleep(delay)
            resp = requests.get(url, headers=HEADERS, timeout=(3, 10))
            if resp.status_code == 200:
                return resp
            if resp.status_code in (429, 500, 502, 503, 504):
                time.sleep((i + 1) * 0.6)  # 简单指数退避
                continue
            return None
        except requests.RequestException:
            time.sleep((i + 1) * 0.6)
            continue
    return None

def parse_list(html: str):
    """解析列表页中的详情链接；选择器尽量稳定。"""
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for a in soup.select('.list-item a.title, a[data-role="headline"]'):
        href = a.get('href')
        if href:
            links.append(urljoin(BASE_URL, href))
    return links

def parse_detail(html: str):
    """解析详情页核心字段：标题/时间/正文。"""
    soup = BeautifulSoup(html, 'html.parser')
    title = soup.select_one('h1.article-title, h1[data-role="title"]')
    date = soup.select_one('time.pub-date, time[datetime]')
    content = soup.select_one('div.article-content, article')
    return {
        'title': title.get_text(strip=True) if title else '',
        'date': (date.get('datetime', '').strip() if date else ''),
        'content': content.get_text('\n', strip=True) if content else ''
    }

def upsert_record(url: str, record: dict):
    """按 URL 主键写入；已有则覆盖（或忽略），确保幂等。"""
    conn = sqlite3.connect(DB); cur = conn.cursor()
    cur.execute(
        """
        INSERT INTO articles(url, title, published_at_utc, content, created_at)
        VALUES(?, ?, ?, ?, strftime('%s','now'))
        ON CONFLICT(url) DO UPDATE SET
            title=excluded.title,
            published_at_utc=excluded.published_at_utc,
            content=excluded.content
        """,
        (url, record['title'], record['date'], record['content'])
    )
    conn.commit(); conn.close()

def crawl_once(list_url: str):
    resp = polite_get(list_url)
    if not resp: return
    for url in parse_list(resp.text):
        d = polite_get(url)
        if not d: continue
        record = parse_detail(d.text)
        if not record['title'] or not record['content']: continue
        upsert_record(url, record)

# 用法：init_db(); crawl_once(urljoin(BASE_URL, '/news'))
```

二、论坛站点：主题 → 分页评论 → 规范化与层级
- 分页评论：识别“下一页/更多”链接，稳定获取所有页；避免重复抓取与评论顺序颠倒。
- 作者与层级：统一作者 ID/昵称；评论层级（楼中楼）按父子引用解析；归一化时间（Topic 13）。
- 失败重试：分页抓取中某页失败时继续其他页并记录缺口，后续补抓。

示例 B：论坛主题分页解析（详注）
```python
# -*- coding: utf-8 -*-
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def parse_thread_page(base: str, html: str):
    """解析单页评论与下一页链接。"""
    soup = BeautifulSoup(html, 'html.parser')
    comments = []
    for div in soup.select('.comment-item'):
        cid = div.get('data-id') or ''
        author = (div.select_one('.author') or {}).get_text(strip=True)
        text = (div.select_one('.content') or {}).get_text('\n', strip=True)
        time_txt = (div.select_one('time[datetime]') or {}).get('datetime', '')
        parent = div.get('data-parent')  # 楼中楼父 ID
        comments.append({"id": cid, "author": author, "text": text, "time": time_txt, "parent": parent})
    # 识别下一页链接（多种写法）
    next_a = soup.select_one('a.next, a[rel="next"], a[href*="page="]')
    next_url = urljoin(base, next_a.get('href')) if next_a and next_a.get('href') else None
    return comments, next_url

def unify_order(comments: list[dict]) -> list[dict]:
    """确保评论输出顺序稳定（按时间/ID）。"""
    return sorted(comments, key=lambda c: (c["time"], c["id"]))

def reattach_children(comments: list[dict]) -> list[dict]:
    """按 parent 建立层级关系，便于后续存储或导出。"""
    by_id = {c["id"]: c for c in comments}
    for c in comments:
        if c.get("parent") and c["parent"] in by_id:
            parent = by_id[c["parent"]]
            parent.setdefault("children", []).append(c)
    return comments
```

三、电商站点：分类 → 商品详情 → 规范化价格/图片
- 价格与货币：本地化数字解析（见 Topic 13）、货币符号归一到 ISO 4217；存储 Decimal 避免浮点误差。
- 图片下载：安全命名与完整性校验（见 Topic 12）；断点续传避免重复网络与 IO。
- 增量策略：库存与价格频繁变化，按商品 ID 做游标或 TTL；详情字段稳定解析与差异化更新。

示例 C：商品详情解析与图片下载（详注）
```python
# -*- coding: utf-8 -*-
import os, hashlib, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

IMG_DIR = "output_images"; os.makedirs(IMG_DIR, exist_ok=True)

def parse_product(html: str) -> dict:
    """解析商品标题、价格文本、主图 URL。"""
    soup = BeautifulSoup(html, 'html.parser')
    title = (soup.select_one('h1.product-title') or {}).get_text(strip=True)
    price_txt = (soup.select_one('.price') or {}).get_text(strip=True)
    img = soup.select_one('img.main')
    img_url = img.get('src') if img else None
    return {"title": title, "price_raw": price_txt, "img_url": img_url}

def normalize_price(price_txt: str) -> tuple[str, str]:
    """简化的价格归一：返回 (标准化数值字符串, 货币代码)。"""
    sym = 'USD'
    if '¥' in price_txt: sym = 'JPY'
    if '€' in price_txt: sym = 'EUR'
    # 去除千分位，统一小数点（示例）
    num = price_txt.replace(',', '').replace('¥', '').replace('€', '').replace('$', '').strip()
    return num, sym

def download_image(url: str, name_hint: str) -> str | None:
    """
    图片下载：
    - 安全命名：使用标题哈希作为文件名，避免非法字符
    - 完整性：下载后计算 SHA256，返回路径；失败返回 None
    """
    if not url:
        return None
    # 安全文件名（SHA256 截断）
    h = hashlib.sha256(name_hint.encode('utf-8')).hexdigest()[:16]
    ext = os.path.splitext(url.split('?')[0])[1] or '.jpg'
    path = os.path.join(IMG_DIR, f"{h}{ext}")
    try:
        with requests.get(url, stream=True, timeout=(3, 15)) as r:
            if r.status_code != 200:
                return None
            sha = hashlib.sha256()
            with open(path, 'wb') as f:
                for chunk in r.iter_content(1 << 15):
                    if not chunk: continue
                    sha.update(chunk); f.write(chunk)
        # 可记录校验值以供后续比对
        _digest = sha.hexdigest()
        return path
    except requests.RequestException:
        return None
```

四、增量与去重：表结构与约束
- 以 URL/商品 ID/帖子 ID 做 PRIMARY KEY 或 UNIQUE；写入采用 UPSERT（冲突则更新）。
- TTL：对易变字段（库存/价格）设置过期时间；调度器发现过期后重抓更新。
- 示例 SQL（SQLite）：
```sql
CREATE TABLE IF NOT EXISTS products (
  id TEXT PRIMARY KEY,
  title TEXT,
  price TEXT,
  currency TEXT,
  image_path TEXT,
  updated_at INTEGER
);

CREATE TABLE IF NOT EXISTS threads (
  id TEXT PRIMARY KEY,
  title TEXT,
  author TEXT,
  created_at_utc TEXT,
  updated_at INTEGER
);
```

五、可观测与回归（建议清单）
- 指标：抓取成功率/解析成功率/平均耗时/重试次数；按站点/类型维度聚合。
- 日志：结构化（JSON）记录 URL、站点、模块、耗时、失败原因；便于审计与复盘。
- 测试：针对解析函数与 SQL 写入做单元测试；新增站点时复制样例页面作为契约测试基线。

—

—

## 结语
本合辑覆盖了构建通用爬虫所需的关键主题与实践细节。建议以“慢而稳”为基本原则，优先礼貌抓取与幂等增量，结合日志与监控持续优化。在需要高并发或复杂场景（如分布式与浏览器渲染）时，务必评估成本与风险，并在合规边界内行动。如需针对特定站点类型或数据字段扩写实战章节，请告知具体目标，我将继续补充更深入的示例与工程落地方案。