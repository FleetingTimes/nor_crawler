# 通用爬虫深度指南（实践与架构，含注释示例）

本指南不依赖任何现有项目或框架，系统性讲解“如何从零到一设计与实现稳定、可维护、可扩展的通用网页爬虫”。全文围绕伦理合规、抓取策略、解析与抽取、数据质量与去重、增量与复跑、并发与调度、反爬与稳定性、存储与架构、测试与监控、生产化与安全等主题展开，并提供多段带详细中文注释的示例代码，帮助你在实践中落地。

—

## 1. 伦理与合规
- 法律与授权：在抓取之前，务必阅读目标站点的使用条款与 robots.txt；如站点提供开放 API 或授权渠道，优先使用官方方式。
- 尊重与透明：适当设置 `User-Agent` 标识；尊重访问频率与禁止抓取的路径；如站点明确拒绝爬取，应停止并寻找替代方案。
- 数据边界：不采集敏感个人信息，不突破认证边界，不绕过付费墙与安全限制。
- 安全：合理保管 Cookie/Token，避免泄露；对账号登录的行为进行风险评估与限制。

—

## 2. 爬虫的基本术语与流程
- 爬取（Crawling）：自动化访问与下载网页内容（HTML/JSON/媒体）。
- 解析（Parsing）：把原始内容转成结构化数据（标题、时间、正文、图片等）。
- 抽取（Extraction）：对结构化数据进一步清洗归一，形成业务需要的数据格式。
- 存储（Storage）：把数据持久化（文件、DB、对象存储），并做好索引与去重。
- 调度（Scheduling）：管理抓取顺序与速率，维护“前沿队列（Frontier）”。
- 去重（Dedup）：避免重复抓取与重复写入；保证幂等性与增量更新。
- 监控（Observability）：日志、指标、告警；定位异常与性能瓶颈。

典型管线：发现入口 URL → 抓取 → 解析出下一步链接与数据 → 去重与过滤 → 写入存储 → 根据策略把新 URL 加入队列 → 循环直到达成目标或满足终止条件。

—

## 3. HTTP 与网络要点
- 状态码：`2xx` 成功、`3xx` 重定向（注意登录页跳转）、`4xx` 客户端错误（403/404）、`5xx` 服务端错误（可重试）。
- 头部与缓存：`ETag`/`Last-Modified`/`If-None-Match`/`If-Modified-Since` 用于增量与避免重复下载。
- 编码与压缩：处理 `Content-Encoding: gzip/br` 与 `charset`；注意解压和正确解码。
- 会话与认证：`Cookie`/`Bearer Token`/`X-CSRF-Token`；登录态需要安全保存与续期。
- 连接策略：合理超时、连接复用、HTTP/2 支持、代理（正/反向）。

—

## 4. URL 规范化与链接发现
- 绝对化：使用 `urljoin(base, href)` 把相对链接转为绝对链接。
- 去噪：移除 `#fragment` 与无关查询参数；归一 `www` 与协议（http/https）。
- Canonical：尊重页面中的 `<link rel="canonical">` 指定的规范 URL。
- Sitemap：优先利用 `sitemap.xml` 收集结构化入口，减少页面遍历。
- Robots：使用 robots 规则约束抓取范围与速率（见第 6 节）。

—

## 5. HTML 解析与数据抽取
- 选择器：CSS 选择器（BeautifulSoup / lxml / parsel）、XPath；优先选择稳定且语义化的结构。
- 文本清洗：去多空格/换行、统一编码、剔除广告脚本与冗余元素。
- 容错：为关键字段设置回退规则；当结构变化时，尽量通过相邻上下文定位。
- 正则增强：对半结构化字段（如编号、日期）采用正则二次匹配。
- 结构化输出：统一字段名与数据类型，定义清晰的数据 Schema（见第 11 节）。

—

## 6. robots.txt 与礼貌策略（Politeness）
- 解析 robots：动态读取并解析 robots.txt，判断是否允许抓取某路径，获取 `Crawl-delay`。
- 礼貌策略：设置域级最小请求间隔与并发限制，避免短时间高频访问；优先慢速稳定。
- 优雅降速：根据 429/403/5xx 以及失败率自动降速或暂停，防止封禁。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示如何解析 robots.txt 并在抓取前进行允许性判断与最小延时控制

import time
from urllib.parse import urlparse
from urllib.robotparser import RobotFileParser

class PoliteFetcher:
    def __init__(self, user_agent: str = "MyCrawler/1.0", per_domain_delay: float = 1.0):
        self.user_agent = user_agent
        self.per_domain_delay = per_domain_delay  # 域级最小间隔（秒）
        self.last_visit = {}  # 记录各域名上次访问时间
        self.robot_cache = {} # 缓存各域 robots 解析结果，避免重复下载

    def is_allowed(self, url: str) -> bool:
        """根据 robots.txt 判断是否允许抓取目标 URL。"""
        domain = urlparse(url).netloc
        rp = self.robot_cache.get(domain)
        if rp is None:
            rp = RobotFileParser()
            robots_url = f"http://{domain}/robots.txt"
            # 注意：生产中应处理 https 与跳转，并设置合理超时
            try:
                rp.set_url(robots_url)
                rp.read()
            except Exception:
                # robots 获取失败时，可选择默认允许或默认禁止，这里默认允许
                pass
            self.robot_cache[domain] = rp
        # 检查是否允许
        return rp.can_fetch(self.user_agent, url) if rp.default_entry is not None else True

    def wait_if_needed(self, url: str):
        """在发起请求前，根据域级最小间隔进行等待，以实现礼貌抓取。"""
        domain = urlparse(url).netloc
        now = time.time()
        last = self.last_visit.get(domain, 0)
        elapsed = now - last
        if elapsed < self.per_domain_delay:
            sleep_time = self.per_domain_delay - elapsed
            time.sleep(sleep_time)
        # 更新上次访问时间戳
        self.last_visit[domain] = time.time()
```

—

## 7. 退避重试与错误处理
- 指数退避（Exponential Backoff）：每次重试增加等待时间，例如倍增到最大阈值；避免雪崩重试。
- 错误分类：对网络超时、DNS、TLS、5xx 采用重试；对 4xx 尤其 403/404 谨慎重试。
- 熔断与回滚：在错误率高时暂停某域抓取；对部分成功的写入进行回滚或补偿。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示 requests 抓取的退避重试与错误分类处理

import requests
import time

def fetch_with_backoff(url: str, max_retries: int = 3, initial_delay: float = 0.5, max_delay: float = 8.0, timeout: float = 10.0):
    """
    使用指数退避的抓取函数：
    - 针对可重试的错误逐步增加等待时间
    - 对 4xx 进行有限重试或直接放弃（根据业务策略）
    """
    delay = initial_delay
    for attempt in range(1, max_retries + 1):
        try:
            resp = requests.get(url, headers={"User-Agent": "MyCrawler/1.0"}, timeout=timeout)
            # 对 2xx 直接返回
            if 200 <= resp.status_code < 300:
                return resp
            # 对 3xx：可以跟随重定向，但要小心跳登录页或语言页
            if 300 <= resp.status_code < 400:
                # 简化处理：交由 requests 默认重定向机制
                return resp
            # 对 5xx：服务端错误，适合重试
            if 500 <= resp.status_code < 600:
                # 记录并进入退避重试
                time.sleep(delay)
                delay = min(delay * 2, max_delay)
                continue
            # 对 4xx：403/404 通常不重试，也可根据策略重试一次
            if 400 <= resp.status_code < 500:
                # 简单策略：直接返回，让上层决定是否忽略
                return resp
        except requests.RequestException:
            # 网络类异常（超时、连接断开、TLS 等）适合重试
            time.sleep(delay)
            delay = min(delay * 2, max_delay)
    # 超出重试次数：返回 None 或抛出异常
    return None
```

—

## 8. 链接队列与调度策略
- Frontier 队列：维护待抓取 URL 的优先队列；支持 BFS/DFS/评分优先。
- 优先级：根据“新鲜度、重要性、站点结构、失败率”动态调整。
- 去重与状态：对已抓取过的 URL 做标记（哈希或指纹）；避免重复访问。
- 并发控制：设置全局并发与域级并发；避免同一域短期内过度压力。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：一个简单的调度器，支持优先队列与域级并发限制

import heapq
import time
from collections import defaultdict
from urllib.parse import urlparse

class ScheduledItem:
    def __init__(self, priority: float, url: str):
        self.priority = priority  # 数值越小优先级越高
        self.url = url
    def __lt__(self, other):
        return self.priority < other.priority

class SimpleScheduler:
    def __init__(self, per_domain_concurrency: int = 2):
        self.frontier = []  # 最小堆实现优先队列
        self.inflight = defaultdict(int)  # 各域当前执行中的请求数量
        self.per_domain_concurrency = per_domain_concurrency
        self.seen = set()  # URL 去重（可改用哈希）

    def push(self, url: str, priority: float = 1.0):
        if url in self.seen:
            return
        self.seen.add(url)
        heapq.heappush(self.frontier, ScheduledItem(priority, url))

    def pop(self):
        """取出下一个可执行的 URL，遵守域级并发上限。"""
        for _ in range(len(self.frontier)):
            item = heapq.heappop(self.frontier)
            domain = urlparse(item.url).netloc
            if self.inflight[domain] < self.per_domain_concurrency:
                self.inflight[domain] += 1
                return item.url
            else:
                # 该域并发已满，放回队列（稍微降低优先级防止饥饿）
                item.priority += 0.1
                heapq.heappush(self.frontier, item)
        return None

    def mark_done(self, url: str):
        domain = urlparse(url).netloc
        self.inflight[domain] = max(0, self.inflight[domain] - 1)
```

—

## 9. 去重、指纹与幂等
- URL 去重：归一化后进行字符串去重或哈希集合；可存储在内存/Redis/SQLite。
- 内容指纹：MD5/SHA1（强重复检测）与 SimHash（近似重复检测）；防止同内容多次写入。
- 幂等写入：保证相同数据不会重复落盘；通过唯一键（如 URL 规范化）或唯一索引（DB）。
- 追加策略：对于日志式文本（如 `data.txt`），采用追加写与去重校验并存（避免覆盖）。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示使用 SQLite 做 URL 去重与写入幂等控制

import sqlite3
from pathlib import Path

class DedupStore:
    def __init__(self, db_path: str):
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(db_path)
        self._init_schema()

    def _init_schema(self):
        cur = self.conn.cursor()
        # 建表：url 唯一约束实现幂等，不重复插入
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS pages (
                url TEXT PRIMARY KEY,
                status INTEGER,
                fetched_at TEXT
            )
            """
        )
        self.conn.commit()

    def seen(self, url: str) -> bool:
        cur = self.conn.cursor()
        cur.execute("SELECT 1 FROM pages WHERE url = ?", (url,))
        return cur.fetchone() is not None

    def mark(self, url: str, status: int, ts: str):
        cur = self.conn.cursor()
        # INSERT OR REPLACE 可按需改为严格 INSERT 并捕获冲突
        cur.execute("INSERT OR REPLACE INTO pages(url, status, fetched_at) VALUES(?,?,?)", (url, status, ts))
        self.conn.commit()
```

—

## 10. 增量抓取与复跑策略
- 条件请求：使用 `ETag`/`If-None-Match` 或 `Last-Modified`/`If-Modified-Since`，避免重复下载。
- 分页策略：`max_pages_per_run` 限制一次遍历深度；`stop_when_zero_new` 当无新增时停止翻页。
- 断点续抓：维护“进行中”状态与失败列表，重启后优先从未完成/失败条目继续。
- 写入幂等：详情成功后写入数据与去重标记；重启不会重复落盘（见第 9 节）。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示使用条件请求实现“增量抓取”

import requests

def fetch_incremental(url: str, etag: str | None = None, last_modified: str | None = None):
    headers = {"User-Agent": "MyCrawler/1.0"}
    if etag:
        headers["If-None-Match"] = etag
    if last_modified:
        headers["If-Modified-Since"] = last_modified
    resp = requests.get(url, headers=headers, timeout=10)
    # 304 表示内容未变，可以跳过解析与存储
    if resp.status_code == 304:
        return None, etag, last_modified
    # 若返回新内容，则更新 etag/last_modified 并继续处理
    new_etag = resp.headers.get("ETag", etag)
    new_last_modified = resp.headers.get("Last-Modified", last_modified)
    return resp, new_etag, new_last_modified
```

—

## 11. 数据 Schema 与质量控制
- Schema 设计：字段名统一、类型明确（字符串/时间/列表/映射）；定义最小必需集。
- 校验：用正则/日期解析/枚举进行校验；缺失字段采用默认值或标记异常。
- 归一化：时间统一时区与格式（ISO 8601）、文本去噪、类别映射。
- 质量指标：完整率、唯一性、重复率、错误率、解析耗时；持续监控。

—

## 12. 文件与媒体下载（命名、扩展名识别）
- 安全命名：替换文件名中的非法字符（`/ : ? * | " < >`）为 `_`；控制长度与编码。
- 扩展识别：优先从 URL 路径后缀识别（.jpg/.png/.webp/.gif/.jpeg），回退到 `Content-Type`；`.jpeg` 归一为 `.jpg`。
- 写入策略：先下载到临时文件，完整性校验通过后再移动到目标路径，避免半文件。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示图片下载的扩展名识别与安全命名

import os
import re
import requests

INVALID_CHARS = r"[/\\:?*|\"<>]"  # Windows/Unix 通用非法字符集合

EXT_PATTERN = re.compile(r"\.(jpg|jpeg|png|webp|gif)(\d+)?$", re.IGNORECASE)

def safe_filename(name: str) -> str:
    return re.sub(INVALID_CHARS, "_", name).strip()[:128]  # 控制最大长度以防文件系统限制

def guess_ext(url: str, content_type: str | None) -> str:
    # 1) 根据 URL 路径猜测扩展名
    m = EXT_PATTERN.search(url.split("?")[0])
    if m:
        ext = m.group(1).lower()
        if ext == "jpeg":
            return ".jpg"  # 归一化
        return f".{ext}"
    # 2) 回退到 Content-Type
    if content_type:
        if "image/jpeg" in content_type:
            return ".jpg"
        if "image/png" in content_type:
            return ".png"
        if "image/webp" in content_type:
            return ".webp"
        if "image/gif" in content_type:
            return ".gif"
    return ".bin"  # 无法识别则作为二进制

def download_image(url: str, keyword: str, dirpath: str):
    os.makedirs(dirpath, exist_ok=True)
    resp = requests.get(url, headers={"User-Agent": "MyCrawler/1.0"}, timeout=15)
    if resp.status_code != 200:
        return False, None
    ext = guess_ext(url, resp.headers.get("Content-Type"))
    filename = safe_filename(keyword) + ext
    tmp = os.path.join(dirpath, filename + ".tmp")
    final = os.path.join(dirpath, filename)
    with open(tmp, "wb") as f:
        f.write(resp.content)
    os.replace(tmp, final)  # 原子替换，避免半文件
    return True, final
```

—

## 13. 并发模型与异步抓取
- 线程池：适合 CPU 轻、I/O 多的场景；注意线程安全与 GIL 影响。
- 协程（asyncio）：高并发 I/O；需要精心设计节流与域级限速。
- 进程池：适合 CPU 重的解析/计算；共享状态管理较复杂。
- 组合：抓取用协程，解析用线程/进程；通过队列解耦。

示例（带详细注释）：
```python
# -*- coding: utf-8 -*-
# 说明：演示 asyncio 协程并发抓取与域级节流

import asyncio
import time
import aiohttp
from urllib.parse import urlparse

class DomainLimiter:
    def __init__(self, per_domain_delay: float = 0.5):
        self.delay = per_domain_delay
        self.last_visit = {}
        self.lock = asyncio.Lock()

    async def wait(self, url: str):
        domain = urlparse(url).netloc
        async with self.lock:
            now = time.time()
            last = self.last_visit.get(domain, 0)
            elapsed = now - last
            if elapsed < self.delay:
                await asyncio.sleep(self.delay - elapsed)
            self.last_visit[domain] = time.time()

async def fetch(session: aiohttp.ClientSession, limiter: DomainLimiter, url: str):
    await limiter.wait(url)  # 域级节流
    try:
        async with session.get(url, timeout=10) as resp:
            # 简化处理：仅返回文本
            if resp.status == 200:
                return await resp.text()
            return None
    except Exception:
        return None

async def crawl(urls: list[str]):
    limiter = DomainLimiter(per_domain_delay=0.5)
    async with aiohttp.ClientSession(headers={"User-Agent": "MyCrawler/1.0"}) as session:
        tasks = [asyncio.create_task(fetch(session, limiter, u)) for u in urls]
        results = await asyncio.gather(*tasks)
        return results

# 使用方法：
# asyncio.run(crawl(["https://example.com", "https://example.org"]))
```

—

## 14. 动态页面与渲染（Playwright/Selenium）
- 何时使用：当内容强依赖 JS 才可见；否则尽量使用静态抓取降低复杂度。
- 渲染流程：启动浏览器 → 访问 → 等待必要元素加载 → 获取渲染后的 HTML → 解析。
- 成本与风险：更重的资源开销、指纹与反自动化检测、运行稳定性要求高。

—

## 15. 代理、UA 轮换与防封策略
- 代理池：健康检查与熔断；对高失败率代理自动降级或移除；记录代理成功率。
- UA 轮换：合理的 UA 列表，避免过于统一；遵守站点要求与规范。
- 速率与并发：根本策略是“慢而稳”；过度并发会触发封禁与限速。
- 黑/白名单：根据站点反馈维护；在异常时自动暂停该域。

—

## 16. 失败恢复与一致性
- 失败列表：把解析失败或写入失败的条目记录到集中列表，便于后续重试。
- 原子写入：先写临时文件，成功后原子替换目标文件；避免部分写入。
- 事务化：DB 写入采用事务；失败回滚，保证数据一致性。

—

## 17. 日志、指标与监控
- 日志：记录抓取 URL、状态码、耗时、解析结果、错误栈；可分级（INFO/WARN/ERROR）。
- 指标：成功率、失败率、平均延时、并发数、队列长度、页面大小；导出到 Prometheus 等。
- 告警：失败率超阈值、连续错误、代理不可用、登录态失效；自动通知。

—

## 18. 架构与模块化设计
- 分层：抓取层（HTTP）/解析层（HTML）/抽取层（业务）/存储层（DB）/调度层（Frontier）。
- 解耦：通过队列或事件总线解耦各层；便于扩展与测试。
- 可替换性：抓取器、解析器、存储实现可替换；根据场景选型。

—

## 19. 数据存储与索引
- 文件：按站点/日期分目录；适合简单文本与媒体存储。
- DB：SQLite/PostgreSQL/ElasticSearch；支持唯一约束、索引与搜索。
- 对象存储：S3/OSS 等；适合媒体与大文件；配合元数据 DB 管理。

—

## 20. 安全与合规的工程实践
- 密钥管理：使用环境变量或安全管理服务（如 KMS），避免硬编码。
- 最小权限：仅授予必要的网络与存储权限；隔离抓取与业务处理环境。
- 审计：记录访问与写入行为；在问题发生时可追溯与定位。

—

## 21. 测试与质量保障
- 单元测试：对解析函数与抽取逻辑编写测试；用离线快照验证稳定性。
- 集成测试：对抓取-解析-存储管线进行端到端测试；模拟异常与超时。
- 回归测试：在站点结构变化时快速验证；维护代表性样本集合。

—

## 22. 生产化与部署
- 任务编排：定时运行、分页上限、早停条件；避免长时间无休止抓取。
- 持续化：数据归档与清理；根据 TTL 与重要性决定重抓周期。
- 灰度策略：对新策略或新站点先小规模运行，稳定后扩大范围。

—

## 23. 实战范式：从零实现一个小型爬虫

示例：抓取一个公开新闻站点的列表页与详情页，提取标题、时间与正文，并保存到本地文件，含详细注释。
```python
# -*- coding: utf-8 -*-
# 一个“从零到一”的小型爬虫示例：
# - 抓取列表页，发现详情链接
# - 抓取详情页，抽取标题/时间/正文
# - 去重与追加写入
# - 具备基础礼貌策略与重试

import os
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

USER_AGENT = "MyCrawler/1.0"
BASE_URL = "https://example-news.com/"
OUTPUT_DIR = "output_news"
DATA_TXT = os.path.join(OUTPUT_DIR, "data.txt")
DEDUP_TXT = os.path.join(OUTPUT_DIR, "dedup.txt")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 读入已抓取的 URL（去重文件），避免重复写入
if not os.path.exists(DEDUP_TXT):
    open(DEDUP_TXT, "w").close()
with open(DEDUP_TXT, "r", encoding="utf-8") as f:
    SEEN_URLS = set(line.strip() for line in f if line.strip())

HEADERS = {"User-Agent": USER_AGENT}

def polite_get(url: str, retries: int = 3, delay: float = 0.5):
    """带基础礼貌等待与有限重试的 GET。"""
    for i in range(retries):
        try:
            time.sleep(delay)  # 简单礼貌等待，生产中建议按域粒度
            resp = requests.get(url, headers=HEADERS, timeout=10)
            if resp.status_code == 200:
                return resp
            if 500 <= resp.status_code < 600:
                # 服务端错误，重试
                continue
            # 其他状态码：简单返回让上层决定
            return resp
        except requests.RequestException:
            # 网络异常：重试
            continue
    return None

def parse_list(html: str, base_url: str):
    """解析列表页，返回详情链接列表。"""
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for a in soup.select(".list-item a.title"):
        href = a.get("href")
        if not href:
            continue
        url = urljoin(base_url, href)
        links.append(url)
    return links

def parse_detail(html: str):
    """解析详情页，返回结构化数据：标题/时间/正文。"""
    soup = BeautifulSoup(html, "html.parser")
    title = soup.select_one("h1.article-title")
    date = soup.select_one("time.pub-date")
    content = soup.select_one("div.article-content")
    # 容错与清洗
    title_text = title.get_text(strip=True) if title else ""
    date_text = date.get("datetime", "").strip() if date else ""
    content_text = content.get_text("\n", strip=True) if content else ""
    return {
        "title": title_text,
        "date": date_text,
        "content": content_text
    }

def append_record(url: str, record: dict):
    """追加写入 data.txt，并更新去重文件。"""
    with open(DATA_TXT, "a", encoding="utf-8") as f:
        f.write(f"URL: {url}\n")
        f.write(f"Title: {record['title']}\n")
        f.write(f"Date: {record['date']}\n")
        f.write("Content:\n")
        f.write(record["content"] + "\n")
        f.write("\n")
    with open(DEDUP_TXT, "a", encoding="utf-8") as f:
        f.write(url + "\n")
    SEEN_URLS.add(url)

def crawl_once(list_url: str):
    """抓取一次列表页与新增详情页。"""
    resp = polite_get(list_url)
    if not resp or resp.status_code != 200:
        print("列表页抓取失败")
        return
    detail_links = parse_list(resp.text, BASE_URL)
    # 增量策略：只抓取“未在去重中的”详情链接
    new_links = [u for u in detail_links if u not in SEEN_URLS]
    for url in new_links:
        d = polite_get(url)
        if not d or d.status_code != 200:
            print("详情抓取失败：", url)
            continue
        record = parse_detail(d.text)
        # 幂等：若解析为空或明显异常，可跳过写入
        if not record["title"] or not record["content"]:
            print("详情字段缺失，跳过：", url)
            continue
        append_record(url, record)
        print("写入成功：", url)

# 用法示例：
# crawl_once(urljoin(BASE_URL, "/news"))
```

—

## 24. 质量与演进：从单站到多站
- 统一抽取接口：为多站点建立统一抽取 Schema（标题/时间/正文/作者/标签）。
- 站点适配：每个站点实现自己的解析器；通过配置驱动入口与规则。
- 评估与演进：跟踪数据质量指标与失败原因；迭代选择器与容错策略。

—

## 25. 总结与实践建议
- 慢而稳：并发与速率以稳定为主；礼貌抓取优先级高。
- 幂等与增量：去重、指纹与条件请求是长期运行的基石。
- 容错与监控：日志与指标可观测是发现问题的关键。
- 模块化与可替换：抓取/解析/存储分层设计；便于维护与扩展。
- 合规与安全：始终在合法边界内行动，保护隐私与账号安全。

—

本指南涵盖爬虫设计与实现的核心要点，并通过带详细中文注释的示例代码帮助你快速落地。若需要进一步扩展专题（如“分布式爬虫、BloomFilter 去重、SimHash 近重复检测、Sitemap 深度利用、条件重抓策略、Playwright 稳定化实践”等），告诉我你的目标与场景，我可以继续细化扩展章节与示例代码。